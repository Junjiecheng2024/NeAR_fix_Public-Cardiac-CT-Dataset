nohup: ignoring input
/home/user/persistent/NeAR_fix_Public-Cardiac-CT-Dataset/NeAR/repairing/near_repairing/stage1_coronary/train_coronary.py:340: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler(enabled=use_amp)
{
    "base_path": "./checkpoints",
    "run_flag": "Coronary_class9_shape_only_251109_092755",
    "data_path": "../../../../dataset/near_format_data",
    "class_name": "Coronary",
    "class_index": 9,
    "n_epochs": 300,
    "appearance": false,
    "decoder_channels": [
        64,
        48,
        32,
        16
    ],
    "latent_dimension": 256,
    "training_resolution": 64,
    "target_resolution": 128,
    "n_training_samples": null,
    "lr": 0.001,
    "batch_size": 2,
    "gradient_accumulation_steps": 2,
    "eval_batch_size": 2,
    "n_workers": 8,
    "use_cosine_schedule": false,
    "milestones": [
        100,
        200
    ],
    "gamma": 0.5,
    "warmup_ratio": 0.01,
    "use_amp": true,
    "eval_interval": 10,
    "grid_noise": 0,
    "uniform_grid_noise": true,
    "sampling_bias_ratio": 0.5,
    "sampling_dilation_radius": 2,
    "l2_penalty_weight": 0.0001,
    "resume_checkpoint": "./checkpoints/Coronary_class9_shape_only_251109_061814/best.pth"
}

======================================================================
Training NeAR for Coronary (Class 9)
Data path: ../../../../dataset/near_format_data
Checkpoint path: ./checkpoints/Coronary_class9_shape_only_251109_092755
======================================================================

Loaded 998 samples for class 'Coronary'
Using biased sampling: 50% near boundaries (dilation radius=2)
Loaded 998 samples for class 'Coronary'
Using biased sampling: 50% near boundaries (dilation radius=2)
GatherGridsFromVolumes: Using boundary-biased sampling (50% near boundaries, dilation=2)
Total samples: 998
Boundary-biased sampling configuration:
  Training bias ratio: 50% near boundaries (dynamic)
  Evaluation bias ratio: 0% (uniform sampling)
  Dilation radius: 2 voxels

Using Automatic Mixed Precision (AMP) training

Using MultiStepLR scheduler:
  Milestones: [100, 200]
  Gamma: 0.5 (lr减半)
Gradient accumulation steps: 2
Effective batch size: 4

Using ENHANCED Combined Loss:
  - 70% Dice Loss (direct metric optimization)
  - 30% Focal Loss (gamma=4.0, 256x hard example weighting)
  - Higher Dice weight ensures we optimize the right metric
  - Gamma=4 provides stronger focus on hard examples


======================================================================
Loading checkpoint from: ./checkpoints/Coronary_class9_shape_only_251109_061814/best.pth
Successfully loaded checkpoint!
======================================================================

Evaluation策略: 第1轮后验证，之后每10轮验证一次


Starting training for 300 epochs...

Curriculum Learning Strategy:
  Epoch 1-100:   50% boundary bias (current)
  Epoch 101-200: 30% boundary bias (reduce)
  Epoch 201-300: 10% boundary bias (final adaptation)

Epoch 1/300 (boundary_bias=50%)
/home/user/miniconda3/envs/near/lib/python3.10/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/user/persistent/NeAR_fix_Public-Cardiac-CT-Dataset/NeAR/repairing/near_repairing/stage1_coronary/train_coronary.py:129: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=use_amp):
  Batch [0/499] Loss: 0.3274 Dice: 0.5427 LR: 0.001000
  Batch [50/499] Loss: 0.2769 Dice: 0.6151 LR: 0.001000
  Batch [100/499] Loss: 0.2417 Dice: 0.6678 LR: 0.001000
  Batch [150/499] Loss: 0.2464 Dice: 0.6638 LR: 0.001000
  Batch [200/499] Loss: 0.3561 Dice: 0.5084 LR: 0.001000
  Batch [250/499] Loss: 0.2971 Dice: 0.5870 LR: 0.001000
  Batch [300/499] Loss: 0.2162 Dice: 0.6987 LR: 0.001000
  Batch [350/499] Loss: 0.2940 Dice: 0.5918 LR: 0.001000
  Batch [400/499] Loss: 0.2928 Dice: 0.5887 LR: 0.001000
  Batch [450/499] Loss: 0.3088 Dice: 0.5718 LR: 0.001000
Epoch 1 Train: 	[total] 0.2907	[shape] 0.2900	[dice] 0.5966	[l2] 6.2532
  Training time: 120.24s
/home/user/persistent/NeAR_fix_Public-Cardiac-CT-Dataset/NeAR/repairing/near_repairing/stage1_coronary/train_coronary.py:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=use_amp):
Epoch 1 Eval:  	[total] 0.6516	[shape] 0.6509	[dice] 0.0757	[l2] 6.2554
  Validation time: 204.30s
======================================================================
Found a new best model! Shape Loss: 0.650933
======================================================================
  Total epoch time: 324.54s

Epoch 2/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2667 Dice: 0.6323 LR: 0.001000
  Batch [50/499] Loss: 0.3335 Dice: 0.5397 LR: 0.001000
  Batch [100/499] Loss: 0.3104 Dice: 0.5704 LR: 0.001000
  Batch [150/499] Loss: 0.3424 Dice: 0.5187 LR: 0.001000
  Batch [200/499] Loss: 0.2925 Dice: 0.5940 LR: 0.001000
  Batch [250/499] Loss: 0.4492 Dice: 0.3742 LR: 0.001000
  Batch [300/499] Loss: 0.3638 Dice: 0.4879 LR: 0.001000
  Batch [350/499] Loss: 0.2949 Dice: 0.5884 LR: 0.001000
  Batch [400/499] Loss: 0.3521 Dice: 0.5157 LR: 0.001000
  Batch [450/499] Loss: 0.2940 Dice: 0.5837 LR: 0.001000
Epoch 2 Train: 	[total] 0.3006	[shape] 0.3000	[dice] 0.5824	[l2] 6.2555
  Training time: 88.11s
  Total epoch time: 88.11s (no validation)

Epoch 3/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3734 Dice: 0.4756 LR: 0.001000
  Batch [50/499] Loss: 0.3031 Dice: 0.5805 LR: 0.001000
  Batch [100/499] Loss: 0.3165 Dice: 0.5607 LR: 0.001000
  Batch [150/499] Loss: 0.2829 Dice: 0.6015 LR: 0.001000
  Batch [200/499] Loss: 0.2893 Dice: 0.6012 LR: 0.001000
  Batch [250/499] Loss: 0.3394 Dice: 0.5292 LR: 0.001000
  Batch [300/499] Loss: 0.3132 Dice: 0.5582 LR: 0.001000
  Batch [350/499] Loss: 0.3437 Dice: 0.5217 LR: 0.001000
  Batch [400/499] Loss: 0.3171 Dice: 0.5555 LR: 0.001000
  Batch [450/499] Loss: 0.2705 Dice: 0.6248 LR: 0.001000
Epoch 3 Train: 	[total] 0.3010	[shape] 0.3004	[dice] 0.5818	[l2] 6.2593
  Training time: 88.07s
  Total epoch time: 88.07s (no validation)

Epoch 4/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3116 Dice: 0.5678 LR: 0.001000
  Batch [50/499] Loss: 0.3189 Dice: 0.5622 LR: 0.001000
  Batch [100/499] Loss: 0.2489 Dice: 0.6559 LR: 0.001000
  Batch [150/499] Loss: 0.2876 Dice: 0.6049 LR: 0.001000
  Batch [200/499] Loss: 0.2840 Dice: 0.6064 LR: 0.001000
  Batch [250/499] Loss: 0.2814 Dice: 0.6131 LR: 0.001000
  Batch [300/499] Loss: 0.3622 Dice: 0.4936 LR: 0.001000
  Batch [350/499] Loss: 0.4294 Dice: 0.4011 LR: 0.001000
  Batch [400/499] Loss: 0.2591 Dice: 0.6275 LR: 0.001000
  Batch [450/499] Loss: 0.3301 Dice: 0.5401 LR: 0.001000
Epoch 4 Train: 	[total] 0.3008	[shape] 0.3002	[dice] 0.5821	[l2] 6.2636
  Training time: 88.24s
  Total epoch time: 88.24s (no validation)

Epoch 5/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3052 Dice: 0.5780 LR: 0.001000
  Batch [50/499] Loss: 0.3015 Dice: 0.5804 LR: 0.001000
  Batch [100/499] Loss: 0.4109 Dice: 0.4290 LR: 0.001000
  Batch [150/499] Loss: 0.2989 Dice: 0.5827 LR: 0.001000
  Batch [200/499] Loss: 0.3091 Dice: 0.5687 LR: 0.001000
  Batch [250/499] Loss: 0.3353 Dice: 0.5281 LR: 0.001000
  Batch [300/499] Loss: 0.2676 Dice: 0.6314 LR: 0.001000
  Batch [350/499] Loss: 0.2425 Dice: 0.6648 LR: 0.001000
  Batch [400/499] Loss: 0.3609 Dice: 0.5011 LR: 0.001000
  Batch [450/499] Loss: 0.2554 Dice: 0.6448 LR: 0.001000
Epoch 5 Train: 	[total] 0.2997	[shape] 0.2991	[dice] 0.5834	[l2] 6.2681
  Training time: 87.97s
  Total epoch time: 87.97s (no validation)

Epoch 6/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2256 Dice: 0.6957 LR: 0.001000
  Batch [50/499] Loss: 0.3557 Dice: 0.4978 LR: 0.001000
  Batch [100/499] Loss: 0.2675 Dice: 0.6287 LR: 0.001000
  Batch [150/499] Loss: 0.2409 Dice: 0.6677 LR: 0.001000
  Batch [200/499] Loss: 0.2608 Dice: 0.6398 LR: 0.001000
  Batch [250/499] Loss: 0.3319 Dice: 0.5369 LR: 0.001000
  Batch [300/499] Loss: 0.2387 Dice: 0.6719 LR: 0.001000
  Batch [350/499] Loss: 0.2353 Dice: 0.6773 LR: 0.001000
  Batch [400/499] Loss: 0.2591 Dice: 0.6426 LR: 0.001000
  Batch [450/499] Loss: 0.2856 Dice: 0.6056 LR: 0.001000
Epoch 6 Train: 	[total] 0.2983	[shape] 0.2977	[dice] 0.5857	[l2] 6.2726
  Training time: 88.17s
  Total epoch time: 88.17s (no validation)

Epoch 7/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2269 Dice: 0.6874 LR: 0.001000
  Batch [50/499] Loss: 0.2693 Dice: 0.6224 LR: 0.001000
  Batch [100/499] Loss: 0.3095 Dice: 0.5753 LR: 0.001000
  Batch [150/499] Loss: 0.3217 Dice: 0.5520 LR: 0.001000
  Batch [200/499] Loss: 0.2971 Dice: 0.5782 LR: 0.001000
  Batch [250/499] Loss: 0.3467 Dice: 0.5132 LR: 0.001000
  Batch [300/499] Loss: 0.3502 Dice: 0.5099 LR: 0.001000
  Batch [350/499] Loss: 0.2364 Dice: 0.6759 LR: 0.001000
  Batch [400/499] Loss: 0.2810 Dice: 0.6094 LR: 0.001000
  Batch [450/499] Loss: 0.2734 Dice: 0.6217 LR: 0.001000
Epoch 7 Train: 	[total] 0.3001	[shape] 0.2995	[dice] 0.5827	[l2] 6.2770
  Training time: 88.08s
  Total epoch time: 88.08s (no validation)

Epoch 8/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2328 Dice: 0.6780 LR: 0.001000
  Batch [50/499] Loss: 0.2831 Dice: 0.6128 LR: 0.001000
  Batch [100/499] Loss: 0.2631 Dice: 0.6425 LR: 0.001000
  Batch [150/499] Loss: 0.2601 Dice: 0.6432 LR: 0.001000
  Batch [200/499] Loss: 0.3005 Dice: 0.5811 LR: 0.001000
  Batch [250/499] Loss: 0.2607 Dice: 0.6390 LR: 0.001000
  Batch [300/499] Loss: 0.2039 Dice: 0.7097 LR: 0.001000
  Batch [350/499] Loss: 0.2766 Dice: 0.6079 LR: 0.001000
  Batch [400/499] Loss: 0.2940 Dice: 0.5791 LR: 0.001000
  Batch [450/499] Loss: 0.2898 Dice: 0.5967 LR: 0.001000
Epoch 8 Train: 	[total] 0.2968	[shape] 0.2961	[dice] 0.5876	[l2] 6.2817
  Training time: 88.47s
  Total epoch time: 88.47s (no validation)

Epoch 9/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2763 Dice: 0.6178 LR: 0.001000
  Batch [50/499] Loss: 0.3198 Dice: 0.5520 LR: 0.001000
  Batch [100/499] Loss: 0.2696 Dice: 0.6229 LR: 0.001000
  Batch [150/499] Loss: 0.3251 Dice: 0.5487 LR: 0.001000
  Batch [200/499] Loss: 0.4172 Dice: 0.4152 LR: 0.001000
  Batch [250/499] Loss: 0.2373 Dice: 0.6699 LR: 0.001000
  Batch [300/499] Loss: 0.2730 Dice: 0.6180 LR: 0.001000
  Batch [350/499] Loss: 0.3102 Dice: 0.5745 LR: 0.001000
  Batch [400/499] Loss: 0.3599 Dice: 0.5002 LR: 0.001000
  Batch [450/499] Loss: 0.2392 Dice: 0.6650 LR: 0.001000
Epoch 9 Train: 	[total] 0.2976	[shape] 0.2970	[dice] 0.5863	[l2] 6.2856
  Training time: 88.18s
  Total epoch time: 88.18s (no validation)

Epoch 10/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2441 Dice: 0.6598 LR: 0.001000
  Batch [50/499] Loss: 0.2632 Dice: 0.6318 LR: 0.001000
  Batch [100/499] Loss: 0.2352 Dice: 0.6730 LR: 0.001000
  Batch [150/499] Loss: 0.2908 Dice: 0.5974 LR: 0.001000
  Batch [200/499] Loss: 0.2766 Dice: 0.6185 LR: 0.001000
  Batch [250/499] Loss: 0.3156 Dice: 0.5603 LR: 0.001000
  Batch [300/499] Loss: 0.3788 Dice: 0.4696 LR: 0.001000
  Batch [350/499] Loss: 0.2269 Dice: 0.6837 LR: 0.001000
  Batch [400/499] Loss: 0.3020 Dice: 0.5766 LR: 0.001000
  Batch [450/499] Loss: 0.2282 Dice: 0.6815 LR: 0.001000
Epoch 10 Train: 	[total] 0.2968	[shape] 0.2962	[dice] 0.5873	[l2] 6.2902
  Training time: 88.19s
Epoch 10 Eval:  	[total] 0.6523	[shape] 0.6517	[dice] 0.0744	[l2] 6.2938
  Validation time: 202.37s
  Total epoch time: 290.56s

Epoch 11/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2718 Dice: 0.6217 LR: 0.001000
  Batch [50/499] Loss: 0.2990 Dice: 0.5865 LR: 0.001000
  Batch [100/499] Loss: 0.4295 Dice: 0.4039 LR: 0.001000
  Batch [150/499] Loss: 0.2706 Dice: 0.6270 LR: 0.001000
  Batch [200/499] Loss: 0.3399 Dice: 0.5285 LR: 0.001000
  Batch [250/499] Loss: 0.2548 Dice: 0.6538 LR: 0.001000
  Batch [300/499] Loss: 0.2493 Dice: 0.6467 LR: 0.001000
  Batch [350/499] Loss: 0.2849 Dice: 0.6039 LR: 0.001000
  Batch [400/499] Loss: 0.2374 Dice: 0.6726 LR: 0.001000
  Batch [450/499] Loss: 0.3447 Dice: 0.5232 LR: 0.001000
Epoch 11 Train: 	[total] 0.2969	[shape] 0.2963	[dice] 0.5873	[l2] 6.2941
  Training time: 88.15s
  Total epoch time: 88.15s (no validation)

Epoch 12/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2348 Dice: 0.6763 LR: 0.001000
  Batch [50/499] Loss: 0.2865 Dice: 0.6033 LR: 0.001000
  Batch [100/499] Loss: 0.4094 Dice: 0.4258 LR: 0.001000
  Batch [150/499] Loss: 0.2068 Dice: 0.7158 LR: 0.001000
  Batch [200/499] Loss: 0.2381 Dice: 0.6714 LR: 0.001000
  Batch [250/499] Loss: 0.3438 Dice: 0.5218 LR: 0.001000
  Batch [300/499] Loss: 0.2881 Dice: 0.6048 LR: 0.001000
  Batch [350/499] Loss: 0.2837 Dice: 0.6085 LR: 0.001000
  Batch [400/499] Loss: 0.3214 Dice: 0.5478 LR: 0.001000
  Batch [450/499] Loss: 0.3265 Dice: 0.5503 LR: 0.001000
Epoch 12 Train: 	[total] 0.2952	[shape] 0.2946	[dice] 0.5899	[l2] 6.2985
  Training time: 88.22s
  Total epoch time: 88.22s (no validation)

Epoch 13/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2845 Dice: 0.6013 LR: 0.001000
  Batch [50/499] Loss: 0.2941 Dice: 0.5959 LR: 0.001000
  Batch [100/499] Loss: 0.3036 Dice: 0.5804 LR: 0.001000
  Batch [150/499] Loss: 0.2735 Dice: 0.6201 LR: 0.001000
  Batch [200/499] Loss: 0.2686 Dice: 0.6326 LR: 0.001000
  Batch [250/499] Loss: 0.2643 Dice: 0.6294 LR: 0.001000
  Batch [300/499] Loss: 0.2473 Dice: 0.6558 LR: 0.001000
  Batch [350/499] Loss: 0.2652 Dice: 0.6377 LR: 0.001000
  Batch [400/499] Loss: 0.2707 Dice: 0.6368 LR: 0.001000
  Batch [450/499] Loss: 0.2698 Dice: 0.6293 LR: 0.001000
Epoch 13 Train: 	[total] 0.2943	[shape] 0.2937	[dice] 0.5913	[l2] 6.3025
  Training time: 87.92s
  Total epoch time: 87.92s (no validation)

Epoch 14/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3523 Dice: 0.5127 LR: 0.001000
  Batch [50/499] Loss: 0.3661 Dice: 0.4900 LR: 0.001000
  Batch [100/499] Loss: 0.3804 Dice: 0.4734 LR: 0.001000
  Batch [150/499] Loss: 0.3316 Dice: 0.5399 LR: 0.001000
  Batch [200/499] Loss: 0.3320 Dice: 0.5326 LR: 0.001000
  Batch [250/499] Loss: 0.3186 Dice: 0.5579 LR: 0.001000
  Batch [300/499] Loss: 0.3421 Dice: 0.5308 LR: 0.001000
  Batch [350/499] Loss: 0.2650 Dice: 0.6347 LR: 0.001000
  Batch [400/499] Loss: 0.3152 Dice: 0.5616 LR: 0.001000
  Batch [450/499] Loss: 0.3407 Dice: 0.5287 LR: 0.001000
Epoch 14 Train: 	[total] 0.2931	[shape] 0.2925	[dice] 0.5930	[l2] 6.3068
  Training time: 87.97s
  Total epoch time: 87.97s (no validation)

Epoch 15/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3121 Dice: 0.5735 LR: 0.001000
  Batch [50/499] Loss: 0.3676 Dice: 0.4872 LR: 0.001000
  Batch [100/499] Loss: 0.1936 Dice: 0.7283 LR: 0.001000
  Batch [150/499] Loss: 0.2084 Dice: 0.7143 LR: 0.001000
  Batch [200/499] Loss: 0.3197 Dice: 0.5603 LR: 0.001000
  Batch [250/499] Loss: 0.2535 Dice: 0.6411 LR: 0.001000
  Batch [300/499] Loss: 0.2892 Dice: 0.6052 LR: 0.001000
  Batch [350/499] Loss: 0.2913 Dice: 0.5955 LR: 0.001000
  Batch [400/499] Loss: 0.4287 Dice: 0.4063 LR: 0.001000
  Batch [450/499] Loss: 0.3315 Dice: 0.5467 LR: 0.001000
Epoch 15 Train: 	[total] 0.2926	[shape] 0.2920	[dice] 0.5937	[l2] 6.3110
  Training time: 88.18s
  Total epoch time: 88.18s (no validation)

Epoch 16/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3327 Dice: 0.5432 LR: 0.001000
  Batch [50/499] Loss: 0.3123 Dice: 0.5716 LR: 0.001000
  Batch [100/499] Loss: 0.4199 Dice: 0.4040 LR: 0.001000
  Batch [150/499] Loss: 0.3162 Dice: 0.5637 LR: 0.001000
  Batch [200/499] Loss: 0.1886 Dice: 0.7374 LR: 0.001000
  Batch [250/499] Loss: 0.2232 Dice: 0.6932 LR: 0.001000
  Batch [300/499] Loss: 0.2611 Dice: 0.6467 LR: 0.001000
  Batch [350/499] Loss: 0.2919 Dice: 0.5891 LR: 0.001000
  Batch [400/499] Loss: 0.2921 Dice: 0.5905 LR: 0.001000
  Batch [450/499] Loss: 0.2988 Dice: 0.5928 LR: 0.001000
Epoch 16 Train: 	[total] 0.2928	[shape] 0.2921	[dice] 0.5932	[l2] 6.3153
  Training time: 88.34s
  Total epoch time: 88.34s (no validation)

Epoch 17/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3356 Dice: 0.5253 LR: 0.001000
  Batch [50/499] Loss: 0.2967 Dice: 0.5872 LR: 0.001000
  Batch [100/499] Loss: 0.2434 Dice: 0.6659 LR: 0.001000
  Batch [150/499] Loss: 0.2484 Dice: 0.6482 LR: 0.001000
  Batch [200/499] Loss: 0.2059 Dice: 0.7159 LR: 0.001000
  Batch [250/499] Loss: 0.3140 Dice: 0.5708 LR: 0.001000
  Batch [300/499] Loss: 0.2598 Dice: 0.6368 LR: 0.001000
  Batch [350/499] Loss: 0.1515 Dice: 0.7923 LR: 0.001000
  Batch [400/499] Loss: 0.3552 Dice: 0.5069 LR: 0.001000
  Batch [450/499] Loss: 0.3000 Dice: 0.5818 LR: 0.001000
Epoch 17 Train: 	[total] 0.2932	[shape] 0.2926	[dice] 0.5926	[l2] 6.3194
  Training time: 88.03s
  Total epoch time: 88.03s (no validation)

Epoch 18/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2846 Dice: 0.5996 LR: 0.001000
  Batch [50/499] Loss: 0.2436 Dice: 0.6613 LR: 0.001000
  Batch [100/499] Loss: 0.3727 Dice: 0.4832 LR: 0.001000
  Batch [150/499] Loss: 0.3306 Dice: 0.5473 LR: 0.001000
  Batch [200/499] Loss: 0.2437 Dice: 0.6592 LR: 0.001000
  Batch [250/499] Loss: 0.2877 Dice: 0.6014 LR: 0.001000
  Batch [300/499] Loss: 0.3404 Dice: 0.5311 LR: 0.001000
  Batch [350/499] Loss: 0.3177 Dice: 0.5643 LR: 0.001000
  Batch [400/499] Loss: 0.3187 Dice: 0.5570 LR: 0.001000
  Batch [450/499] Loss: 0.3492 Dice: 0.5157 LR: 0.001000
Epoch 18 Train: 	[total] 0.2930	[shape] 0.2924	[dice] 0.5928	[l2] 6.3238
  Training time: 88.38s
  Total epoch time: 88.38s (no validation)

Epoch 19/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2649 Dice: 0.6425 LR: 0.001000
  Batch [50/499] Loss: 0.2621 Dice: 0.6334 LR: 0.001000
  Batch [100/499] Loss: 0.3388 Dice: 0.5222 LR: 0.001000
  Batch [150/499] Loss: 0.3823 Dice: 0.4635 LR: 0.001000
  Batch [200/499] Loss: 0.4229 Dice: 0.4070 LR: 0.001000
  Batch [250/499] Loss: 0.2698 Dice: 0.6309 LR: 0.001000
  Batch [300/499] Loss: 0.3994 Dice: 0.4384 LR: 0.001000
  Batch [350/499] Loss: 0.2756 Dice: 0.6063 LR: 0.001000
  Batch [400/499] Loss: 0.2964 Dice: 0.5906 LR: 0.001000
  Batch [450/499] Loss: 0.2534 Dice: 0.6521 LR: 0.001000
Epoch 19 Train: 	[total] 0.2935	[shape] 0.2929	[dice] 0.5920	[l2] 6.3278
  Training time: 88.41s
  Total epoch time: 88.41s (no validation)

Epoch 20/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.1834 Dice: 0.7456 LR: 0.001000
  Batch [50/499] Loss: 0.2586 Dice: 0.6424 LR: 0.001000
  Batch [100/499] Loss: 0.1994 Dice: 0.7209 LR: 0.001000
  Batch [150/499] Loss: 0.2779 Dice: 0.6120 LR: 0.001000
  Batch [200/499] Loss: 0.2670 Dice: 0.6342 LR: 0.001000
  Batch [250/499] Loss: 0.2260 Dice: 0.6913 LR: 0.001000
  Batch [300/499] Loss: 0.2577 Dice: 0.6447 LR: 0.001000
  Batch [350/499] Loss: 0.3340 Dice: 0.5360 LR: 0.001000
  Batch [400/499] Loss: 0.3030 Dice: 0.5791 LR: 0.001000
  Batch [450/499] Loss: 0.3213 Dice: 0.5556 LR: 0.001000
Epoch 20 Train: 	[total] 0.2914	[shape] 0.2907	[dice] 0.5950	[l2] 6.3323
  Training time: 87.96s
Epoch 20 Eval:  	[total] 0.6482	[shape] 0.6476	[dice] 0.0805	[l2] 6.3363
  Validation time: 202.20s
======================================================================
Found a new best model! Shape Loss: 0.647610
======================================================================
  Total epoch time: 290.16s

Epoch 21/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2043 Dice: 0.7128 LR: 0.001000
  Batch [50/499] Loss: 0.2850 Dice: 0.6029 LR: 0.001000
  Batch [100/499] Loss: 0.3244 Dice: 0.5454 LR: 0.001000
  Batch [150/499] Loss: 0.3338 Dice: 0.5424 LR: 0.001000
  Batch [200/499] Loss: 0.3118 Dice: 0.5670 LR: 0.001000
  Batch [250/499] Loss: 0.3631 Dice: 0.5035 LR: 0.001000
  Batch [300/499] Loss: 0.3794 Dice: 0.4768 LR: 0.001000
  Batch [350/499] Loss: 0.2261 Dice: 0.6888 LR: 0.001000
  Batch [400/499] Loss: 0.1795 Dice: 0.7553 LR: 0.001000
  Batch [450/499] Loss: 0.2645 Dice: 0.6343 LR: 0.001000
Epoch 21 Train: 	[total] 0.2905	[shape] 0.2899	[dice] 0.5964	[l2] 6.3365
  Training time: 88.10s
  Total epoch time: 88.10s (no validation)

Epoch 22/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3674 Dice: 0.4871 LR: 0.001000
  Batch [50/499] Loss: 0.2484 Dice: 0.6588 LR: 0.001000
  Batch [100/499] Loss: 0.3166 Dice: 0.5573 LR: 0.001000
  Batch [150/499] Loss: 0.3318 Dice: 0.5317 LR: 0.001000
  Batch [200/499] Loss: 0.3083 Dice: 0.5698 LR: 0.001000
  Batch [250/499] Loss: 0.3486 Dice: 0.5188 LR: 0.001000
  Batch [300/499] Loss: 0.2661 Dice: 0.6376 LR: 0.001000
  Batch [350/499] Loss: 0.2584 Dice: 0.6484 LR: 0.001000
  Batch [400/499] Loss: 0.2479 Dice: 0.6578 LR: 0.001000
  Batch [450/499] Loss: 0.2528 Dice: 0.6482 LR: 0.001000
Epoch 22 Train: 	[total] 0.2894	[shape] 0.2888	[dice] 0.5981	[l2] 6.3407
  Training time: 88.09s
  Total epoch time: 88.09s (no validation)

Epoch 23/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3506 Dice: 0.5103 LR: 0.001000
  Batch [50/499] Loss: 0.3677 Dice: 0.4904 LR: 0.001000
  Batch [100/499] Loss: 0.2969 Dice: 0.5959 LR: 0.001000
  Batch [150/499] Loss: 0.2364 Dice: 0.6646 LR: 0.001000
  Batch [200/499] Loss: 0.2813 Dice: 0.6118 LR: 0.001000
  Batch [250/499] Loss: 0.3097 Dice: 0.5638 LR: 0.001000
  Batch [300/499] Loss: 0.2651 Dice: 0.6319 LR: 0.001000
  Batch [350/499] Loss: 0.4600 Dice: 0.3553 LR: 0.001000
  Batch [400/499] Loss: 0.3386 Dice: 0.5282 LR: 0.001000
  Batch [450/499] Loss: 0.3652 Dice: 0.4869 LR: 0.001000
Epoch 23 Train: 	[total] 0.2877	[shape] 0.2871	[dice] 0.6002	[l2] 6.3445
  Training time: 87.94s
  Total epoch time: 87.94s (no validation)

Epoch 24/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3445 Dice: 0.5181 LR: 0.001000
  Batch [50/499] Loss: 0.3622 Dice: 0.4977 LR: 0.001000
  Batch [100/499] Loss: 0.4323 Dice: 0.3935 LR: 0.001000
  Batch [150/499] Loss: 0.2884 Dice: 0.5964 LR: 0.001000
  Batch [200/499] Loss: 0.2905 Dice: 0.5916 LR: 0.001000
  Batch [250/499] Loss: 0.3912 Dice: 0.4532 LR: 0.001000
  Batch [300/499] Loss: 0.2723 Dice: 0.6163 LR: 0.001000
  Batch [350/499] Loss: 0.2540 Dice: 0.6526 LR: 0.001000
  Batch [400/499] Loss: 0.2727 Dice: 0.6119 LR: 0.001000
  Batch [450/499] Loss: 0.2162 Dice: 0.6955 LR: 0.001000
Epoch 24 Train: 	[total] 0.2890	[shape] 0.2884	[dice] 0.5983	[l2] 6.3483
  Training time: 87.97s
  Total epoch time: 87.97s (no validation)

Epoch 25/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2650 Dice: 0.6420 LR: 0.001000
  Batch [50/499] Loss: 0.2658 Dice: 0.6279 LR: 0.001000
  Batch [100/499] Loss: 0.3520 Dice: 0.5151 LR: 0.001000
  Batch [150/499] Loss: 0.3137 Dice: 0.5655 LR: 0.001000
  Batch [200/499] Loss: 0.2557 Dice: 0.6439 LR: 0.001000
  Batch [250/499] Loss: 0.1977 Dice: 0.7314 LR: 0.001000
  Batch [300/499] Loss: 0.3069 Dice: 0.5712 LR: 0.001000
  Batch [350/499] Loss: 0.2814 Dice: 0.6104 LR: 0.001000
  Batch [400/499] Loss: 0.3018 Dice: 0.5837 LR: 0.001000
  Batch [450/499] Loss: 0.3906 Dice: 0.4576 LR: 0.001000
Epoch 25 Train: 	[total] 0.2870	[shape] 0.2863	[dice] 0.6014	[l2] 6.3525
  Training time: 87.87s
  Total epoch time: 87.87s (no validation)

Epoch 26/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2663 Dice: 0.6291 LR: 0.001000
  Batch [50/499] Loss: 0.3666 Dice: 0.4886 LR: 0.001000
  Batch [100/499] Loss: 0.3339 Dice: 0.5307 LR: 0.001000
  Batch [150/499] Loss: 0.3743 Dice: 0.4824 LR: 0.001000
  Batch [200/499] Loss: 0.2214 Dice: 0.6940 LR: 0.001000
  Batch [250/499] Loss: 0.2106 Dice: 0.7075 LR: 0.001000
  Batch [300/499] Loss: 0.2704 Dice: 0.6274 LR: 0.001000
  Batch [350/499] Loss: 0.2985 Dice: 0.5844 LR: 0.001000
  Batch [400/499] Loss: 0.3320 Dice: 0.5367 LR: 0.001000
  Batch [450/499] Loss: 0.3051 Dice: 0.5782 LR: 0.001000
Epoch 26 Train: 	[total] 0.2885	[shape] 0.2879	[dice] 0.5992	[l2] 6.3565
  Training time: 87.73s
  Total epoch time: 87.73s (no validation)

Epoch 27/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2161 Dice: 0.6985 LR: 0.001000
  Batch [50/499] Loss: 0.2017 Dice: 0.7270 LR: 0.001000
  Batch [100/499] Loss: 0.2440 Dice: 0.6663 LR: 0.001000
  Batch [150/499] Loss: 0.3437 Dice: 0.5212 LR: 0.001000
  Batch [200/499] Loss: 0.1962 Dice: 0.7312 LR: 0.001000
  Batch [250/499] Loss: 0.3581 Dice: 0.5003 LR: 0.001000
  Batch [300/499] Loss: 0.2832 Dice: 0.6007 LR: 0.001000
  Batch [350/499] Loss: 0.3349 Dice: 0.5299 LR: 0.001000
  Batch [400/499] Loss: 0.3185 Dice: 0.5493 LR: 0.001000
  Batch [450/499] Loss: 0.4278 Dice: 0.4009 LR: 0.001000
Epoch 27 Train: 	[total] 0.2890	[shape] 0.2883	[dice] 0.5986	[l2] 6.3606
  Training time: 88.13s
  Total epoch time: 88.13s (no validation)

Epoch 28/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3317 Dice: 0.5370 LR: 0.001000
  Batch [50/499] Loss: 0.3880 Dice: 0.4527 LR: 0.001000
  Batch [100/499] Loss: 0.2352 Dice: 0.6748 LR: 0.001000
  Batch [150/499] Loss: 0.2895 Dice: 0.5945 LR: 0.001000
  Batch [200/499] Loss: 0.3915 Dice: 0.4519 LR: 0.001000
  Batch [250/499] Loss: 0.2329 Dice: 0.6793 LR: 0.001000
  Batch [300/499] Loss: 0.2471 Dice: 0.6570 LR: 0.001000
  Batch [350/499] Loss: 0.1809 Dice: 0.7567 LR: 0.001000
  Batch [400/499] Loss: 0.3679 Dice: 0.4925 LR: 0.001000
  Batch [450/499] Loss: 0.3180 Dice: 0.5686 LR: 0.001000
Epoch 28 Train: 	[total] 0.2882	[shape] 0.2875	[dice] 0.5998	[l2] 6.3648
  Training time: 88.35s
  Total epoch time: 88.35s (no validation)

Epoch 29/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2393 Dice: 0.6640 LR: 0.001000
  Batch [50/499] Loss: 0.4044 Dice: 0.4411 LR: 0.001000
  Batch [100/499] Loss: 0.3482 Dice: 0.5171 LR: 0.001000
  Batch [150/499] Loss: 0.3454 Dice: 0.5191 LR: 0.001000
  Batch [200/499] Loss: 0.2535 Dice: 0.6528 LR: 0.001000
  Batch [250/499] Loss: 0.2919 Dice: 0.5993 LR: 0.001000
  Batch [300/499] Loss: 0.3378 Dice: 0.5289 LR: 0.001000
  Batch [350/499] Loss: 0.2726 Dice: 0.6267 LR: 0.001000
  Batch [400/499] Loss: 0.2931 Dice: 0.5920 LR: 0.001000
  Batch [450/499] Loss: 0.2828 Dice: 0.6043 LR: 0.001000
Epoch 29 Train: 	[total] 0.2846	[shape] 0.2840	[dice] 0.6048	[l2] 6.3688
  Training time: 87.92s
  Total epoch time: 87.92s (no validation)

Epoch 30/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3066 Dice: 0.5722 LR: 0.001000
  Batch [50/499] Loss: 0.3232 Dice: 0.5489 LR: 0.001000
  Batch [100/499] Loss: 0.2625 Dice: 0.6317 LR: 0.001000
  Batch [150/499] Loss: 0.2728 Dice: 0.6194 LR: 0.001000
  Batch [200/499] Loss: 0.3359 Dice: 0.5315 LR: 0.001000
  Batch [250/499] Loss: 0.4047 Dice: 0.4310 LR: 0.001000
  Batch [300/499] Loss: 0.2629 Dice: 0.6357 LR: 0.001000
  Batch [350/499] Loss: 0.2178 Dice: 0.6960 LR: 0.001000
  Batch [400/499] Loss: 0.2829 Dice: 0.6123 LR: 0.001000
  Batch [450/499] Loss: 0.2607 Dice: 0.6387 LR: 0.001000
Epoch 30 Train: 	[total] 0.2860	[shape] 0.2854	[dice] 0.6026	[l2] 6.3729
  Training time: 87.91s
Epoch 30 Eval:  	[total] 0.6464	[shape] 0.6458	[dice] 0.0836	[l2] 6.3767
  Validation time: 202.79s
======================================================================
Found a new best model! Shape Loss: 0.645758
======================================================================
  Total epoch time: 290.70s

Epoch 31/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2252 Dice: 0.6880 LR: 0.001000
  Batch [50/499] Loss: 0.3756 Dice: 0.4682 LR: 0.001000
  Batch [100/499] Loss: 0.2004 Dice: 0.7268 LR: 0.001000
  Batch [150/499] Loss: 0.2170 Dice: 0.6929 LR: 0.001000
  Batch [200/499] Loss: 0.4137 Dice: 0.4302 LR: 0.001000
  Batch [250/499] Loss: 0.2515 Dice: 0.6557 LR: 0.001000
  Batch [300/499] Loss: 0.2739 Dice: 0.6243 LR: 0.001000
  Batch [350/499] Loss: 0.2803 Dice: 0.6131 LR: 0.001000
  Batch [400/499] Loss: 0.2644 Dice: 0.6338 LR: 0.001000
  Batch [450/499] Loss: 0.2132 Dice: 0.7051 LR: 0.001000
Epoch 31 Train: 	[total] 0.2857	[shape] 0.2850	[dice] 0.6035	[l2] 6.3770
  Training time: 88.14s
  Total epoch time: 88.14s (no validation)

Epoch 32/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3017 Dice: 0.5821 LR: 0.001000
  Batch [50/499] Loss: 0.2268 Dice: 0.6841 LR: 0.001000
  Batch [100/499] Loss: 0.3490 Dice: 0.5165 LR: 0.001000
  Batch [150/499] Loss: 0.2879 Dice: 0.6009 LR: 0.001000
  Batch [200/499] Loss: 0.2716 Dice: 0.6282 LR: 0.001000
  Batch [250/499] Loss: 0.2625 Dice: 0.6354 LR: 0.001000
  Batch [300/499] Loss: 0.2611 Dice: 0.6343 LR: 0.001000
  Batch [350/499] Loss: 0.1997 Dice: 0.7321 LR: 0.001000
  Batch [400/499] Loss: 0.3347 Dice: 0.5351 LR: 0.001000
  Batch [450/499] Loss: 0.3335 Dice: 0.5317 LR: 0.001000
Epoch 32 Train: 	[total] 0.2861	[shape] 0.2854	[dice] 0.6026	[l2] 6.3809
  Training time: 87.62s
  Total epoch time: 87.62s (no validation)

Epoch 33/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2137 Dice: 0.7004 LR: 0.001000
  Batch [50/499] Loss: 0.2442 Dice: 0.6621 LR: 0.001000
  Batch [100/499] Loss: 0.3122 Dice: 0.5652 LR: 0.001000
  Batch [150/499] Loss: 0.3351 Dice: 0.5334 LR: 0.001000
  Batch [200/499] Loss: 0.2314 Dice: 0.6829 LR: 0.001000
  Batch [250/499] Loss: 0.2951 Dice: 0.5925 LR: 0.001000
  Batch [300/499] Loss: 0.2992 Dice: 0.5843 LR: 0.001000
  Batch [350/499] Loss: 0.2664 Dice: 0.6356 LR: 0.001000
  Batch [400/499] Loss: 0.2668 Dice: 0.6392 LR: 0.001000
  Batch [450/499] Loss: 0.2951 Dice: 0.5941 LR: 0.001000
Epoch 33 Train: 	[total] 0.2829	[shape] 0.2822	[dice] 0.6073	[l2] 6.3850
  Training time: 87.95s
  Total epoch time: 87.95s (no validation)

Epoch 34/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2167 Dice: 0.6998 LR: 0.001000
  Batch [50/499] Loss: 0.2646 Dice: 0.6248 LR: 0.001000
  Batch [100/499] Loss: 0.3665 Dice: 0.4830 LR: 0.001000
  Batch [150/499] Loss: 0.3410 Dice: 0.5261 LR: 0.001000
  Batch [200/499] Loss: 0.3030 Dice: 0.5770 LR: 0.001000
  Batch [250/499] Loss: 0.3731 Dice: 0.4808 LR: 0.001000
  Batch [300/499] Loss: 0.3340 Dice: 0.5357 LR: 0.001000
  Batch [350/499] Loss: 0.1566 Dice: 0.7865 LR: 0.001000
  Batch [400/499] Loss: 0.2652 Dice: 0.6310 LR: 0.001000
  Batch [450/499] Loss: 0.3460 Dice: 0.5112 LR: 0.001000
Epoch 34 Train: 	[total] 0.2829	[shape] 0.2823	[dice] 0.6071	[l2] 6.3893
  Training time: 87.62s
  Total epoch time: 87.62s (no validation)

Epoch 35/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2716 Dice: 0.6209 LR: 0.001000
  Batch [50/499] Loss: 0.1921 Dice: 0.7345 LR: 0.001000
  Batch [100/499] Loss: 0.3538 Dice: 0.5055 LR: 0.001000
  Batch [150/499] Loss: 0.2689 Dice: 0.6325 LR: 0.001000
  Batch [200/499] Loss: 0.3455 Dice: 0.5162 LR: 0.001000
  Batch [250/499] Loss: 0.2541 Dice: 0.6395 LR: 0.001000
  Batch [300/499] Loss: 0.3824 Dice: 0.4698 LR: 0.001000
  Batch [350/499] Loss: 0.2771 Dice: 0.6053 LR: 0.001000
  Batch [400/499] Loss: 0.3480 Dice: 0.5149 LR: 0.001000
  Batch [450/499] Loss: 0.3350 Dice: 0.5340 LR: 0.001000
Epoch 35 Train: 	[total] 0.2814	[shape] 0.2808	[dice] 0.6093	[l2] 6.3930
  Training time: 87.76s
  Total epoch time: 87.76s (no validation)

Epoch 36/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3197 Dice: 0.5562 LR: 0.001000
  Batch [50/499] Loss: 0.2525 Dice: 0.6482 LR: 0.001000
  Batch [100/499] Loss: 0.2300 Dice: 0.6874 LR: 0.001000
  Batch [150/499] Loss: 0.3758 Dice: 0.4741 LR: 0.001000
  Batch [200/499] Loss: 0.3001 Dice: 0.5848 LR: 0.001000
  Batch [250/499] Loss: 0.2927 Dice: 0.5928 LR: 0.001000
  Batch [300/499] Loss: 0.2909 Dice: 0.6033 LR: 0.001000
  Batch [350/499] Loss: 0.3295 Dice: 0.5471 LR: 0.001000
  Batch [400/499] Loss: 0.2412 Dice: 0.6684 LR: 0.001000
  Batch [450/499] Loss: 0.2137 Dice: 0.7050 LR: 0.001000
Epoch 36 Train: 	[total] 0.2829	[shape] 0.2823	[dice] 0.6072	[l2] 6.3970
  Training time: 88.11s
  Total epoch time: 88.11s (no validation)

Epoch 37/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2696 Dice: 0.6325 LR: 0.001000
  Batch [50/499] Loss: 0.2537 Dice: 0.6467 LR: 0.001000
  Batch [100/499] Loss: 0.3477 Dice: 0.5101 LR: 0.001000
  Batch [150/499] Loss: 0.2786 Dice: 0.6172 LR: 0.001000
  Batch [200/499] Loss: 0.3241 Dice: 0.5486 LR: 0.001000
  Batch [250/499] Loss: 0.2529 Dice: 0.6540 LR: 0.001000
  Batch [300/499] Loss: 0.3068 Dice: 0.5723 LR: 0.001000
  Batch [350/499] Loss: 0.3251 Dice: 0.5450 LR: 0.001000
  Batch [400/499] Loss: 0.4099 Dice: 0.4245 LR: 0.001000
  Batch [450/499] Loss: 0.2374 Dice: 0.6720 LR: 0.001000
Epoch 37 Train: 	[total] 0.2826	[shape] 0.2820	[dice] 0.6074	[l2] 6.4010
  Training time: 88.06s
  Total epoch time: 88.06s (no validation)

Epoch 38/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2714 Dice: 0.6237 LR: 0.001000
  Batch [50/499] Loss: 0.2976 Dice: 0.5952 LR: 0.001000
  Batch [100/499] Loss: 0.2686 Dice: 0.6266 LR: 0.001000
  Batch [150/499] Loss: 0.2450 Dice: 0.6537 LR: 0.001000
  Batch [200/499] Loss: 0.2579 Dice: 0.6428 LR: 0.001000
  Batch [250/499] Loss: 0.2970 Dice: 0.5860 LR: 0.001000
  Batch [300/499] Loss: 0.2318 Dice: 0.6731 LR: 0.001000
  Batch [350/499] Loss: 0.3618 Dice: 0.4954 LR: 0.001000
  Batch [400/499] Loss: 0.2949 Dice: 0.5828 LR: 0.001000
  Batch [450/499] Loss: 0.2247 Dice: 0.6889 LR: 0.001000
Epoch 38 Train: 	[total] 0.2798	[shape] 0.2791	[dice] 0.6114	[l2] 6.4051
  Training time: 87.79s
  Total epoch time: 87.79s (no validation)

Epoch 39/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3090 Dice: 0.5690 LR: 0.001000
  Batch [50/499] Loss: 0.2353 Dice: 0.6683 LR: 0.001000
  Batch [100/499] Loss: 0.3415 Dice: 0.5264 LR: 0.001000
  Batch [150/499] Loss: 0.2700 Dice: 0.6236 LR: 0.001000
  Batch [200/499] Loss: 0.3005 Dice: 0.5844 LR: 0.001000
  Batch [250/499] Loss: 0.2275 Dice: 0.6794 LR: 0.001000
  Batch [300/499] Loss: 0.2721 Dice: 0.6223 LR: 0.001000
  Batch [350/499] Loss: 0.2039 Dice: 0.7192 LR: 0.001000
  Batch [400/499] Loss: 0.3983 Dice: 0.4349 LR: 0.001000
  Batch [450/499] Loss: 0.2695 Dice: 0.6281 LR: 0.001000
Epoch 39 Train: 	[total] 0.2834	[shape] 0.2827	[dice] 0.6064	[l2] 6.4092
  Training time: 87.91s
  Total epoch time: 87.91s (no validation)

Epoch 40/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2591 Dice: 0.6400 LR: 0.001000
  Batch [50/499] Loss: 0.3251 Dice: 0.5502 LR: 0.001000
  Batch [100/499] Loss: 0.2915 Dice: 0.5898 LR: 0.001000
  Batch [150/499] Loss: 0.1936 Dice: 0.7304 LR: 0.001000
  Batch [200/499] Loss: 0.3094 Dice: 0.5738 LR: 0.001000
  Batch [250/499] Loss: 0.3054 Dice: 0.5691 LR: 0.001000
  Batch [300/499] Loss: 0.2966 Dice: 0.5907 LR: 0.001000
  Batch [350/499] Loss: 0.2437 Dice: 0.6614 LR: 0.001000
  Batch [400/499] Loss: 0.2747 Dice: 0.6183 LR: 0.001000
  Batch [450/499] Loss: 0.3474 Dice: 0.5246 LR: 0.001000
Epoch 40 Train: 	[total] 0.2824	[shape] 0.2818	[dice] 0.6077	[l2] 6.4133
  Training time: 87.97s
Epoch 40 Eval:  	[total] 0.6537	[shape] 0.6530	[dice] 0.0726	[l2] 6.4171
  Validation time: 202.68s
  Total epoch time: 290.66s

Epoch 41/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3675 Dice: 0.4876 LR: 0.001000
  Batch [50/499] Loss: 0.3256 Dice: 0.5509 LR: 0.001000
  Batch [100/499] Loss: 0.3549 Dice: 0.5022 LR: 0.001000
  Batch [150/499] Loss: 0.3979 Dice: 0.4589 LR: 0.001000
  Batch [200/499] Loss: 0.2588 Dice: 0.6412 LR: 0.001000
  Batch [250/499] Loss: 0.3643 Dice: 0.4955 LR: 0.001000
  Batch [300/499] Loss: 0.3487 Dice: 0.5136 LR: 0.001000
  Batch [350/499] Loss: 0.2354 Dice: 0.6734 LR: 0.001000
  Batch [400/499] Loss: 0.2015 Dice: 0.7207 LR: 0.001000
  Batch [450/499] Loss: 0.1809 Dice: 0.7494 LR: 0.001000
Epoch 41 Train: 	[total] 0.2809	[shape] 0.2803	[dice] 0.6099	[l2] 6.4174
  Training time: 88.01s
  Total epoch time: 88.01s (no validation)

Epoch 42/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2325 Dice: 0.6755 LR: 0.001000
  Batch [50/499] Loss: 0.2108 Dice: 0.7146 LR: 0.001000
  Batch [100/499] Loss: 0.2606 Dice: 0.6409 LR: 0.001000
  Batch [150/499] Loss: 0.3455 Dice: 0.5142 LR: 0.001000
  Batch [200/499] Loss: 0.2889 Dice: 0.6037 LR: 0.001000
  Batch [250/499] Loss: 0.3139 Dice: 0.5591 LR: 0.001000
  Batch [300/499] Loss: 0.2764 Dice: 0.6075 LR: 0.001000
  Batch [350/499] Loss: 0.3790 Dice: 0.4724 LR: 0.001000
  Batch [400/499] Loss: 0.1646 Dice: 0.7792 LR: 0.001000
  Batch [450/499] Loss: 0.2703 Dice: 0.6281 LR: 0.001000
Epoch 42 Train: 	[total] 0.2802	[shape] 0.2795	[dice] 0.6111	[l2] 6.4213
  Training time: 87.97s
  Total epoch time: 87.97s (no validation)

Epoch 43/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3256 Dice: 0.5417 LR: 0.001000
  Batch [50/499] Loss: 0.2385 Dice: 0.6685 LR: 0.001000
  Batch [100/499] Loss: 0.2913 Dice: 0.5981 LR: 0.001000
  Batch [150/499] Loss: 0.2504 Dice: 0.6506 LR: 0.001000
  Batch [200/499] Loss: 0.2305 Dice: 0.6761 LR: 0.001000
  Batch [250/499] Loss: 0.1799 Dice: 0.7482 LR: 0.001000
  Batch [300/499] Loss: 0.2538 Dice: 0.6445 LR: 0.001000
  Batch [350/499] Loss: 0.3470 Dice: 0.5085 LR: 0.001000
  Batch [400/499] Loss: 0.3168 Dice: 0.5523 LR: 0.001000
  Batch [450/499] Loss: 0.3071 Dice: 0.5702 LR: 0.001000
Epoch 43 Train: 	[total] 0.2780	[shape] 0.2774	[dice] 0.6137	[l2] 6.4253
  Training time: 88.07s
  Total epoch time: 88.07s (no validation)

Epoch 44/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.4359 Dice: 0.3848 LR: 0.001000
  Batch [50/499] Loss: 0.2549 Dice: 0.6411 LR: 0.001000
  Batch [100/499] Loss: 0.2394 Dice: 0.6682 LR: 0.001000
  Batch [150/499] Loss: 0.2261 Dice: 0.6866 LR: 0.001000
  Batch [200/499] Loss: 0.3371 Dice: 0.5294 LR: 0.001000
  Batch [250/499] Loss: 0.2593 Dice: 0.6389 LR: 0.001000
  Batch [300/499] Loss: 0.2148 Dice: 0.7020 LR: 0.001000
  Batch [350/499] Loss: 0.2638 Dice: 0.6299 LR: 0.001000
  Batch [400/499] Loss: 0.3872 Dice: 0.4580 LR: 0.001000
  Batch [450/499] Loss: 0.3434 Dice: 0.5229 LR: 0.001000
Epoch 44 Train: 	[total] 0.2802	[shape] 0.2796	[dice] 0.6108	[l2] 6.4293
  Training time: 88.18s
  Total epoch time: 88.18s (no validation)

Epoch 45/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2607 Dice: 0.6429 LR: 0.001000
  Batch [50/499] Loss: 0.2136 Dice: 0.7019 LR: 0.001000
  Batch [100/499] Loss: 0.4279 Dice: 0.4027 LR: 0.001000
  Batch [150/499] Loss: 0.2510 Dice: 0.6480 LR: 0.001000
  Batch [200/499] Loss: 0.2269 Dice: 0.6845 LR: 0.001000
  Batch [250/499] Loss: 0.2572 Dice: 0.6467 LR: 0.001000
  Batch [300/499] Loss: 0.2324 Dice: 0.6798 LR: 0.001000
  Batch [350/499] Loss: 0.3302 Dice: 0.5464 LR: 0.001000
  Batch [400/499] Loss: 0.2833 Dice: 0.6088 LR: 0.001000
  Batch [450/499] Loss: 0.2453 Dice: 0.6638 LR: 0.001000
Epoch 45 Train: 	[total] 0.2782	[shape] 0.2776	[dice] 0.6136	[l2] 6.4332
  Training time: 87.95s
  Total epoch time: 87.95s (no validation)

Epoch 46/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.1789 Dice: 0.7554 LR: 0.001000
  Batch [50/499] Loss: 0.3409 Dice: 0.5342 LR: 0.001000
  Batch [100/499] Loss: 0.2702 Dice: 0.6226 LR: 0.001000
  Batch [150/499] Loss: 0.3078 Dice: 0.5623 LR: 0.001000
  Batch [200/499] Loss: 0.3659 Dice: 0.4864 LR: 0.001000
  Batch [250/499] Loss: 0.3044 Dice: 0.5722 LR: 0.001000
  Batch [300/499] Loss: 0.2748 Dice: 0.6214 LR: 0.001000
  Batch [350/499] Loss: 0.3143 Dice: 0.5630 LR: 0.001000
  Batch [400/499] Loss: 0.2274 Dice: 0.6867 LR: 0.001000
  Batch [450/499] Loss: 0.3557 Dice: 0.5058 LR: 0.001000
Epoch 46 Train: 	[total] 0.2780	[shape] 0.2773	[dice] 0.6141	[l2] 6.4371
  Training time: 88.09s
  Total epoch time: 88.09s (no validation)

Epoch 47/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.1955 Dice: 0.7345 LR: 0.001000
  Batch [50/499] Loss: 0.2460 Dice: 0.6622 LR: 0.001000
  Batch [100/499] Loss: 0.2040 Dice: 0.7106 LR: 0.001000
  Batch [150/499] Loss: 0.3147 Dice: 0.5642 LR: 0.001000
  Batch [200/499] Loss: 0.3098 Dice: 0.5729 LR: 0.001000
  Batch [250/499] Loss: 0.2379 Dice: 0.6652 LR: 0.001000
  Batch [300/499] Loss: 0.2764 Dice: 0.6149 LR: 0.001000
  Batch [350/499] Loss: 0.2258 Dice: 0.6877 LR: 0.001000
  Batch [400/499] Loss: 0.2285 Dice: 0.6876 LR: 0.001000
  Batch [450/499] Loss: 0.2519 Dice: 0.6537 LR: 0.001000
Epoch 47 Train: 	[total] 0.2771	[shape] 0.2765	[dice] 0.6154	[l2] 6.4408
  Training time: 87.96s
  Total epoch time: 87.96s (no validation)

Epoch 48/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2593 Dice: 0.6453 LR: 0.001000
  Batch [50/499] Loss: 0.2415 Dice: 0.6624 LR: 0.001000
  Batch [100/499] Loss: 0.3271 Dice: 0.5429 LR: 0.001000
  Batch [150/499] Loss: 0.3040 Dice: 0.5790 LR: 0.001000
  Batch [200/499] Loss: 0.3255 Dice: 0.5471 LR: 0.001000
  Batch [250/499] Loss: 0.3701 Dice: 0.4843 LR: 0.001000
  Batch [300/499] Loss: 0.4167 Dice: 0.4187 LR: 0.001000
  Batch [350/499] Loss: 0.3195 Dice: 0.5529 LR: 0.001000
  Batch [400/499] Loss: 0.1986 Dice: 0.7265 LR: 0.001000
  Batch [450/499] Loss: 0.2470 Dice: 0.6550 LR: 0.001000
Epoch 48 Train: 	[total] 0.2772	[shape] 0.2766	[dice] 0.6152	[l2] 6.4450
  Training time: 87.95s
  Total epoch time: 87.95s (no validation)

Epoch 49/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2365 Dice: 0.6709 LR: 0.001000
  Batch [50/499] Loss: 0.2231 Dice: 0.6859 LR: 0.001000
  Batch [100/499] Loss: 0.2851 Dice: 0.6034 LR: 0.001000
  Batch [150/499] Loss: 0.2148 Dice: 0.7057 LR: 0.001000
  Batch [200/499] Loss: 0.3559 Dice: 0.5061 LR: 0.001000
  Batch [250/499] Loss: 0.2935 Dice: 0.5895 LR: 0.001000
  Batch [300/499] Loss: 0.2862 Dice: 0.6044 LR: 0.001000
  Batch [350/499] Loss: 0.3198 Dice: 0.5457 LR: 0.001000
  Batch [400/499] Loss: 0.2096 Dice: 0.7055 LR: 0.001000
  Batch [450/499] Loss: 0.2184 Dice: 0.6901 LR: 0.001000
Epoch 49 Train: 	[total] 0.2767	[shape] 0.2760	[dice] 0.6157	[l2] 6.4489
  Training time: 88.24s
  Total epoch time: 88.24s (no validation)

Epoch 50/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2942 Dice: 0.5923 LR: 0.001000
  Batch [50/499] Loss: 0.1798 Dice: 0.7506 LR: 0.001000
  Batch [100/499] Loss: 0.2518 Dice: 0.6498 LR: 0.001000
  Batch [150/499] Loss: 0.2618 Dice: 0.6342 LR: 0.001000
  Batch [200/499] Loss: 0.2618 Dice: 0.6414 LR: 0.001000
  Batch [250/499] Loss: 0.3305 Dice: 0.5354 LR: 0.001000
  Batch [300/499] Loss: 0.3015 Dice: 0.5856 LR: 0.001000
  Batch [350/499] Loss: 0.2918 Dice: 0.5994 LR: 0.001000
  Batch [400/499] Loss: 0.3031 Dice: 0.5721 LR: 0.001000
  Batch [450/499] Loss: 0.2748 Dice: 0.6136 LR: 0.001000
Epoch 50 Train: 	[total] 0.2772	[shape] 0.2766	[dice] 0.6153	[l2] 6.4530
  Training time: 88.12s
Epoch 50 Eval:  	[total] 0.6511	[shape] 0.6504	[dice] 0.0765	[l2] 6.4567
  Validation time: 202.92s
  Total epoch time: 291.04s

Epoch 51/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2359 Dice: 0.6744 LR: 0.001000
  Batch [50/499] Loss: 0.2499 Dice: 0.6555 LR: 0.001000
  Batch [100/499] Loss: 0.2302 Dice: 0.6850 LR: 0.001000
  Batch [150/499] Loss: 0.3213 Dice: 0.5545 LR: 0.001000
  Batch [200/499] Loss: 0.2978 Dice: 0.5843 LR: 0.001000
  Batch [250/499] Loss: 0.3458 Dice: 0.5202 LR: 0.001000
  Batch [300/499] Loss: 0.3154 Dice: 0.5617 LR: 0.001000
  Batch [350/499] Loss: 0.2935 Dice: 0.5936 LR: 0.001000
  Batch [400/499] Loss: 0.2622 Dice: 0.6345 LR: 0.001000
  Batch [450/499] Loss: 0.2457 Dice: 0.6547 LR: 0.001000
Epoch 51 Train: 	[total] 0.2764	[shape] 0.2758	[dice] 0.6163	[l2] 6.4569
  Training time: 87.95s
  Total epoch time: 87.95s (no validation)

Epoch 52/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2997 Dice: 0.5848 LR: 0.001000
  Batch [50/499] Loss: 0.2552 Dice: 0.6589 LR: 0.001000
  Batch [100/499] Loss: 0.2812 Dice: 0.6103 LR: 0.001000
  Batch [150/499] Loss: 0.1747 Dice: 0.7629 LR: 0.001000
  Batch [200/499] Loss: 0.2477 Dice: 0.6533 LR: 0.001000
  Batch [250/499] Loss: 0.2255 Dice: 0.6858 LR: 0.001000
  Batch [300/499] Loss: 0.2425 Dice: 0.6638 LR: 0.001000
  Batch [350/499] Loss: 0.3682 Dice: 0.4950 LR: 0.001000
  Batch [400/499] Loss: 0.3067 Dice: 0.5814 LR: 0.001000
  Batch [450/499] Loss: 0.3245 Dice: 0.5492 LR: 0.001000
Epoch 52 Train: 	[total] 0.2753	[shape] 0.2747	[dice] 0.6178	[l2] 6.4608
  Training time: 88.03s
  Total epoch time: 88.03s (no validation)

Epoch 53/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2593 Dice: 0.6448 LR: 0.001000
  Batch [50/499] Loss: 0.2624 Dice: 0.6351 LR: 0.001000
  Batch [100/499] Loss: 0.2850 Dice: 0.6071 LR: 0.001000
  Batch [150/499] Loss: 0.4159 Dice: 0.4302 LR: 0.001000
  Batch [200/499] Loss: 0.2638 Dice: 0.6354 LR: 0.001000
  Batch [250/499] Loss: 0.3027 Dice: 0.5832 LR: 0.001000
  Batch [300/499] Loss: 0.2506 Dice: 0.6515 LR: 0.001000
  Batch [350/499] Loss: 0.2609 Dice: 0.6369 LR: 0.001000
  Batch [400/499] Loss: 0.2782 Dice: 0.6096 LR: 0.001000
  Batch [450/499] Loss: 0.2791 Dice: 0.6185 LR: 0.001000
Epoch 53 Train: 	[total] 0.2759	[shape] 0.2753	[dice] 0.6166	[l2] 6.4646
  Training time: 87.84s
  Total epoch time: 87.84s (no validation)

Epoch 54/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2263 Dice: 0.6907 LR: 0.001000
  Batch [50/499] Loss: 0.2608 Dice: 0.6354 LR: 0.001000
  Batch [100/499] Loss: 0.2458 Dice: 0.6545 LR: 0.001000
  Batch [150/499] Loss: 0.2331 Dice: 0.6711 LR: 0.001000
  Batch [200/499] Loss: 0.1844 Dice: 0.7481 LR: 0.001000
  Batch [250/499] Loss: 0.3287 Dice: 0.5474 LR: 0.001000
  Batch [300/499] Loss: 0.2247 Dice: 0.6853 LR: 0.001000
  Batch [350/499] Loss: 0.2748 Dice: 0.6203 LR: 0.001000
  Batch [400/499] Loss: 0.1841 Dice: 0.7481 LR: 0.001000
  Batch [450/499] Loss: 0.3272 Dice: 0.5441 LR: 0.001000
Epoch 54 Train: 	[total] 0.2742	[shape] 0.2735	[dice] 0.6192	[l2] 6.4684
  Training time: 87.83s
  Total epoch time: 87.83s (no validation)

Epoch 55/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3476 Dice: 0.5096 LR: 0.001000
  Batch [50/499] Loss: 0.2293 Dice: 0.6731 LR: 0.001000
  Batch [100/499] Loss: 0.1870 Dice: 0.7428 LR: 0.001000
  Batch [150/499] Loss: 0.2355 Dice: 0.6748 LR: 0.001000
  Batch [200/499] Loss: 0.2168 Dice: 0.7009 LR: 0.001000
  Batch [250/499] Loss: 0.3198 Dice: 0.5497 LR: 0.001000
  Batch [300/499] Loss: 0.1675 Dice: 0.7725 LR: 0.001000
  Batch [350/499] Loss: 0.3509 Dice: 0.5086 LR: 0.001000
  Batch [400/499] Loss: 0.2145 Dice: 0.7019 LR: 0.001000
  Batch [450/499] Loss: 0.3241 Dice: 0.5545 LR: 0.001000
Epoch 55 Train: 	[total] 0.2751	[shape] 0.2745	[dice] 0.6180	[l2] 6.4723
  Training time: 87.76s
  Total epoch time: 87.76s (no validation)

Epoch 56/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2175 Dice: 0.6970 LR: 0.001000
  Batch [50/499] Loss: 0.3173 Dice: 0.5564 LR: 0.001000
  Batch [100/499] Loss: 0.2393 Dice: 0.6682 LR: 0.001000
  Batch [150/499] Loss: 0.2154 Dice: 0.7053 LR: 0.001000
  Batch [200/499] Loss: 0.2745 Dice: 0.6214 LR: 0.001000
  Batch [250/499] Loss: 0.2781 Dice: 0.6162 LR: 0.001000
  Batch [300/499] Loss: 0.2781 Dice: 0.6115 LR: 0.001000
  Batch [350/499] Loss: 0.1922 Dice: 0.7370 LR: 0.001000
  Batch [400/499] Loss: 0.2347 Dice: 0.6750 LR: 0.001000
  Batch [450/499] Loss: 0.2529 Dice: 0.6433 LR: 0.001000
Epoch 56 Train: 	[total] 0.2746	[shape] 0.2740	[dice] 0.6183	[l2] 6.4761
  Training time: 87.92s
  Total epoch time: 87.92s (no validation)

Epoch 57/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2021 Dice: 0.7186 LR: 0.001000
  Batch [50/499] Loss: 0.2274 Dice: 0.6928 LR: 0.001000
  Batch [100/499] Loss: 0.3729 Dice: 0.4793 LR: 0.001000
  Batch [150/499] Loss: 0.2734 Dice: 0.6239 LR: 0.001000
  Batch [200/499] Loss: 0.1956 Dice: 0.7289 LR: 0.001000
  Batch [250/499] Loss: 0.2431 Dice: 0.6606 LR: 0.001000
  Batch [300/499] Loss: 0.1839 Dice: 0.7491 LR: 0.001000
  Batch [350/499] Loss: 0.2891 Dice: 0.5980 LR: 0.001000
  Batch [400/499] Loss: 0.2780 Dice: 0.6096 LR: 0.001000
  Batch [450/499] Loss: 0.3255 Dice: 0.5554 LR: 0.001000
Epoch 57 Train: 	[total] 0.2728	[shape] 0.2722	[dice] 0.6212	[l2] 6.4798
  Training time: 87.75s
  Total epoch time: 87.75s (no validation)

Epoch 58/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2019 Dice: 0.7156 LR: 0.001000
  Batch [50/499] Loss: 0.2412 Dice: 0.6681 LR: 0.001000
  Batch [100/499] Loss: 0.2773 Dice: 0.6219 LR: 0.001000
  Batch [150/499] Loss: 0.2139 Dice: 0.7041 LR: 0.001000
  Batch [200/499] Loss: 0.2568 Dice: 0.6425 LR: 0.001000
  Batch [250/499] Loss: 0.3011 Dice: 0.5843 LR: 0.001000
  Batch [300/499] Loss: 0.2916 Dice: 0.6033 LR: 0.001000
  Batch [350/499] Loss: 0.2470 Dice: 0.6588 LR: 0.001000
  Batch [400/499] Loss: 0.3248 Dice: 0.5502 LR: 0.001000
  Batch [450/499] Loss: 0.2109 Dice: 0.7134 LR: 0.001000
Epoch 58 Train: 	[total] 0.2741	[shape] 0.2735	[dice] 0.6194	[l2] 6.4838
  Training time: 88.06s
  Total epoch time: 88.06s (no validation)

Epoch 59/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3083 Dice: 0.5745 LR: 0.001000
  Batch [50/499] Loss: 0.2875 Dice: 0.5965 LR: 0.001000
  Batch [100/499] Loss: 0.1935 Dice: 0.7335 LR: 0.001000
  Batch [150/499] Loss: 0.2267 Dice: 0.6876 LR: 0.001000
  Batch [200/499] Loss: 0.2748 Dice: 0.6142 LR: 0.001000
  Batch [250/499] Loss: 0.3969 Dice: 0.4415 LR: 0.001000
  Batch [300/499] Loss: 0.3238 Dice: 0.5486 LR: 0.001000
  Batch [350/499] Loss: 0.2701 Dice: 0.6188 LR: 0.001000
  Batch [400/499] Loss: 0.2029 Dice: 0.7180 LR: 0.001000
  Batch [450/499] Loss: 0.3739 Dice: 0.4821 LR: 0.001000
Epoch 59 Train: 	[total] 0.2721	[shape] 0.2715	[dice] 0.6219	[l2] 6.4879
  Training time: 87.85s
  Total epoch time: 87.85s (no validation)

Epoch 60/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2853 Dice: 0.5988 LR: 0.001000
  Batch [50/499] Loss: 0.3065 Dice: 0.5786 LR: 0.001000
  Batch [100/499] Loss: 0.2361 Dice: 0.6710 LR: 0.001000
  Batch [150/499] Loss: 0.1928 Dice: 0.7354 LR: 0.001000
  Batch [200/499] Loss: 0.2977 Dice: 0.5932 LR: 0.001000
  Batch [250/499] Loss: 0.2487 Dice: 0.6476 LR: 0.001000
  Batch [300/499] Loss: 0.2167 Dice: 0.7004 LR: 0.001000
  Batch [350/499] Loss: 0.2937 Dice: 0.5880 LR: 0.001000
  Batch [400/499] Loss: 0.2028 Dice: 0.7213 LR: 0.001000
  Batch [450/499] Loss: 0.3056 Dice: 0.5718 LR: 0.001000
Epoch 60 Train: 	[total] 0.2728	[shape] 0.2722	[dice] 0.6212	[l2] 6.4915
  Training time: 88.02s
Epoch 60 Eval:  	[total] 0.6434	[shape] 0.6427	[dice] 0.0884	[l2] 6.4951
  Validation time: 202.44s
======================================================================
Found a new best model! Shape Loss: 0.642740
======================================================================
  Total epoch time: 290.46s

Epoch 61/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3619 Dice: 0.4905 LR: 0.001000
  Batch [50/499] Loss: 0.2258 Dice: 0.6865 LR: 0.001000
  Batch [100/499] Loss: 0.2626 Dice: 0.6349 LR: 0.001000
  Batch [150/499] Loss: 0.2478 Dice: 0.6503 LR: 0.001000
  Batch [200/499] Loss: 0.2924 Dice: 0.5920 LR: 0.001000
  Batch [250/499] Loss: 0.2336 Dice: 0.6771 LR: 0.001000
  Batch [300/499] Loss: 0.4329 Dice: 0.3844 LR: 0.001000
  Batch [350/499] Loss: 0.3043 Dice: 0.5712 LR: 0.001000
  Batch [400/499] Loss: 0.3356 Dice: 0.5329 LR: 0.001000
  Batch [450/499] Loss: 0.2192 Dice: 0.6933 LR: 0.001000
Epoch 61 Train: 	[total] 0.2730	[shape] 0.2724	[dice] 0.6208	[l2] 6.4954
  Training time: 88.09s
  Total epoch time: 88.09s (no validation)

Epoch 62/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.4010 Dice: 0.4394 LR: 0.001000
  Batch [50/499] Loss: 0.3234 Dice: 0.5507 LR: 0.001000
  Batch [100/499] Loss: 0.2684 Dice: 0.6281 LR: 0.001000
  Batch [150/499] Loss: 0.2675 Dice: 0.6323 LR: 0.001000
  Batch [200/499] Loss: 0.2855 Dice: 0.6067 LR: 0.001000
  Batch [250/499] Loss: 0.2724 Dice: 0.6200 LR: 0.001000
  Batch [300/499] Loss: 0.3125 Dice: 0.5675 LR: 0.001000
  Batch [350/499] Loss: 0.2845 Dice: 0.6101 LR: 0.001000
  Batch [400/499] Loss: 0.2490 Dice: 0.6562 LR: 0.001000
  Batch [450/499] Loss: 0.2950 Dice: 0.5922 LR: 0.001000
Epoch 62 Train: 	[total] 0.2740	[shape] 0.2733	[dice] 0.6195	[l2] 6.4994
  Training time: 88.11s
  Total epoch time: 88.11s (no validation)

Epoch 63/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2319 Dice: 0.6792 LR: 0.001000
  Batch [50/499] Loss: 0.2129 Dice: 0.7056 LR: 0.001000
  Batch [100/499] Loss: 0.2545 Dice: 0.6470 LR: 0.001000
  Batch [150/499] Loss: 0.2439 Dice: 0.6601 LR: 0.001000
  Batch [200/499] Loss: 0.3082 Dice: 0.5679 LR: 0.001000
  Batch [250/499] Loss: 0.2286 Dice: 0.6821 LR: 0.001000
  Batch [300/499] Loss: 0.3744 Dice: 0.4806 LR: 0.001000
  Batch [350/499] Loss: 0.3666 Dice: 0.4978 LR: 0.001000
  Batch [400/499] Loss: 0.4341 Dice: 0.3951 LR: 0.001000
  Batch [450/499] Loss: 0.3023 Dice: 0.5836 LR: 0.001000
Epoch 63 Train: 	[total] 0.2717	[shape] 0.2711	[dice] 0.6229	[l2] 6.5034
  Training time: 87.69s
  Total epoch time: 87.69s (no validation)

Epoch 64/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3009 Dice: 0.5760 LR: 0.001000
  Batch [50/499] Loss: 0.2559 Dice: 0.6508 LR: 0.001000
  Batch [100/499] Loss: 0.2869 Dice: 0.6041 LR: 0.001000
  Batch [150/499] Loss: 0.2742 Dice: 0.6175 LR: 0.001000
  Batch [200/499] Loss: 0.2246 Dice: 0.6920 LR: 0.001000
  Batch [250/499] Loss: 0.2437 Dice: 0.6649 LR: 0.001000
  Batch [300/499] Loss: 0.2304 Dice: 0.6872 LR: 0.001000
  Batch [350/499] Loss: 0.2274 Dice: 0.6831 LR: 0.001000
  Batch [400/499] Loss: 0.3034 Dice: 0.5733 LR: 0.001000
  Batch [450/499] Loss: 0.2546 Dice: 0.6455 LR: 0.001000
Epoch 64 Train: 	[total] 0.2723	[shape] 0.2716	[dice] 0.6218	[l2] 6.5071
  Training time: 87.72s
  Total epoch time: 87.72s (no validation)

Epoch 65/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2489 Dice: 0.6524 LR: 0.001000
  Batch [50/499] Loss: 0.2244 Dice: 0.6884 LR: 0.001000
  Batch [100/499] Loss: 0.2467 Dice: 0.6572 LR: 0.001000
  Batch [150/499] Loss: 0.2649 Dice: 0.6347 LR: 0.001000
  Batch [200/499] Loss: 0.2409 Dice: 0.6630 LR: 0.001000
  Batch [250/499] Loss: 0.2860 Dice: 0.6016 LR: 0.001000
  Batch [300/499] Loss: 0.2848 Dice: 0.6057 LR: 0.001000
  Batch [350/499] Loss: 0.3769 Dice: 0.4694 LR: 0.001000
  Batch [400/499] Loss: 0.2252 Dice: 0.6861 LR: 0.001000
  Batch [450/499] Loss: 0.2158 Dice: 0.7065 LR: 0.001000
Epoch 65 Train: 	[total] 0.2714	[shape] 0.2707	[dice] 0.6229	[l2] 6.5112
  Training time: 87.98s
  Total epoch time: 87.98s (no validation)

Epoch 66/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2829 Dice: 0.6034 LR: 0.001000
  Batch [50/499] Loss: 0.3583 Dice: 0.5080 LR: 0.001000
  Batch [100/499] Loss: 0.2134 Dice: 0.7088 LR: 0.001000
  Batch [150/499] Loss: 0.2307 Dice: 0.6825 LR: 0.001000
  Batch [200/499] Loss: 0.3537 Dice: 0.5039 LR: 0.001000
  Batch [250/499] Loss: 0.2574 Dice: 0.6378 LR: 0.001000
  Batch [300/499] Loss: 0.2434 Dice: 0.6695 LR: 0.001000
  Batch [350/499] Loss: 0.2287 Dice: 0.6821 LR: 0.001000
  Batch [400/499] Loss: 0.2726 Dice: 0.6224 LR: 0.001000
  Batch [450/499] Loss: 0.4258 Dice: 0.4069 LR: 0.001000
Epoch 66 Train: 	[total] 0.2704	[shape] 0.2697	[dice] 0.6245	[l2] 6.5152
  Training time: 87.86s
  Total epoch time: 87.86s (no validation)

Epoch 67/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2397 Dice: 0.6655 LR: 0.001000
  Batch [50/499] Loss: 0.3237 Dice: 0.5478 LR: 0.001000
  Batch [100/499] Loss: 0.2319 Dice: 0.6721 LR: 0.001000
  Batch [150/499] Loss: 0.2300 Dice: 0.6807 LR: 0.001000
  Batch [200/499] Loss: 0.2990 Dice: 0.5844 LR: 0.001000
  Batch [250/499] Loss: 0.3385 Dice: 0.5264 LR: 0.001000
  Batch [300/499] Loss: 0.2003 Dice: 0.7224 LR: 0.001000
  Batch [350/499] Loss: 0.2053 Dice: 0.7171 LR: 0.001000
  Batch [400/499] Loss: 0.3354 Dice: 0.5377 LR: 0.001000
  Batch [450/499] Loss: 0.2354 Dice: 0.6740 LR: 0.001000
Epoch 67 Train: 	[total] 0.2701	[shape] 0.2695	[dice] 0.6249	[l2] 6.5193
  Training time: 88.11s
  Total epoch time: 88.11s (no validation)

Epoch 68/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2389 Dice: 0.6750 LR: 0.001000
  Batch [50/499] Loss: 0.3948 Dice: 0.4507 LR: 0.001000
  Batch [100/499] Loss: 0.2519 Dice: 0.6513 LR: 0.001000
  Batch [150/499] Loss: 0.2135 Dice: 0.7029 LR: 0.001000
  Batch [200/499] Loss: 0.3184 Dice: 0.5594 LR: 0.001000
  Batch [250/499] Loss: 0.2624 Dice: 0.6398 LR: 0.001000
  Batch [300/499] Loss: 0.2466 Dice: 0.6564 LR: 0.001000
  Batch [350/499] Loss: 0.2723 Dice: 0.6219 LR: 0.001000
  Batch [400/499] Loss: 0.2495 Dice: 0.6575 LR: 0.001000
  Batch [450/499] Loss: 0.3007 Dice: 0.5818 LR: 0.001000
Epoch 68 Train: 	[total] 0.2702	[shape] 0.2696	[dice] 0.6246	[l2] 6.5232
  Training time: 87.82s
  Total epoch time: 87.82s (no validation)

Epoch 69/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3145 Dice: 0.5684 LR: 0.001000
  Batch [50/499] Loss: 0.1952 Dice: 0.7328 LR: 0.001000
  Batch [100/499] Loss: 0.2764 Dice: 0.6158 LR: 0.001000
  Batch [150/499] Loss: 0.2035 Dice: 0.7216 LR: 0.001000
  Batch [200/499] Loss: 0.1719 Dice: 0.7598 LR: 0.001000
  Batch [250/499] Loss: 0.2315 Dice: 0.6788 LR: 0.001000
  Batch [300/499] Loss: 0.2244 Dice: 0.6873 LR: 0.001000
  Batch [350/499] Loss: 0.3343 Dice: 0.5323 LR: 0.001000
  Batch [400/499] Loss: 0.2584 Dice: 0.6390 LR: 0.001000
  Batch [450/499] Loss: 0.2519 Dice: 0.6422 LR: 0.001000
Epoch 69 Train: 	[total] 0.2703	[shape] 0.2697	[dice] 0.6245	[l2] 6.5270
  Training time: 87.89s
  Total epoch time: 87.89s (no validation)

Epoch 70/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2074 Dice: 0.7143 LR: 0.001000
  Batch [50/499] Loss: 0.2413 Dice: 0.6646 LR: 0.001000
  Batch [100/499] Loss: 0.2134 Dice: 0.7007 LR: 0.001000
  Batch [150/499] Loss: 0.2591 Dice: 0.6456 LR: 0.001000
  Batch [200/499] Loss: 0.2526 Dice: 0.6546 LR: 0.001000
  Batch [250/499] Loss: 0.3568 Dice: 0.4989 LR: 0.001000
  Batch [300/499] Loss: 0.2848 Dice: 0.5986 LR: 0.001000
  Batch [350/499] Loss: 0.2696 Dice: 0.6212 LR: 0.001000
  Batch [400/499] Loss: 0.2699 Dice: 0.6182 LR: 0.001000
  Batch [450/499] Loss: 0.2231 Dice: 0.6937 LR: 0.001000
Epoch 70 Train: 	[total] 0.2720	[shape] 0.2714	[dice] 0.6222	[l2] 6.5309
  Training time: 87.99s
Epoch 70 Eval:  	[total] 0.6465	[shape] 0.6458	[dice] 0.0835	[l2] 6.5347
  Validation time: 202.69s
  Total epoch time: 290.68s

Epoch 71/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2929 Dice: 0.5946 LR: 0.001000
  Batch [50/499] Loss: 0.3392 Dice: 0.5314 LR: 0.001000
  Batch [100/499] Loss: 0.2292 Dice: 0.6823 LR: 0.001000
  Batch [150/499] Loss: 0.4182 Dice: 0.4108 LR: 0.001000
  Batch [200/499] Loss: 0.2169 Dice: 0.6984 LR: 0.001000
  Batch [250/499] Loss: 0.2765 Dice: 0.6173 LR: 0.001000
  Batch [300/499] Loss: 0.3890 Dice: 0.4396 LR: 0.001000
  Batch [350/499] Loss: 0.3458 Dice: 0.5206 LR: 0.001000
  Batch [400/499] Loss: 0.2636 Dice: 0.6315 LR: 0.001000
  Batch [450/499] Loss: 0.1913 Dice: 0.7392 LR: 0.001000
Epoch 71 Train: 	[total] 0.2719	[shape] 0.2713	[dice] 0.6224	[l2] 6.5350
  Training time: 88.11s
  Total epoch time: 88.11s (no validation)

Epoch 72/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2412 Dice: 0.6679 LR: 0.001000
  Batch [50/499] Loss: 0.2574 Dice: 0.6385 LR: 0.001000
  Batch [100/499] Loss: 0.2715 Dice: 0.6205 LR: 0.001000
  Batch [150/499] Loss: 0.3258 Dice: 0.5499 LR: 0.001000
  Batch [200/499] Loss: 0.2879 Dice: 0.5983 LR: 0.001000
  Batch [250/499] Loss: 0.2480 Dice: 0.6525 LR: 0.001000
  Batch [300/499] Loss: 0.2197 Dice: 0.7002 LR: 0.001000
  Batch [350/499] Loss: 0.2621 Dice: 0.6402 LR: 0.001000
  Batch [400/499] Loss: 0.2035 Dice: 0.7143 LR: 0.001000
  Batch [450/499] Loss: 0.2872 Dice: 0.6006 LR: 0.001000
Epoch 72 Train: 	[total] 0.2701	[shape] 0.2695	[dice] 0.6248	[l2] 6.5388
  Training time: 87.84s
  Total epoch time: 87.84s (no validation)

Epoch 73/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3271 Dice: 0.5431 LR: 0.001000
  Batch [50/499] Loss: 0.1936 Dice: 0.7371 LR: 0.001000
  Batch [100/499] Loss: 0.3312 Dice: 0.5418 LR: 0.001000
  Batch [150/499] Loss: 0.2135 Dice: 0.7061 LR: 0.001000
  Batch [200/499] Loss: 0.2074 Dice: 0.7159 LR: 0.001000
  Batch [250/499] Loss: 0.2256 Dice: 0.7006 LR: 0.001000
  Batch [300/499] Loss: 0.3433 Dice: 0.5267 LR: 0.001000
  Batch [350/499] Loss: 0.2438 Dice: 0.6638 LR: 0.001000
  Batch [400/499] Loss: 0.2279 Dice: 0.6861 LR: 0.001000
  Batch [450/499] Loss: 0.2626 Dice: 0.6388 LR: 0.001000
Epoch 73 Train: 	[total] 0.2698	[shape] 0.2691	[dice] 0.6253	[l2] 6.5424
  Training time: 88.06s
  Total epoch time: 88.06s (no validation)

Epoch 74/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2037 Dice: 0.7211 LR: 0.001000
  Batch [50/499] Loss: 0.2781 Dice: 0.6198 LR: 0.001000
  Batch [100/499] Loss: 0.2469 Dice: 0.6611 LR: 0.001000
  Batch [150/499] Loss: 0.2539 Dice: 0.6467 LR: 0.001000
  Batch [200/499] Loss: 0.2564 Dice: 0.6445 LR: 0.001000
  Batch [250/499] Loss: 0.2052 Dice: 0.7152 LR: 0.001000
  Batch [300/499] Loss: 0.1979 Dice: 0.7250 LR: 0.001000
  Batch [350/499] Loss: 0.3273 Dice: 0.5463 LR: 0.001000
  Batch [400/499] Loss: 0.2679 Dice: 0.6227 LR: 0.001000
  Batch [450/499] Loss: 0.2213 Dice: 0.6896 LR: 0.001000
Epoch 74 Train: 	[total] 0.2682	[shape] 0.2675	[dice] 0.6276	[l2] 6.5462
  Training time: 88.13s
  Total epoch time: 88.13s (no validation)

Epoch 75/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2437 Dice: 0.6581 LR: 0.001000
  Batch [50/499] Loss: 0.2694 Dice: 0.6294 LR: 0.001000
  Batch [100/499] Loss: 0.3488 Dice: 0.5173 LR: 0.001000
  Batch [150/499] Loss: 0.2173 Dice: 0.6992 LR: 0.001000
  Batch [200/499] Loss: 0.2296 Dice: 0.6830 LR: 0.001000
  Batch [250/499] Loss: 0.1732 Dice: 0.7654 LR: 0.001000
  Batch [300/499] Loss: 0.3422 Dice: 0.5175 LR: 0.001000
  Batch [350/499] Loss: 0.2795 Dice: 0.6156 LR: 0.001000
  Batch [400/499] Loss: 0.2485 Dice: 0.6596 LR: 0.001000
  Batch [450/499] Loss: 0.2147 Dice: 0.7041 LR: 0.001000
Epoch 75 Train: 	[total] 0.2669	[shape] 0.2663	[dice] 0.6293	[l2] 6.5500
  Training time: 87.70s
  Total epoch time: 87.70s (no validation)

Epoch 76/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2218 Dice: 0.6927 LR: 0.001000
  Batch [50/499] Loss: 0.2626 Dice: 0.6426 LR: 0.001000
  Batch [100/499] Loss: 0.2134 Dice: 0.7064 LR: 0.001000
  Batch [150/499] Loss: 0.2832 Dice: 0.6097 LR: 0.001000
  Batch [200/499] Loss: 0.2003 Dice: 0.7203 LR: 0.001000
  Batch [250/499] Loss: 0.1919 Dice: 0.7294 LR: 0.001000
  Batch [300/499] Loss: 0.2883 Dice: 0.6025 LR: 0.001000
  Batch [350/499] Loss: 0.3045 Dice: 0.5790 LR: 0.001000
  Batch [400/499] Loss: 0.3791 Dice: 0.4707 LR: 0.001000
  Batch [450/499] Loss: 0.3038 Dice: 0.5822 LR: 0.001000
Epoch 76 Train: 	[total] 0.2681	[shape] 0.2674	[dice] 0.6278	[l2] 6.5539
  Training time: 87.96s
  Total epoch time: 87.96s (no validation)

Epoch 77/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2077 Dice: 0.7220 LR: 0.001000
  Batch [50/499] Loss: 0.2080 Dice: 0.7133 LR: 0.001000
  Batch [100/499] Loss: 0.2507 Dice: 0.6498 LR: 0.001000
  Batch [150/499] Loss: 0.2866 Dice: 0.6011 LR: 0.001000
  Batch [200/499] Loss: 0.4156 Dice: 0.4158 LR: 0.001000
  Batch [250/499] Loss: 0.3043 Dice: 0.5700 LR: 0.001000
  Batch [300/499] Loss: 0.2590 Dice: 0.6397 LR: 0.001000
  Batch [350/499] Loss: 0.2574 Dice: 0.6427 LR: 0.001000
  Batch [400/499] Loss: 0.2242 Dice: 0.6933 LR: 0.001000
  Batch [450/499] Loss: 0.2164 Dice: 0.7087 LR: 0.001000
Epoch 77 Train: 	[total] 0.2675	[shape] 0.2669	[dice] 0.6288	[l2] 6.5575
  Training time: 88.00s
  Total epoch time: 88.00s (no validation)

Epoch 78/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2785 Dice: 0.6081 LR: 0.001000
  Batch [50/499] Loss: 0.3266 Dice: 0.5474 LR: 0.001000
  Batch [100/499] Loss: 0.2139 Dice: 0.6982 LR: 0.001000
  Batch [150/499] Loss: 0.2773 Dice: 0.6189 LR: 0.001000
  Batch [200/499] Loss: 0.3096 Dice: 0.5724 LR: 0.001000
  Batch [250/499] Loss: 0.3029 Dice: 0.5658 LR: 0.001000
  Batch [300/499] Loss: 0.2944 Dice: 0.5950 LR: 0.001000
  Batch [350/499] Loss: 0.1867 Dice: 0.7407 LR: 0.001000
  Batch [400/499] Loss: 0.2424 Dice: 0.6567 LR: 0.001000
  Batch [450/499] Loss: 0.1833 Dice: 0.7416 LR: 0.001000
Epoch 78 Train: 	[total] 0.2690	[shape] 0.2684	[dice] 0.6264	[l2] 6.5611
  Training time: 88.04s
  Total epoch time: 88.04s (no validation)

Epoch 79/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2325 Dice: 0.6804 LR: 0.001000
  Batch [50/499] Loss: 0.2956 Dice: 0.5917 LR: 0.001000
  Batch [100/499] Loss: 0.2962 Dice: 0.5926 LR: 0.001000
  Batch [150/499] Loss: 0.2712 Dice: 0.6216 LR: 0.001000
  Batch [200/499] Loss: 0.2965 Dice: 0.5898 LR: 0.001000
  Batch [250/499] Loss: 0.1334 Dice: 0.8135 LR: 0.001000
  Batch [300/499] Loss: 0.2354 Dice: 0.6767 LR: 0.001000
  Batch [350/499] Loss: 0.3966 Dice: 0.4427 LR: 0.001000
  Batch [400/499] Loss: 0.2533 Dice: 0.6511 LR: 0.001000
  Batch [450/499] Loss: 0.2240 Dice: 0.6925 LR: 0.001000
Epoch 79 Train: 	[total] 0.2671	[shape] 0.2665	[dice] 0.6292	[l2] 6.5652
  Training time: 87.84s
  Total epoch time: 87.84s (no validation)

Epoch 80/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3028 Dice: 0.5750 LR: 0.001000
  Batch [50/499] Loss: 0.2538 Dice: 0.6473 LR: 0.001000
  Batch [100/499] Loss: 0.3267 Dice: 0.5465 LR: 0.001000
  Batch [150/499] Loss: 0.2648 Dice: 0.6326 LR: 0.001000
  Batch [200/499] Loss: 0.3394 Dice: 0.5366 LR: 0.001000
  Batch [250/499] Loss: 0.3132 Dice: 0.5703 LR: 0.001000
  Batch [300/499] Loss: 0.2572 Dice: 0.6387 LR: 0.001000
  Batch [350/499] Loss: 0.2080 Dice: 0.7124 LR: 0.001000
  Batch [400/499] Loss: 0.3336 Dice: 0.5339 LR: 0.001000
  Batch [450/499] Loss: 0.2854 Dice: 0.6003 LR: 0.001000
Epoch 80 Train: 	[total] 0.2687	[shape] 0.2680	[dice] 0.6269	[l2] 6.5694
  Training time: 87.98s
Epoch 80 Eval:  	[total] 0.6470	[shape] 0.6464	[dice] 0.0831	[l2] 6.5731
  Validation time: 202.50s
  Total epoch time: 290.49s

Epoch 81/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2310 Dice: 0.6802 LR: 0.001000
  Batch [50/499] Loss: 0.2831 Dice: 0.6073 LR: 0.001000
  Batch [100/499] Loss: 0.2318 Dice: 0.6759 LR: 0.001000
  Batch [150/499] Loss: 0.2039 Dice: 0.7183 LR: 0.001000
  Batch [200/499] Loss: 0.2765 Dice: 0.6171 LR: 0.001000
  Batch [250/499] Loss: 0.2843 Dice: 0.6043 LR: 0.001000
  Batch [300/499] Loss: 0.1547 Dice: 0.7826 LR: 0.001000
  Batch [350/499] Loss: 0.3029 Dice: 0.5792 LR: 0.001000
  Batch [400/499] Loss: 0.3601 Dice: 0.4975 LR: 0.001000
  Batch [450/499] Loss: 0.2551 Dice: 0.6469 LR: 0.001000
Epoch 81 Train: 	[total] 0.2676	[shape] 0.2669	[dice] 0.6285	[l2] 6.5733
  Training time: 88.12s
  Total epoch time: 88.12s (no validation)

Epoch 82/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2104 Dice: 0.7034 LR: 0.001000
  Batch [50/499] Loss: 0.2718 Dice: 0.6235 LR: 0.001000
  Batch [100/499] Loss: 0.2833 Dice: 0.6089 LR: 0.001000
  Batch [150/499] Loss: 0.2111 Dice: 0.7082 LR: 0.001000
  Batch [200/499] Loss: 0.2690 Dice: 0.6270 LR: 0.001000
  Batch [250/499] Loss: 0.3263 Dice: 0.5481 LR: 0.001000
  Batch [300/499] Loss: 0.2849 Dice: 0.5989 LR: 0.001000
  Batch [350/499] Loss: 0.1900 Dice: 0.7429 LR: 0.001000
  Batch [400/499] Loss: 0.2982 Dice: 0.5837 LR: 0.001000
  Batch [450/499] Loss: 0.2688 Dice: 0.6260 LR: 0.001000
Epoch 82 Train: 	[total] 0.2667	[shape] 0.2660	[dice] 0.6297	[l2] 6.5771
  Training time: 88.25s
  Total epoch time: 88.25s (no validation)

Epoch 83/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2048 Dice: 0.7204 LR: 0.001000
  Batch [50/499] Loss: 0.3794 Dice: 0.4705 LR: 0.001000
  Batch [100/499] Loss: 0.3258 Dice: 0.5445 LR: 0.001000
  Batch [150/499] Loss: 0.1957 Dice: 0.7313 LR: 0.001000
  Batch [200/499] Loss: 0.3213 Dice: 0.5536 LR: 0.001000
  Batch [250/499] Loss: 0.2811 Dice: 0.6025 LR: 0.001000
  Batch [300/499] Loss: 0.2506 Dice: 0.6495 LR: 0.001000
  Batch [350/499] Loss: 0.2751 Dice: 0.6200 LR: 0.001000
  Batch [400/499] Loss: 0.2789 Dice: 0.6031 LR: 0.001000
  Batch [450/499] Loss: 0.2178 Dice: 0.6957 LR: 0.001000
Epoch 83 Train: 	[total] 0.2675	[shape] 0.2668	[dice] 0.6287	[l2] 6.5808
  Training time: 88.03s
  Total epoch time: 88.03s (no validation)

Epoch 84/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2822 Dice: 0.6055 LR: 0.001000
  Batch [50/499] Loss: 0.1817 Dice: 0.7524 LR: 0.001000
  Batch [100/499] Loss: 0.2785 Dice: 0.6150 LR: 0.001000
  Batch [150/499] Loss: 0.3832 Dice: 0.4689 LR: 0.001000
  Batch [200/499] Loss: 0.2478 Dice: 0.6626 LR: 0.001000
  Batch [250/499] Loss: 0.2850 Dice: 0.6074 LR: 0.001000
  Batch [300/499] Loss: 0.2832 Dice: 0.6020 LR: 0.001000
  Batch [350/499] Loss: 0.2414 Dice: 0.6622 LR: 0.001000
  Batch [400/499] Loss: 0.3057 Dice: 0.5798 LR: 0.001000
  Batch [450/499] Loss: 0.2321 Dice: 0.6845 LR: 0.001000
Epoch 84 Train: 	[total] 0.2665	[shape] 0.2658	[dice] 0.6300	[l2] 6.5846
  Training time: 88.28s
  Total epoch time: 88.28s (no validation)

Epoch 85/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2985 Dice: 0.5784 LR: 0.001000
  Batch [50/499] Loss: 0.1891 Dice: 0.7382 LR: 0.001000
  Batch [100/499] Loss: 0.3187 Dice: 0.5534 LR: 0.001000
  Batch [150/499] Loss: 0.2649 Dice: 0.6281 LR: 0.001000
  Batch [200/499] Loss: 0.2310 Dice: 0.6713 LR: 0.001000
  Batch [250/499] Loss: 0.2576 Dice: 0.6433 LR: 0.001000
  Batch [300/499] Loss: 0.2281 Dice: 0.6837 LR: 0.001000
  Batch [350/499] Loss: 0.2715 Dice: 0.6177 LR: 0.001000
  Batch [400/499] Loss: 0.2909 Dice: 0.5921 LR: 0.001000
  Batch [450/499] Loss: 0.2803 Dice: 0.6185 LR: 0.001000
Epoch 85 Train: 	[total] 0.2663	[shape] 0.2656	[dice] 0.6303	[l2] 6.5884
  Training time: 88.24s
  Total epoch time: 88.24s (no validation)

Epoch 86/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3450 Dice: 0.5154 LR: 0.001000
  Batch [50/499] Loss: 0.2936 Dice: 0.5966 LR: 0.001000
  Batch [100/499] Loss: 0.1799 Dice: 0.7534 LR: 0.001000
  Batch [150/499] Loss: 0.2504 Dice: 0.6532 LR: 0.001000
  Batch [200/499] Loss: 0.3152 Dice: 0.5670 LR: 0.001000
  Batch [250/499] Loss: 0.2752 Dice: 0.6234 LR: 0.001000
  Batch [300/499] Loss: 0.2157 Dice: 0.7067 LR: 0.001000
  Batch [350/499] Loss: 0.2395 Dice: 0.6680 LR: 0.001000
  Batch [400/499] Loss: 0.2899 Dice: 0.5908 LR: 0.001000
  Batch [450/499] Loss: 0.2533 Dice: 0.6439 LR: 0.001000
Epoch 86 Train: 	[total] 0.2646	[shape] 0.2640	[dice] 0.6324	[l2] 6.5922
  Training time: 87.98s
  Total epoch time: 87.98s (no validation)

Epoch 87/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2999 Dice: 0.5720 LR: 0.001000
  Batch [50/499] Loss: 0.2728 Dice: 0.6273 LR: 0.001000
  Batch [100/499] Loss: 0.3675 Dice: 0.4818 LR: 0.001000
  Batch [150/499] Loss: 0.2929 Dice: 0.5886 LR: 0.001000
  Batch [200/499] Loss: 0.3325 Dice: 0.5368 LR: 0.001000
  Batch [250/499] Loss: 0.2656 Dice: 0.6323 LR: 0.001000
  Batch [300/499] Loss: 0.2821 Dice: 0.6178 LR: 0.001000
  Batch [350/499] Loss: 0.2743 Dice: 0.6168 LR: 0.001000
  Batch [400/499] Loss: 0.2970 Dice: 0.5884 LR: 0.001000
  Batch [450/499] Loss: 0.2517 Dice: 0.6508 LR: 0.001000
Epoch 87 Train: 	[total] 0.2635	[shape] 0.2629	[dice] 0.6341	[l2] 6.5963
  Training time: 87.99s
  Total epoch time: 87.99s (no validation)

Epoch 88/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2297 Dice: 0.6843 LR: 0.001000
  Batch [50/499] Loss: 0.3476 Dice: 0.5220 LR: 0.001000
  Batch [100/499] Loss: 0.3486 Dice: 0.5130 LR: 0.001000
  Batch [150/499] Loss: 0.3268 Dice: 0.5397 LR: 0.001000
  Batch [200/499] Loss: 0.2524 Dice: 0.6548 LR: 0.001000
  Batch [250/499] Loss: 0.2845 Dice: 0.6067 LR: 0.001000
  Batch [300/499] Loss: 0.3505 Dice: 0.5103 LR: 0.001000
  Batch [350/499] Loss: 0.2405 Dice: 0.6691 LR: 0.001000
  Batch [400/499] Loss: 0.2527 Dice: 0.6446 LR: 0.001000
  Batch [450/499] Loss: 0.2618 Dice: 0.6432 LR: 0.001000
Epoch 88 Train: 	[total] 0.2645	[shape] 0.2638	[dice] 0.6329	[l2] 6.5996
  Training time: 87.94s
  Total epoch time: 87.94s (no validation)

Epoch 89/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2179 Dice: 0.7026 LR: 0.001000
  Batch [50/499] Loss: 0.2984 Dice: 0.5883 LR: 0.001000
  Batch [100/499] Loss: 0.3060 Dice: 0.5741 LR: 0.001000
  Batch [150/499] Loss: 0.2703 Dice: 0.6211 LR: 0.001000
  Batch [200/499] Loss: 0.2189 Dice: 0.6933 LR: 0.001000
  Batch [250/499] Loss: 0.2995 Dice: 0.5808 LR: 0.001000
  Batch [300/499] Loss: 0.2156 Dice: 0.7022 LR: 0.001000
  Batch [350/499] Loss: 0.2174 Dice: 0.6954 LR: 0.001000
  Batch [400/499] Loss: 0.2740 Dice: 0.6150 LR: 0.001000
  Batch [450/499] Loss: 0.2220 Dice: 0.6958 LR: 0.001000
Epoch 89 Train: 	[total] 0.2645	[shape] 0.2639	[dice] 0.6323	[l2] 6.6034
  Training time: 87.83s
  Total epoch time: 87.83s (no validation)

Epoch 90/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3119 Dice: 0.5677 LR: 0.001000
  Batch [50/499] Loss: 0.2347 Dice: 0.6695 LR: 0.001000
  Batch [100/499] Loss: 0.3019 Dice: 0.5803 LR: 0.001000
  Batch [150/499] Loss: 0.1590 Dice: 0.7826 LR: 0.001000
  Batch [200/499] Loss: 0.2278 Dice: 0.6882 LR: 0.001000
  Batch [250/499] Loss: 0.2344 Dice: 0.6728 LR: 0.001000
  Batch [300/499] Loss: 0.2366 Dice: 0.6749 LR: 0.001000
  Batch [350/499] Loss: 0.2217 Dice: 0.6958 LR: 0.001000
  Batch [400/499] Loss: 0.3636 Dice: 0.4973 LR: 0.001000
  Batch [450/499] Loss: 0.2824 Dice: 0.6081 LR: 0.001000
Epoch 90 Train: 	[total] 0.2644	[shape] 0.2638	[dice] 0.6330	[l2] 6.6070
  Training time: 87.96s
Epoch 90 Eval:  	[total] 0.6506	[shape] 0.6500	[dice] 0.0774	[l2] 6.6106
  Validation time: 202.30s
  Total epoch time: 290.26s

Epoch 91/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3053 Dice: 0.5661 LR: 0.001000
  Batch [50/499] Loss: 0.2850 Dice: 0.5991 LR: 0.001000
  Batch [100/499] Loss: 0.2964 Dice: 0.5875 LR: 0.001000
  Batch [150/499] Loss: 0.1695 Dice: 0.7725 LR: 0.001000
  Batch [200/499] Loss: 0.2775 Dice: 0.6150 LR: 0.001000
  Batch [250/499] Loss: 0.3098 Dice: 0.5686 LR: 0.001000
  Batch [300/499] Loss: 0.2341 Dice: 0.6788 LR: 0.001000
  Batch [350/499] Loss: 0.2425 Dice: 0.6631 LR: 0.001000
  Batch [400/499] Loss: 0.1926 Dice: 0.7319 LR: 0.001000
  Batch [450/499] Loss: 0.2016 Dice: 0.7198 LR: 0.001000
Epoch 91 Train: 	[total] 0.2652	[shape] 0.2646	[dice] 0.6316	[l2] 6.6108
  Training time: 88.08s
  Total epoch time: 88.08s (no validation)

Epoch 92/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2954 Dice: 0.5913 LR: 0.001000
  Batch [50/499] Loss: 0.2755 Dice: 0.6153 LR: 0.001000
  Batch [100/499] Loss: 0.2160 Dice: 0.7031 LR: 0.001000
  Batch [150/499] Loss: 0.2426 Dice: 0.6613 LR: 0.001000
  Batch [200/499] Loss: 0.2844 Dice: 0.5988 LR: 0.001000
  Batch [250/499] Loss: 0.2191 Dice: 0.6898 LR: 0.001000
  Batch [300/499] Loss: 0.2334 Dice: 0.6774 LR: 0.001000
  Batch [350/499] Loss: 0.3972 Dice: 0.4424 LR: 0.001000
  Batch [400/499] Loss: 0.2664 Dice: 0.6257 LR: 0.001000
  Batch [450/499] Loss: 0.2743 Dice: 0.6257 LR: 0.001000
Epoch 92 Train: 	[total] 0.2637	[shape] 0.2631	[dice] 0.6337	[l2] 6.6146
  Training time: 87.68s
  Total epoch time: 87.68s (no validation)

Epoch 93/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2420 Dice: 0.6668 LR: 0.001000
  Batch [50/499] Loss: 0.2451 Dice: 0.6623 LR: 0.001000
  Batch [100/499] Loss: 0.1922 Dice: 0.7380 LR: 0.001000
  Batch [150/499] Loss: 0.2098 Dice: 0.7090 LR: 0.001000
  Batch [200/499] Loss: 0.2373 Dice: 0.6717 LR: 0.001000
  Batch [250/499] Loss: 0.2475 Dice: 0.6600 LR: 0.001000
  Batch [300/499] Loss: 0.3458 Dice: 0.5113 LR: 0.001000
  Batch [350/499] Loss: 0.2315 Dice: 0.6810 LR: 0.001000
  Batch [400/499] Loss: 0.2793 Dice: 0.6107 LR: 0.001000
  Batch [450/499] Loss: 0.2038 Dice: 0.7174 LR: 0.001000
Epoch 93 Train: 	[total] 0.2639	[shape] 0.2633	[dice] 0.6338	[l2] 6.6184
  Training time: 87.71s
  Total epoch time: 87.71s (no validation)

Epoch 94/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.1970 Dice: 0.7288 LR: 0.001000
  Batch [50/499] Loss: 0.1706 Dice: 0.7649 LR: 0.001000
  Batch [100/499] Loss: 0.2669 Dice: 0.6279 LR: 0.001000
  Batch [150/499] Loss: 0.2857 Dice: 0.6038 LR: 0.001000
  Batch [200/499] Loss: 0.3408 Dice: 0.5264 LR: 0.001000
  Batch [250/499] Loss: 0.2989 Dice: 0.5775 LR: 0.001000
  Batch [300/499] Loss: 0.2297 Dice: 0.6827 LR: 0.001000
  Batch [350/499] Loss: 0.2251 Dice: 0.6884 LR: 0.001000
  Batch [400/499] Loss: 0.2064 Dice: 0.7144 LR: 0.001000
  Batch [450/499] Loss: 0.2648 Dice: 0.6352 LR: 0.001000
Epoch 94 Train: 	[total] 0.2628	[shape] 0.2622	[dice] 0.6351	[l2] 6.6220
  Training time: 87.66s
  Total epoch time: 87.66s (no validation)

Epoch 95/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2572 Dice: 0.6474 LR: 0.001000
  Batch [50/499] Loss: 0.2168 Dice: 0.6979 LR: 0.001000
  Batch [100/499] Loss: 0.2632 Dice: 0.6351 LR: 0.001000
  Batch [150/499] Loss: 0.1910 Dice: 0.7309 LR: 0.001000
  Batch [200/499] Loss: 0.3160 Dice: 0.5716 LR: 0.001000
  Batch [250/499] Loss: 0.2795 Dice: 0.6039 LR: 0.001000
  Batch [300/499] Loss: 0.2485 Dice: 0.6535 LR: 0.001000
  Batch [350/499] Loss: 0.3443 Dice: 0.5165 LR: 0.001000
  Batch [400/499] Loss: 0.2906 Dice: 0.5983 LR: 0.001000
  Batch [450/499] Loss: 0.3556 Dice: 0.5087 LR: 0.001000
Epoch 95 Train: 	[total] 0.2638	[shape] 0.2631	[dice] 0.6335	[l2] 6.6259
  Training time: 88.18s
  Total epoch time: 88.18s (no validation)

Epoch 96/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2874 Dice: 0.5947 LR: 0.001000
  Batch [50/499] Loss: 0.3015 Dice: 0.5906 LR: 0.001000
  Batch [100/499] Loss: 0.1877 Dice: 0.7370 LR: 0.001000
  Batch [150/499] Loss: 0.2092 Dice: 0.7120 LR: 0.001000
  Batch [200/499] Loss: 0.3636 Dice: 0.4919 LR: 0.001000
  Batch [250/499] Loss: 0.3411 Dice: 0.5208 LR: 0.001000
  Batch [300/499] Loss: 0.3798 Dice: 0.4675 LR: 0.001000
  Batch [350/499] Loss: 0.2887 Dice: 0.6010 LR: 0.001000
  Batch [400/499] Loss: 0.2825 Dice: 0.6084 LR: 0.001000
  Batch [450/499] Loss: 0.2982 Dice: 0.5782 LR: 0.001000
Epoch 96 Train: 	[total] 0.2630	[shape] 0.2623	[dice] 0.6347	[l2] 6.6295
  Training time: 88.20s
  Total epoch time: 88.20s (no validation)

Epoch 97/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.2845 Dice: 0.6009 LR: 0.001000
  Batch [50/499] Loss: 0.2667 Dice: 0.6290 LR: 0.001000
  Batch [100/499] Loss: 0.2718 Dice: 0.6151 LR: 0.001000
  Batch [150/499] Loss: 0.2562 Dice: 0.6444 LR: 0.001000
  Batch [200/499] Loss: 0.2503 Dice: 0.6555 LR: 0.001000
  Batch [250/499] Loss: 0.2566 Dice: 0.6381 LR: 0.001000
  Batch [300/499] Loss: 0.2088 Dice: 0.7037 LR: 0.001000
  Batch [350/499] Loss: 0.2440 Dice: 0.6645 LR: 0.001000
  Batch [400/499] Loss: 0.1394 Dice: 0.8078 LR: 0.001000
  Batch [450/499] Loss: 0.2422 Dice: 0.6669 LR: 0.001000
Epoch 97 Train: 	[total] 0.2631	[shape] 0.2625	[dice] 0.6345	[l2] 6.6332
  Training time: 87.78s
  Total epoch time: 87.78s (no validation)

Epoch 98/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.1997 Dice: 0.7223 LR: 0.001000
  Batch [50/499] Loss: 0.3940 Dice: 0.4452 LR: 0.001000
  Batch [100/499] Loss: 0.2610 Dice: 0.6380 LR: 0.001000
  Batch [150/499] Loss: 0.3540 Dice: 0.5034 LR: 0.001000
  Batch [200/499] Loss: 0.2058 Dice: 0.7198 LR: 0.001000
  Batch [250/499] Loss: 0.3148 Dice: 0.5637 LR: 0.001000
  Batch [300/499] Loss: 0.2349 Dice: 0.6785 LR: 0.001000
  Batch [350/499] Loss: 0.1737 Dice: 0.7616 LR: 0.001000
  Batch [400/499] Loss: 0.2342 Dice: 0.6695 LR: 0.001000
  Batch [450/499] Loss: 0.2799 Dice: 0.6065 LR: 0.001000
Epoch 98 Train: 	[total] 0.2626	[shape] 0.2620	[dice] 0.6355	[l2] 6.6370
  Training time: 88.08s
  Total epoch time: 88.08s (no validation)

Epoch 99/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3223 Dice: 0.5478 LR: 0.001000
  Batch [50/499] Loss: 0.1502 Dice: 0.7968 LR: 0.001000
  Batch [100/499] Loss: 0.3255 Dice: 0.5430 LR: 0.001000
  Batch [150/499] Loss: 0.2078 Dice: 0.7149 LR: 0.001000
  Batch [200/499] Loss: 0.3096 Dice: 0.5667 LR: 0.001000
  Batch [250/499] Loss: 0.2524 Dice: 0.6502 LR: 0.001000
  Batch [300/499] Loss: 0.2474 Dice: 0.6576 LR: 0.001000
  Batch [350/499] Loss: 0.2131 Dice: 0.6987 LR: 0.001000
  Batch [400/499] Loss: 0.2480 Dice: 0.6537 LR: 0.001000
  Batch [450/499] Loss: 0.1995 Dice: 0.7273 LR: 0.001000
Epoch 99 Train: 	[total] 0.2612	[shape] 0.2605	[dice] 0.6374	[l2] 6.6407
  Training time: 87.80s
  Total epoch time: 87.80s (no validation)

Epoch 100/300 (boundary_bias=50%)
  Batch [0/499] Loss: 0.3281 Dice: 0.5447 LR: 0.001000
  Batch [50/499] Loss: 0.2621 Dice: 0.6332 LR: 0.001000
  Batch [100/499] Loss: 0.2666 Dice: 0.6325 LR: 0.001000
  Batch [150/499] Loss: 0.2626 Dice: 0.6261 LR: 0.001000
  Batch [200/499] Loss: 0.2502 Dice: 0.6473 LR: 0.001000
  Batch [250/499] Loss: 0.2434 Dice: 0.6655 LR: 0.001000
  Batch [300/499] Loss: 0.1760 Dice: 0.7596 LR: 0.001000
  Batch [350/499] Loss: 0.2236 Dice: 0.6950 LR: 0.001000
  Batch [400/499] Loss: 0.2353 Dice: 0.6741 LR: 0.001000
  Batch [450/499] Loss: 0.3381 Dice: 0.5282 LR: 0.001000
Epoch 100 Train: 	[total] 0.2597	[shape] 0.2591	[dice] 0.6395	[l2] 6.6445
  Training time: 87.72s
Epoch 100 Eval:  	[total] 0.6478	[shape] 0.6472	[dice] 0.0815	[l2] 6.6479
  Validation time: 202.51s
  Total epoch time: 290.22s

Epoch 101/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3509 Dice: 0.5045 LR: 0.000500
  Batch [50/499] Loss: 0.3134 Dice: 0.5664 LR: 0.000500
  Batch [100/499] Loss: 0.3226 Dice: 0.5513 LR: 0.000500
  Batch [150/499] Loss: 0.3237 Dice: 0.5427 LR: 0.000500
  Batch [200/499] Loss: 0.3228 Dice: 0.5448 LR: 0.000500
  Batch [250/499] Loss: 0.3888 Dice: 0.4538 LR: 0.000500
  Batch [300/499] Loss: 0.2246 Dice: 0.6893 LR: 0.000500
  Batch [350/499] Loss: 0.3453 Dice: 0.5120 LR: 0.000500
  Batch [400/499] Loss: 0.2534 Dice: 0.6462 LR: 0.000500
  Batch [450/499] Loss: 0.2631 Dice: 0.6322 LR: 0.000500
Epoch 101 Train: 	[total] 0.2955	[shape] 0.2948	[dice] 0.5861	[l2] 6.6480
  Training time: 87.71s
  Total epoch time: 87.71s (no validation)

Epoch 102/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2149 Dice: 0.7019 LR: 0.000500
  Batch [50/499] Loss: 0.2103 Dice: 0.7102 LR: 0.000500
  Batch [100/499] Loss: 0.2485 Dice: 0.6536 LR: 0.000500
  Batch [150/499] Loss: 0.3215 Dice: 0.5507 LR: 0.000500
  Batch [200/499] Loss: 0.2591 Dice: 0.6381 LR: 0.000500
  Batch [250/499] Loss: 0.2618 Dice: 0.6339 LR: 0.000500
  Batch [300/499] Loss: 0.3181 Dice: 0.5548 LR: 0.000500
  Batch [350/499] Loss: 0.2593 Dice: 0.6351 LR: 0.000500
  Batch [400/499] Loss: 0.3699 Dice: 0.4804 LR: 0.000500
  Batch [450/499] Loss: 0.3046 Dice: 0.5716 LR: 0.000500
Epoch 102 Train: 	[total] 0.2792	[shape] 0.2785	[dice] 0.6091	[l2] 6.6489
  Training time: 87.63s
  Total epoch time: 87.63s (no validation)

Epoch 103/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3248 Dice: 0.5474 LR: 0.000500
  Batch [50/499] Loss: 0.3334 Dice: 0.5314 LR: 0.000500
  Batch [100/499] Loss: 0.1950 Dice: 0.7273 LR: 0.000500
  Batch [150/499] Loss: 0.2834 Dice: 0.6038 LR: 0.000500
  Batch [200/499] Loss: 0.2553 Dice: 0.6445 LR: 0.000500
  Batch [250/499] Loss: 0.3334 Dice: 0.5315 LR: 0.000500
  Batch [300/499] Loss: 0.2509 Dice: 0.6501 LR: 0.000500
  Batch [350/499] Loss: 0.3191 Dice: 0.5575 LR: 0.000500
  Batch [400/499] Loss: 0.2403 Dice: 0.6590 LR: 0.000500
  Batch [450/499] Loss: 0.2735 Dice: 0.6201 LR: 0.000500
Epoch 103 Train: 	[total] 0.2725	[shape] 0.2719	[dice] 0.6187	[l2] 6.6496
  Training time: 87.27s
  Total epoch time: 87.27s (no validation)

Epoch 104/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2723 Dice: 0.6174 LR: 0.000500
  Batch [50/499] Loss: 0.3309 Dice: 0.5314 LR: 0.000500
  Batch [100/499] Loss: 0.2185 Dice: 0.6988 LR: 0.000500
  Batch [150/499] Loss: 0.2780 Dice: 0.6104 LR: 0.000500
  Batch [200/499] Loss: 0.2783 Dice: 0.6087 LR: 0.000500
  Batch [250/499] Loss: 0.2843 Dice: 0.6024 LR: 0.000500
  Batch [300/499] Loss: 0.3285 Dice: 0.5383 LR: 0.000500
  Batch [350/499] Loss: 0.3170 Dice: 0.5562 LR: 0.000500
  Batch [400/499] Loss: 0.2335 Dice: 0.6769 LR: 0.000500
  Batch [450/499] Loss: 0.3589 Dice: 0.4986 LR: 0.000500
Epoch 104 Train: 	[total] 0.2707	[shape] 0.2701	[dice] 0.6211	[l2] 6.6502
  Training time: 87.60s
  Total epoch time: 87.60s (no validation)

Epoch 105/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2034 Dice: 0.7190 LR: 0.000500
  Batch [50/499] Loss: 0.2049 Dice: 0.7110 LR: 0.000500
  Batch [100/499] Loss: 0.2854 Dice: 0.6044 LR: 0.000500
  Batch [150/499] Loss: 0.2424 Dice: 0.6603 LR: 0.000500
  Batch [200/499] Loss: 0.3431 Dice: 0.5212 LR: 0.000500
  Batch [250/499] Loss: 0.2248 Dice: 0.6872 LR: 0.000500
  Batch [300/499] Loss: 0.2636 Dice: 0.6270 LR: 0.000500
  Batch [350/499] Loss: 0.2337 Dice: 0.6728 LR: 0.000500
  Batch [400/499] Loss: 0.2150 Dice: 0.7039 LR: 0.000500
  Batch [450/499] Loss: 0.2487 Dice: 0.6506 LR: 0.000500
Epoch 105 Train: 	[total] 0.2694	[shape] 0.2688	[dice] 0.6230	[l2] 6.6508
  Training time: 87.37s
  Total epoch time: 87.37s (no validation)

Epoch 106/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2323 Dice: 0.6748 LR: 0.000500
  Batch [50/499] Loss: 0.2381 Dice: 0.6625 LR: 0.000500
  Batch [100/499] Loss: 0.2219 Dice: 0.6893 LR: 0.000500
  Batch [150/499] Loss: 0.3064 Dice: 0.5713 LR: 0.000500
  Batch [200/499] Loss: 0.3332 Dice: 0.5303 LR: 0.000500
  Batch [250/499] Loss: 0.2884 Dice: 0.6032 LR: 0.000500
  Batch [300/499] Loss: 0.2445 Dice: 0.6595 LR: 0.000500
  Batch [350/499] Loss: 0.3121 Dice: 0.5609 LR: 0.000500
  Batch [400/499] Loss: 0.2491 Dice: 0.6465 LR: 0.000500
  Batch [450/499] Loss: 0.2229 Dice: 0.6857 LR: 0.000500
Epoch 106 Train: 	[total] 0.2690	[shape] 0.2684	[dice] 0.6235	[l2] 6.6514
  Training time: 87.50s
  Total epoch time: 87.50s (no validation)

Epoch 107/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2286 Dice: 0.6764 LR: 0.000500
  Batch [50/499] Loss: 0.2899 Dice: 0.5888 LR: 0.000500
  Batch [100/499] Loss: 0.2275 Dice: 0.6909 LR: 0.000500
  Batch [150/499] Loss: 0.2418 Dice: 0.6623 LR: 0.000500
  Batch [200/499] Loss: 0.2580 Dice: 0.6428 LR: 0.000500
  Batch [250/499] Loss: 0.2291 Dice: 0.6803 LR: 0.000500
  Batch [300/499] Loss: 0.3249 Dice: 0.5419 LR: 0.000500
  Batch [350/499] Loss: 0.2218 Dice: 0.6878 LR: 0.000500
  Batch [400/499] Loss: 0.2768 Dice: 0.6125 LR: 0.000500
  Batch [450/499] Loss: 0.1939 Dice: 0.7284 LR: 0.000500
Epoch 107 Train: 	[total] 0.2695	[shape] 0.2688	[dice] 0.6228	[l2] 6.6519
  Training time: 87.70s
  Total epoch time: 87.70s (no validation)

Epoch 108/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2628 Dice: 0.6365 LR: 0.000500
  Batch [50/499] Loss: 0.2508 Dice: 0.6486 LR: 0.000500
  Batch [100/499] Loss: 0.2665 Dice: 0.6348 LR: 0.000500
  Batch [150/499] Loss: 0.2951 Dice: 0.5849 LR: 0.000500
  Batch [200/499] Loss: 0.1707 Dice: 0.7624 LR: 0.000500
  Batch [250/499] Loss: 0.3505 Dice: 0.5083 LR: 0.000500
  Batch [300/499] Loss: 0.3430 Dice: 0.5191 LR: 0.000500
  Batch [350/499] Loss: 0.3112 Dice: 0.5576 LR: 0.000500
  Batch [400/499] Loss: 0.2658 Dice: 0.6260 LR: 0.000500
  Batch [450/499] Loss: 0.3354 Dice: 0.5264 LR: 0.000500
Epoch 108 Train: 	[total] 0.2682	[shape] 0.2676	[dice] 0.6245	[l2] 6.6527
  Training time: 87.62s
  Total epoch time: 87.62s (no validation)

Epoch 109/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.4006 Dice: 0.4351 LR: 0.000500
  Batch [50/499] Loss: 0.3243 Dice: 0.5472 LR: 0.000500
  Batch [100/499] Loss: 0.2677 Dice: 0.6241 LR: 0.000500
  Batch [150/499] Loss: 0.2430 Dice: 0.6609 LR: 0.000500
  Batch [200/499] Loss: 0.3095 Dice: 0.5689 LR: 0.000500
  Batch [250/499] Loss: 0.2479 Dice: 0.6576 LR: 0.000500
  Batch [300/499] Loss: 0.2449 Dice: 0.6562 LR: 0.000500
  Batch [350/499] Loss: 0.2281 Dice: 0.6819 LR: 0.000500
  Batch [400/499] Loss: 0.1746 Dice: 0.7561 LR: 0.000500
  Batch [450/499] Loss: 0.2680 Dice: 0.6229 LR: 0.000500
Epoch 109 Train: 	[total] 0.2692	[shape] 0.2685	[dice] 0.6231	[l2] 6.6535
  Training time: 87.37s
  Total epoch time: 87.37s (no validation)

Epoch 110/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2308 Dice: 0.6825 LR: 0.000500
  Batch [50/499] Loss: 0.1991 Dice: 0.7240 LR: 0.000500
  Batch [100/499] Loss: 0.2508 Dice: 0.6536 LR: 0.000500
  Batch [150/499] Loss: 0.2254 Dice: 0.6900 LR: 0.000500
  Batch [200/499] Loss: 0.1995 Dice: 0.7206 LR: 0.000500
  Batch [250/499] Loss: 0.2365 Dice: 0.6695 LR: 0.000500
  Batch [300/499] Loss: 0.2483 Dice: 0.6551 LR: 0.000500
  Batch [350/499] Loss: 0.3872 Dice: 0.4563 LR: 0.000500
  Batch [400/499] Loss: 0.2722 Dice: 0.6200 LR: 0.000500
  Batch [450/499] Loss: 0.2910 Dice: 0.5956 LR: 0.000500
Epoch 110 Train: 	[total] 0.2684	[shape] 0.2678	[dice] 0.6245	[l2] 6.6543
  Training time: 87.64s
Epoch 110 Eval:  	[total] 0.6415	[shape] 0.6408	[dice] 0.0901	[l2] 6.6552
  Validation time: 202.69s
======================================================================
Found a new best model! Shape Loss: 0.640826
======================================================================
  Total epoch time: 290.33s

Epoch 111/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2528 Dice: 0.6425 LR: 0.000500
  Batch [50/499] Loss: 0.2273 Dice: 0.6880 LR: 0.000500
  Batch [100/499] Loss: 0.2160 Dice: 0.7036 LR: 0.000500
  Batch [150/499] Loss: 0.3167 Dice: 0.5512 LR: 0.000500
  Batch [200/499] Loss: 0.2892 Dice: 0.5902 LR: 0.000500
  Batch [250/499] Loss: 0.2558 Dice: 0.6400 LR: 0.000500
  Batch [300/499] Loss: 0.3083 Dice: 0.5676 LR: 0.000500
  Batch [350/499] Loss: 0.2768 Dice: 0.6123 LR: 0.000500
  Batch [400/499] Loss: 0.2395 Dice: 0.6658 LR: 0.000500
  Batch [450/499] Loss: 0.3020 Dice: 0.5742 LR: 0.000500
Epoch 111 Train: 	[total] 0.2685	[shape] 0.2679	[dice] 0.6242	[l2] 6.6552
  Training time: 87.79s
  Total epoch time: 87.79s (no validation)

Epoch 112/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1974 Dice: 0.7281 LR: 0.000500
  Batch [50/499] Loss: 0.2824 Dice: 0.6067 LR: 0.000500
  Batch [100/499] Loss: 0.2306 Dice: 0.6763 LR: 0.000500
  Batch [150/499] Loss: 0.2692 Dice: 0.6196 LR: 0.000500
  Batch [200/499] Loss: 0.2706 Dice: 0.6167 LR: 0.000500
  Batch [250/499] Loss: 0.2758 Dice: 0.6114 LR: 0.000500
  Batch [300/499] Loss: 0.1945 Dice: 0.7275 LR: 0.000500
  Batch [350/499] Loss: 0.2380 Dice: 0.6656 LR: 0.000500
  Batch [400/499] Loss: 0.2218 Dice: 0.6843 LR: 0.000500
  Batch [450/499] Loss: 0.2524 Dice: 0.6451 LR: 0.000500
Epoch 112 Train: 	[total] 0.2684	[shape] 0.2677	[dice] 0.6244	[l2] 6.6560
  Training time: 87.63s
  Total epoch time: 87.63s (no validation)

Epoch 113/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3116 Dice: 0.5633 LR: 0.000500
  Batch [50/499] Loss: 0.2459 Dice: 0.6548 LR: 0.000500
  Batch [100/499] Loss: 0.3734 Dice: 0.4780 LR: 0.000500
  Batch [150/499] Loss: 0.2696 Dice: 0.6239 LR: 0.000500
  Batch [200/499] Loss: 0.2389 Dice: 0.6657 LR: 0.000500
  Batch [250/499] Loss: 0.2655 Dice: 0.6298 LR: 0.000500
  Batch [300/499] Loss: 0.2304 Dice: 0.6756 LR: 0.000500
  Batch [350/499] Loss: 0.2831 Dice: 0.6054 LR: 0.000500
  Batch [400/499] Loss: 0.1532 Dice: 0.7845 LR: 0.000500
  Batch [450/499] Loss: 0.3341 Dice: 0.5335 LR: 0.000500
Epoch 113 Train: 	[total] 0.2681	[shape] 0.2674	[dice] 0.6248	[l2] 6.6568
  Training time: 87.56s
  Total epoch time: 87.56s (no validation)

Epoch 114/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3916 Dice: 0.4442 LR: 0.000500
  Batch [50/499] Loss: 0.2766 Dice: 0.6103 LR: 0.000500
  Batch [100/499] Loss: 0.2487 Dice: 0.6491 LR: 0.000500
  Batch [150/499] Loss: 0.2353 Dice: 0.6719 LR: 0.000500
  Batch [200/499] Loss: 0.2447 Dice: 0.6551 LR: 0.000500
  Batch [250/499] Loss: 0.2473 Dice: 0.6538 LR: 0.000500
  Batch [300/499] Loss: 0.3455 Dice: 0.5123 LR: 0.000500
  Batch [350/499] Loss: 0.2805 Dice: 0.6053 LR: 0.000500
  Batch [400/499] Loss: 0.3206 Dice: 0.5506 LR: 0.000500
  Batch [450/499] Loss: 0.1779 Dice: 0.7554 LR: 0.000500
Epoch 114 Train: 	[total] 0.2670	[shape] 0.2664	[dice] 0.6264	[l2] 6.6576
  Training time: 87.45s
  Total epoch time: 87.45s (no validation)

Epoch 115/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2093 Dice: 0.7085 LR: 0.000500
  Batch [50/499] Loss: 0.2433 Dice: 0.6609 LR: 0.000500
  Batch [100/499] Loss: 0.1479 Dice: 0.7939 LR: 0.000500
  Batch [150/499] Loss: 0.2454 Dice: 0.6542 LR: 0.000500
  Batch [200/499] Loss: 0.3125 Dice: 0.5673 LR: 0.000500
  Batch [250/499] Loss: 0.2829 Dice: 0.6027 LR: 0.000500
  Batch [300/499] Loss: 0.2057 Dice: 0.7136 LR: 0.000500
  Batch [350/499] Loss: 0.2387 Dice: 0.6650 LR: 0.000500
  Batch [400/499] Loss: 0.2943 Dice: 0.5861 LR: 0.000500
  Batch [450/499] Loss: 0.2856 Dice: 0.6031 LR: 0.000500
Epoch 115 Train: 	[total] 0.2660	[shape] 0.2653	[dice] 0.6278	[l2] 6.6585
  Training time: 87.37s
  Total epoch time: 87.37s (no validation)

Epoch 116/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2486 Dice: 0.6534 LR: 0.000500
  Batch [50/499] Loss: 0.2265 Dice: 0.6863 LR: 0.000500
  Batch [100/499] Loss: 0.3152 Dice: 0.5551 LR: 0.000500
  Batch [150/499] Loss: 0.2290 Dice: 0.6800 LR: 0.000500
  Batch [200/499] Loss: 0.2244 Dice: 0.6882 LR: 0.000500
  Batch [250/499] Loss: 0.2513 Dice: 0.6488 LR: 0.000500
  Batch [300/499] Loss: 0.2474 Dice: 0.6541 LR: 0.000500
  Batch [350/499] Loss: 0.3727 Dice: 0.4749 LR: 0.000500
  Batch [400/499] Loss: 0.3258 Dice: 0.5461 LR: 0.000500
  Batch [450/499] Loss: 0.1881 Dice: 0.7389 LR: 0.000500
Epoch 116 Train: 	[total] 0.2674	[shape] 0.2667	[dice] 0.6258	[l2] 6.6593
  Training time: 87.75s
  Total epoch time: 87.75s (no validation)

Epoch 117/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2478 Dice: 0.6493 LR: 0.000500
  Batch [50/499] Loss: 0.2682 Dice: 0.6237 LR: 0.000500
  Batch [100/499] Loss: 0.2390 Dice: 0.6583 LR: 0.000500
  Batch [150/499] Loss: 0.1599 Dice: 0.7799 LR: 0.000500
  Batch [200/499] Loss: 0.3124 Dice: 0.5607 LR: 0.000500
  Batch [250/499] Loss: 0.2513 Dice: 0.6449 LR: 0.000500
  Batch [300/499] Loss: 0.2307 Dice: 0.6757 LR: 0.000500
  Batch [350/499] Loss: 0.2367 Dice: 0.6696 LR: 0.000500
  Batch [400/499] Loss: 0.2386 Dice: 0.6635 LR: 0.000500
  Batch [450/499] Loss: 0.2666 Dice: 0.6289 LR: 0.000500
Epoch 117 Train: 	[total] 0.2670	[shape] 0.2663	[dice] 0.6265	[l2] 6.6602
  Training time: 87.63s
  Total epoch time: 87.63s (no validation)

Epoch 118/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3049 Dice: 0.5705 LR: 0.000500
  Batch [50/499] Loss: 0.2617 Dice: 0.6319 LR: 0.000500
  Batch [100/499] Loss: 0.2222 Dice: 0.6906 LR: 0.000500
  Batch [150/499] Loss: 0.2191 Dice: 0.6977 LR: 0.000500
  Batch [200/499] Loss: 0.2689 Dice: 0.6257 LR: 0.000500
  Batch [250/499] Loss: 0.3439 Dice: 0.5161 LR: 0.000500
  Batch [300/499] Loss: 0.2002 Dice: 0.7216 LR: 0.000500
  Batch [350/499] Loss: 0.2806 Dice: 0.6089 LR: 0.000500
  Batch [400/499] Loss: 0.1790 Dice: 0.7521 LR: 0.000500
  Batch [450/499] Loss: 0.2892 Dice: 0.5947 LR: 0.000500
Epoch 118 Train: 	[total] 0.2672	[shape] 0.2666	[dice] 0.6260	[l2] 6.6610
  Training time: 87.88s
  Total epoch time: 87.88s (no validation)

Epoch 119/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2089 Dice: 0.7054 LR: 0.000500
  Batch [50/499] Loss: 0.2309 Dice: 0.6782 LR: 0.000500
  Batch [100/499] Loss: 0.2139 Dice: 0.7050 LR: 0.000500
  Batch [150/499] Loss: 0.2638 Dice: 0.6341 LR: 0.000500
  Batch [200/499] Loss: 0.2790 Dice: 0.6104 LR: 0.000500
  Batch [250/499] Loss: 0.2822 Dice: 0.6038 LR: 0.000500
  Batch [300/499] Loss: 0.2688 Dice: 0.6277 LR: 0.000500
  Batch [350/499] Loss: 0.2117 Dice: 0.7098 LR: 0.000500
  Batch [400/499] Loss: 0.2452 Dice: 0.6640 LR: 0.000500
  Batch [450/499] Loss: 0.2204 Dice: 0.6895 LR: 0.000500
Epoch 119 Train: 	[total] 0.2655	[shape] 0.2648	[dice] 0.6287	[l2] 6.6619
  Training time: 87.94s
  Total epoch time: 87.94s (no validation)

Epoch 120/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2151 Dice: 0.6943 LR: 0.000500
  Batch [50/499] Loss: 0.3529 Dice: 0.5011 LR: 0.000500
  Batch [100/499] Loss: 0.2156 Dice: 0.7021 LR: 0.000500
  Batch [150/499] Loss: 0.2052 Dice: 0.7132 LR: 0.000500
  Batch [200/499] Loss: 0.3918 Dice: 0.4522 LR: 0.000500
  Batch [250/499] Loss: 0.1867 Dice: 0.7363 LR: 0.000500
  Batch [300/499] Loss: 0.2660 Dice: 0.6256 LR: 0.000500
  Batch [350/499] Loss: 0.2409 Dice: 0.6647 LR: 0.000500
  Batch [400/499] Loss: 0.2253 Dice: 0.6877 LR: 0.000500
  Batch [450/499] Loss: 0.3315 Dice: 0.5387 LR: 0.000500
Epoch 120 Train: 	[total] 0.2667	[shape] 0.2660	[dice] 0.6269	[l2] 6.6629
  Training time: 87.52s
Epoch 120 Eval:  	[total] 0.6413	[shape] 0.6407	[dice] 0.0905	[l2] 6.6636
  Validation time: 201.21s
======================================================================
Found a new best model! Shape Loss: 0.640655
======================================================================
  Total epoch time: 288.73s

Epoch 121/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3509 Dice: 0.5051 LR: 0.000500
  Batch [50/499] Loss: 0.4017 Dice: 0.4324 LR: 0.000500
  Batch [100/499] Loss: 0.2276 Dice: 0.6805 LR: 0.000500
  Batch [150/499] Loss: 0.1989 Dice: 0.7254 LR: 0.000500
  Batch [200/499] Loss: 0.2973 Dice: 0.5819 LR: 0.000500
  Batch [250/499] Loss: 0.2965 Dice: 0.5823 LR: 0.000500
  Batch [300/499] Loss: 0.2789 Dice: 0.6056 LR: 0.000500
  Batch [350/499] Loss: 0.2905 Dice: 0.5944 LR: 0.000500
  Batch [400/499] Loss: 0.3070 Dice: 0.5735 LR: 0.000500
  Batch [450/499] Loss: 0.2268 Dice: 0.6836 LR: 0.000500
Epoch 121 Train: 	[total] 0.2663	[shape] 0.2657	[dice] 0.6273	[l2] 6.6637
  Training time: 87.88s
  Total epoch time: 87.88s (no validation)

Epoch 122/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2733 Dice: 0.6201 LR: 0.000500
  Batch [50/499] Loss: 0.1728 Dice: 0.7565 LR: 0.000500
  Batch [100/499] Loss: 0.2921 Dice: 0.5906 LR: 0.000500
  Batch [150/499] Loss: 0.2020 Dice: 0.7182 LR: 0.000500
  Batch [200/499] Loss: 0.3025 Dice: 0.5759 LR: 0.000500
  Batch [250/499] Loss: 0.2250 Dice: 0.6891 LR: 0.000500
  Batch [300/499] Loss: 0.2210 Dice: 0.6873 LR: 0.000500
  Batch [350/499] Loss: 0.2545 Dice: 0.6447 LR: 0.000500
  Batch [400/499] Loss: 0.1821 Dice: 0.7505 LR: 0.000500
  Batch [450/499] Loss: 0.2030 Dice: 0.7167 LR: 0.000500
Epoch 122 Train: 	[total] 0.2661	[shape] 0.2655	[dice] 0.6274	[l2] 6.6645
  Training time: 88.09s
  Total epoch time: 88.09s (no validation)

Epoch 123/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2442 Dice: 0.6584 LR: 0.000500
  Batch [50/499] Loss: 0.2646 Dice: 0.6274 LR: 0.000500
  Batch [100/499] Loss: 0.2573 Dice: 0.6372 LR: 0.000500
  Batch [150/499] Loss: 0.2764 Dice: 0.6178 LR: 0.000500
  Batch [200/499] Loss: 0.2119 Dice: 0.7083 LR: 0.000500
  Batch [250/499] Loss: 0.2883 Dice: 0.5948 LR: 0.000500
  Batch [300/499] Loss: 0.1877 Dice: 0.7449 LR: 0.000500
  Batch [350/499] Loss: 0.2915 Dice: 0.5942 LR: 0.000500
  Batch [400/499] Loss: 0.2423 Dice: 0.6597 LR: 0.000500
  Batch [450/499] Loss: 0.3053 Dice: 0.5712 LR: 0.000500
Epoch 123 Train: 	[total] 0.2653	[shape] 0.2647	[dice] 0.6288	[l2] 6.6653
  Training time: 87.86s
  Total epoch time: 87.86s (no validation)

Epoch 124/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2931 Dice: 0.5884 LR: 0.000500
  Batch [50/499] Loss: 0.2145 Dice: 0.7015 LR: 0.000500
  Batch [100/499] Loss: 0.2063 Dice: 0.7132 LR: 0.000500
  Batch [150/499] Loss: 0.2870 Dice: 0.5958 LR: 0.000500
  Batch [200/499] Loss: 0.3205 Dice: 0.5532 LR: 0.000500
  Batch [250/499] Loss: 0.2351 Dice: 0.6762 LR: 0.000500
  Batch [300/499] Loss: 0.3171 Dice: 0.5561 LR: 0.000500
  Batch [350/499] Loss: 0.2906 Dice: 0.5930 LR: 0.000500
  Batch [400/499] Loss: 0.2680 Dice: 0.6287 LR: 0.000500
  Batch [450/499] Loss: 0.2148 Dice: 0.6958 LR: 0.000500
Epoch 124 Train: 	[total] 0.2642	[shape] 0.2636	[dice] 0.6305	[l2] 6.6660
  Training time: 87.69s
  Total epoch time: 87.69s (no validation)

Epoch 125/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2234 Dice: 0.6900 LR: 0.000500
  Batch [50/499] Loss: 0.3365 Dice: 0.5268 LR: 0.000500
  Batch [100/499] Loss: 0.2278 Dice: 0.6784 LR: 0.000500
  Batch [150/499] Loss: 0.3410 Dice: 0.5241 LR: 0.000500
  Batch [200/499] Loss: 0.2497 Dice: 0.6512 LR: 0.000500
  Batch [250/499] Loss: 0.2482 Dice: 0.6516 LR: 0.000500
  Batch [300/499] Loss: 0.2926 Dice: 0.5840 LR: 0.000500
  Batch [350/499] Loss: 0.2140 Dice: 0.6991 LR: 0.000500
  Batch [400/499] Loss: 0.2884 Dice: 0.5965 LR: 0.000500
  Batch [450/499] Loss: 0.2205 Dice: 0.6917 LR: 0.000500
Epoch 125 Train: 	[total] 0.2642	[shape] 0.2635	[dice] 0.6303	[l2] 6.6670
  Training time: 87.49s
  Total epoch time: 87.49s (no validation)

Epoch 126/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3509 Dice: 0.5110 LR: 0.000500
  Batch [50/499] Loss: 0.2702 Dice: 0.6202 LR: 0.000500
  Batch [100/499] Loss: 0.1843 Dice: 0.7426 LR: 0.000500
  Batch [150/499] Loss: 0.3201 Dice: 0.5518 LR: 0.000500
  Batch [200/499] Loss: 0.3099 Dice: 0.5662 LR: 0.000500
  Batch [250/499] Loss: 0.2725 Dice: 0.6180 LR: 0.000500
  Batch [300/499] Loss: 0.2282 Dice: 0.6722 LR: 0.000500
  Batch [350/499] Loss: 0.2411 Dice: 0.6665 LR: 0.000500
  Batch [400/499] Loss: 0.2660 Dice: 0.6333 LR: 0.000500
  Batch [450/499] Loss: 0.2364 Dice: 0.6708 LR: 0.000500
Epoch 126 Train: 	[total] 0.2643	[shape] 0.2636	[dice] 0.6302	[l2] 6.6677
  Training time: 87.43s
  Total epoch time: 87.43s (no validation)

Epoch 127/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3248 Dice: 0.5349 LR: 0.000500
  Batch [50/499] Loss: 0.2636 Dice: 0.6313 LR: 0.000500
  Batch [100/499] Loss: 0.2605 Dice: 0.6336 LR: 0.000500
  Batch [150/499] Loss: 0.2909 Dice: 0.5927 LR: 0.000500
  Batch [200/499] Loss: 0.3037 Dice: 0.5756 LR: 0.000500
  Batch [250/499] Loss: 0.2229 Dice: 0.6893 LR: 0.000500
  Batch [300/499] Loss: 0.2742 Dice: 0.6121 LR: 0.000500
  Batch [350/499] Loss: 0.2941 Dice: 0.5853 LR: 0.000500
  Batch [400/499] Loss: 0.3654 Dice: 0.4893 LR: 0.000500
  Batch [450/499] Loss: 0.2115 Dice: 0.7071 LR: 0.000500
Epoch 127 Train: 	[total] 0.2635	[shape] 0.2628	[dice] 0.6313	[l2] 6.6686
  Training time: 87.69s
  Total epoch time: 87.69s (no validation)

Epoch 128/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2764 Dice: 0.6072 LR: 0.000500
  Batch [50/499] Loss: 0.2923 Dice: 0.5882 LR: 0.000500
  Batch [100/499] Loss: 0.2289 Dice: 0.6794 LR: 0.000500
  Batch [150/499] Loss: 0.2478 Dice: 0.6552 LR: 0.000500
  Batch [200/499] Loss: 0.2551 Dice: 0.6431 LR: 0.000500
  Batch [250/499] Loss: 0.2077 Dice: 0.7062 LR: 0.000500
  Batch [300/499] Loss: 0.3256 Dice: 0.5436 LR: 0.000500
  Batch [350/499] Loss: 0.3063 Dice: 0.5699 LR: 0.000500
  Batch [400/499] Loss: 0.3167 Dice: 0.5564 LR: 0.000500
  Batch [450/499] Loss: 0.2931 Dice: 0.5951 LR: 0.000500
Epoch 128 Train: 	[total] 0.2636	[shape] 0.2630	[dice] 0.6310	[l2] 6.6694
  Training time: 87.83s
  Total epoch time: 87.83s (no validation)

Epoch 129/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2667 Dice: 0.6257 LR: 0.000500
  Batch [50/499] Loss: 0.2788 Dice: 0.6079 LR: 0.000500
  Batch [100/499] Loss: 0.3156 Dice: 0.5642 LR: 0.000500
  Batch [150/499] Loss: 0.2420 Dice: 0.6587 LR: 0.000500
  Batch [200/499] Loss: 0.1958 Dice: 0.7266 LR: 0.000500
  Batch [250/499] Loss: 0.3402 Dice: 0.5190 LR: 0.000500
  Batch [300/499] Loss: 0.2477 Dice: 0.6490 LR: 0.000500
  Batch [350/499] Loss: 0.2902 Dice: 0.5927 LR: 0.000500
  Batch [400/499] Loss: 0.2280 Dice: 0.6789 LR: 0.000500
  Batch [450/499] Loss: 0.2089 Dice: 0.7079 LR: 0.000500
Epoch 129 Train: 	[total] 0.2639	[shape] 0.2632	[dice] 0.6306	[l2] 6.6703
  Training time: 87.84s
  Total epoch time: 87.84s (no validation)

Epoch 130/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2968 Dice: 0.5834 LR: 0.000500
  Batch [50/499] Loss: 0.2466 Dice: 0.6575 LR: 0.000500
  Batch [100/499] Loss: 0.2013 Dice: 0.7174 LR: 0.000500
  Batch [150/499] Loss: 0.2995 Dice: 0.5739 LR: 0.000500
  Batch [200/499] Loss: 0.2517 Dice: 0.6503 LR: 0.000500
  Batch [250/499] Loss: 0.2872 Dice: 0.5989 LR: 0.000500
  Batch [300/499] Loss: 0.2801 Dice: 0.6073 LR: 0.000500
  Batch [350/499] Loss: 0.2623 Dice: 0.6355 LR: 0.000500
  Batch [400/499] Loss: 0.2569 Dice: 0.6374 LR: 0.000500
  Batch [450/499] Loss: 0.2116 Dice: 0.7011 LR: 0.000500
Epoch 130 Train: 	[total] 0.2630	[shape] 0.2623	[dice] 0.6320	[l2] 6.6711
  Training time: 87.90s
Epoch 130 Eval:  	[total] 0.6399	[shape] 0.6393	[dice] 0.0927	[l2] 6.6720
  Validation time: 203.04s
======================================================================
Found a new best model! Shape Loss: 0.639279
======================================================================
  Total epoch time: 290.94s

Epoch 131/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2826 Dice: 0.6037 LR: 0.000500
  Batch [50/499] Loss: 0.3255 Dice: 0.5440 LR: 0.000500
  Batch [100/499] Loss: 0.2738 Dice: 0.6166 LR: 0.000500
  Batch [150/499] Loss: 0.2073 Dice: 0.7102 LR: 0.000500
  Batch [200/499] Loss: 0.2190 Dice: 0.6949 LR: 0.000500
  Batch [250/499] Loss: 0.3148 Dice: 0.5636 LR: 0.000500
  Batch [300/499] Loss: 0.2516 Dice: 0.6519 LR: 0.000500
  Batch [350/499] Loss: 0.2545 Dice: 0.6418 LR: 0.000500
  Batch [400/499] Loss: 0.2225 Dice: 0.6904 LR: 0.000500
  Batch [450/499] Loss: 0.1491 Dice: 0.7950 LR: 0.000500
Epoch 131 Train: 	[total] 0.2638	[shape] 0.2632	[dice] 0.6307	[l2] 6.6720
  Training time: 87.94s
  Total epoch time: 87.94s (no validation)

Epoch 132/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2052 Dice: 0.7158 LR: 0.000500
  Batch [50/499] Loss: 0.3705 Dice: 0.4781 LR: 0.000500
  Batch [100/499] Loss: 0.2099 Dice: 0.7073 LR: 0.000500
  Batch [150/499] Loss: 0.1884 Dice: 0.7391 LR: 0.000500
  Batch [200/499] Loss: 0.2407 Dice: 0.6634 LR: 0.000500
  Batch [250/499] Loss: 0.2308 Dice: 0.6752 LR: 0.000500
  Batch [300/499] Loss: 0.3223 Dice: 0.5466 LR: 0.000500
  Batch [350/499] Loss: 0.1642 Dice: 0.7694 LR: 0.000500
  Batch [400/499] Loss: 0.2913 Dice: 0.5971 LR: 0.000500
  Batch [450/499] Loss: 0.3414 Dice: 0.5173 LR: 0.000500
Epoch 132 Train: 	[total] 0.2624	[shape] 0.2618	[dice] 0.6326	[l2] 6.6729
  Training time: 87.81s
  Total epoch time: 87.81s (no validation)

Epoch 133/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1932 Dice: 0.7321 LR: 0.000500
  Batch [50/499] Loss: 0.2756 Dice: 0.6111 LR: 0.000500
  Batch [100/499] Loss: 0.2884 Dice: 0.5954 LR: 0.000500
  Batch [150/499] Loss: 0.2518 Dice: 0.6512 LR: 0.000500
  Batch [200/499] Loss: 0.2919 Dice: 0.5890 LR: 0.000500
  Batch [250/499] Loss: 0.3175 Dice: 0.5537 LR: 0.000500
  Batch [300/499] Loss: 0.2979 Dice: 0.5794 LR: 0.000500
  Batch [350/499] Loss: 0.3505 Dice: 0.5116 LR: 0.000500
  Batch [400/499] Loss: 0.2428 Dice: 0.6639 LR: 0.000500
  Batch [450/499] Loss: 0.2436 Dice: 0.6567 LR: 0.000500
Epoch 133 Train: 	[total] 0.2623	[shape] 0.2617	[dice] 0.6329	[l2] 6.6737
  Training time: 87.72s
  Total epoch time: 87.72s (no validation)

Epoch 134/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2643 Dice: 0.6296 LR: 0.000500
  Batch [50/499] Loss: 0.2684 Dice: 0.6289 LR: 0.000500
  Batch [100/499] Loss: 0.2241 Dice: 0.6916 LR: 0.000500
  Batch [150/499] Loss: 0.3090 Dice: 0.5690 LR: 0.000500
  Batch [200/499] Loss: 0.3181 Dice: 0.5526 LR: 0.000500
  Batch [250/499] Loss: 0.2474 Dice: 0.6520 LR: 0.000500
  Batch [300/499] Loss: 0.3145 Dice: 0.5631 LR: 0.000500
  Batch [350/499] Loss: 0.2383 Dice: 0.6645 LR: 0.000500
  Batch [400/499] Loss: 0.2413 Dice: 0.6665 LR: 0.000500
  Batch [450/499] Loss: 0.1847 Dice: 0.7403 LR: 0.000500
Epoch 134 Train: 	[total] 0.2616	[shape] 0.2609	[dice] 0.6340	[l2] 6.6746
  Training time: 87.75s
  Total epoch time: 87.75s (no validation)

Epoch 135/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3153 Dice: 0.5528 LR: 0.000500
  Batch [50/499] Loss: 0.3733 Dice: 0.4737 LR: 0.000500
  Batch [100/499] Loss: 0.2240 Dice: 0.6886 LR: 0.000500
  Batch [150/499] Loss: 0.2444 Dice: 0.6590 LR: 0.000500
  Batch [200/499] Loss: 0.2088 Dice: 0.7093 LR: 0.000500
  Batch [250/499] Loss: 0.3774 Dice: 0.4703 LR: 0.000500
  Batch [300/499] Loss: 0.2965 Dice: 0.5772 LR: 0.000500
  Batch [350/499] Loss: 0.3590 Dice: 0.4985 LR: 0.000500
  Batch [400/499] Loss: 0.3494 Dice: 0.5080 LR: 0.000500
  Batch [450/499] Loss: 0.2222 Dice: 0.6887 LR: 0.000500
Epoch 135 Train: 	[total] 0.2625	[shape] 0.2619	[dice] 0.6328	[l2] 6.6754
  Training time: 88.07s
  Total epoch time: 88.07s (no validation)

Epoch 136/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3165 Dice: 0.5576 LR: 0.000500
  Batch [50/499] Loss: 0.2695 Dice: 0.6260 LR: 0.000500
  Batch [100/499] Loss: 0.2457 Dice: 0.6644 LR: 0.000500
  Batch [150/499] Loss: 0.3083 Dice: 0.5705 LR: 0.000500
  Batch [200/499] Loss: 0.2683 Dice: 0.6245 LR: 0.000500
  Batch [250/499] Loss: 0.2034 Dice: 0.7178 LR: 0.000500
  Batch [300/499] Loss: 0.3192 Dice: 0.5513 LR: 0.000500
  Batch [350/499] Loss: 0.2179 Dice: 0.6949 LR: 0.000500
  Batch [400/499] Loss: 0.2063 Dice: 0.7093 LR: 0.000500
  Batch [450/499] Loss: 0.2123 Dice: 0.6998 LR: 0.000500
Epoch 136 Train: 	[total] 0.2611	[shape] 0.2605	[dice] 0.6346	[l2] 6.6763
  Training time: 87.94s
  Total epoch time: 87.94s (no validation)

Epoch 137/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2810 Dice: 0.6028 LR: 0.000500
  Batch [50/499] Loss: 0.2368 Dice: 0.6697 LR: 0.000500
  Batch [100/499] Loss: 0.2123 Dice: 0.7019 LR: 0.000500
  Batch [150/499] Loss: 0.3088 Dice: 0.5684 LR: 0.000500
  Batch [200/499] Loss: 0.1354 Dice: 0.8112 LR: 0.000500
  Batch [250/499] Loss: 0.2831 Dice: 0.6057 LR: 0.000500
  Batch [300/499] Loss: 0.2306 Dice: 0.6759 LR: 0.000500
  Batch [350/499] Loss: 0.2615 Dice: 0.6370 LR: 0.000500
  Batch [400/499] Loss: 0.3365 Dice: 0.5291 LR: 0.000500
  Batch [450/499] Loss: 0.2925 Dice: 0.5883 LR: 0.000500
Epoch 137 Train: 	[total] 0.2616	[shape] 0.2609	[dice] 0.6341	[l2] 6.6771
  Training time: 88.07s
  Total epoch time: 88.07s (no validation)

Epoch 138/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2749 Dice: 0.6135 LR: 0.000500
  Batch [50/499] Loss: 0.2536 Dice: 0.6487 LR: 0.000500
  Batch [100/499] Loss: 0.2437 Dice: 0.6578 LR: 0.000500
  Batch [150/499] Loss: 0.2621 Dice: 0.6352 LR: 0.000500
  Batch [200/499] Loss: 0.2193 Dice: 0.6925 LR: 0.000500
  Batch [250/499] Loss: 0.2756 Dice: 0.6112 LR: 0.000500
  Batch [300/499] Loss: 0.2249 Dice: 0.6915 LR: 0.000500
  Batch [350/499] Loss: 0.3663 Dice: 0.4874 LR: 0.000500
  Batch [400/499] Loss: 0.1442 Dice: 0.8043 LR: 0.000500
  Batch [450/499] Loss: 0.2302 Dice: 0.6763 LR: 0.000500
Epoch 138 Train: 	[total] 0.2618	[shape] 0.2612	[dice] 0.6337	[l2] 6.6781
  Training time: 87.71s
  Total epoch time: 87.71s (no validation)

Epoch 139/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2640 Dice: 0.6360 LR: 0.000500
  Batch [50/499] Loss: 0.3021 Dice: 0.5775 LR: 0.000500
  Batch [100/499] Loss: 0.2961 Dice: 0.5833 LR: 0.000500
  Batch [150/499] Loss: 0.2315 Dice: 0.6762 LR: 0.000500
  Batch [200/499] Loss: 0.2799 Dice: 0.6129 LR: 0.000500
  Batch [250/499] Loss: 0.3195 Dice: 0.5515 LR: 0.000500
  Batch [300/499] Loss: 0.3502 Dice: 0.5080 LR: 0.000500
  Batch [350/499] Loss: 0.2668 Dice: 0.6258 LR: 0.000500
  Batch [400/499] Loss: 0.2588 Dice: 0.6376 LR: 0.000500
  Batch [450/499] Loss: 0.3198 Dice: 0.5521 LR: 0.000500
Epoch 139 Train: 	[total] 0.2612	[shape] 0.2605	[dice] 0.6345	[l2] 6.6788
  Training time: 87.82s
  Total epoch time: 87.82s (no validation)

Epoch 140/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2439 Dice: 0.6601 LR: 0.000500
  Batch [50/499] Loss: 0.2798 Dice: 0.6098 LR: 0.000500
  Batch [100/499] Loss: 0.2361 Dice: 0.6669 LR: 0.000500
  Batch [150/499] Loss: 0.1954 Dice: 0.7283 LR: 0.000500
  Batch [200/499] Loss: 0.2075 Dice: 0.7132 LR: 0.000500
  Batch [250/499] Loss: 0.2976 Dice: 0.5804 LR: 0.000500
  Batch [300/499] Loss: 0.2644 Dice: 0.6310 LR: 0.000500
  Batch [350/499] Loss: 0.3484 Dice: 0.5111 LR: 0.000500
  Batch [400/499] Loss: 0.2605 Dice: 0.6363 LR: 0.000500
  Batch [450/499] Loss: 0.2710 Dice: 0.6197 LR: 0.000500
Epoch 140 Train: 	[total] 0.2612	[shape] 0.2605	[dice] 0.6347	[l2] 6.6797
  Training time: 88.15s
Epoch 140 Eval:  	[total] 0.6375	[shape] 0.6368	[dice] 0.0964	[l2] 6.6805
  Validation time: 203.40s
======================================================================
Found a new best model! Shape Loss: 0.636805
======================================================================
  Total epoch time: 291.55s

Epoch 141/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2307 Dice: 0.6784 LR: 0.000500
  Batch [50/499] Loss: 0.3998 Dice: 0.4379 LR: 0.000500
  Batch [100/499] Loss: 0.2902 Dice: 0.5921 LR: 0.000500
  Batch [150/499] Loss: 0.2439 Dice: 0.6587 LR: 0.000500
  Batch [200/499] Loss: 0.2302 Dice: 0.6749 LR: 0.000500
  Batch [250/499] Loss: 0.2106 Dice: 0.7031 LR: 0.000500
  Batch [300/499] Loss: 0.2596 Dice: 0.6405 LR: 0.000500
  Batch [350/499] Loss: 0.2198 Dice: 0.6952 LR: 0.000500
  Batch [400/499] Loss: 0.2142 Dice: 0.6978 LR: 0.000500
  Batch [450/499] Loss: 0.2986 Dice: 0.5862 LR: 0.000500
Epoch 141 Train: 	[total] 0.2611	[shape] 0.2604	[dice] 0.6349	[l2] 6.6805
  Training time: 87.93s
  Total epoch time: 87.93s (no validation)

Epoch 142/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3109 Dice: 0.5653 LR: 0.000500
  Batch [50/499] Loss: 0.2811 Dice: 0.6095 LR: 0.000500
  Batch [100/499] Loss: 0.2996 Dice: 0.5738 LR: 0.000500
  Batch [150/499] Loss: 0.4113 Dice: 0.4232 LR: 0.000500
  Batch [200/499] Loss: 0.3263 Dice: 0.5356 LR: 0.000500
  Batch [250/499] Loss: 0.2254 Dice: 0.6895 LR: 0.000500
  Batch [300/499] Loss: 0.3023 Dice: 0.5706 LR: 0.000500
  Batch [350/499] Loss: 0.2236 Dice: 0.6872 LR: 0.000500
  Batch [400/499] Loss: 0.2530 Dice: 0.6443 LR: 0.000500
  Batch [450/499] Loss: 0.2310 Dice: 0.6799 LR: 0.000500
Epoch 142 Train: 	[total] 0.2606	[shape] 0.2600	[dice] 0.6353	[l2] 6.6813
  Training time: 87.76s
  Total epoch time: 87.76s (no validation)

Epoch 143/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2812 Dice: 0.6074 LR: 0.000500
  Batch [50/499] Loss: 0.2752 Dice: 0.6141 LR: 0.000500
  Batch [100/499] Loss: 0.3105 Dice: 0.5669 LR: 0.000500
  Batch [150/499] Loss: 0.2248 Dice: 0.6838 LR: 0.000500
  Batch [200/499] Loss: 0.1585 Dice: 0.7824 LR: 0.000500
  Batch [250/499] Loss: 0.3528 Dice: 0.5059 LR: 0.000500
  Batch [300/499] Loss: 0.2380 Dice: 0.6648 LR: 0.000500
  Batch [350/499] Loss: 0.2537 Dice: 0.6453 LR: 0.000500
  Batch [400/499] Loss: 0.3502 Dice: 0.5095 LR: 0.000500
  Batch [450/499] Loss: 0.2807 Dice: 0.6024 LR: 0.000500
Epoch 143 Train: 	[total] 0.2608	[shape] 0.2602	[dice] 0.6351	[l2] 6.6822
  Training time: 88.05s
  Total epoch time: 88.05s (no validation)

Epoch 144/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2316 Dice: 0.6729 LR: 0.000500
  Batch [50/499] Loss: 0.2172 Dice: 0.6963 LR: 0.000500
  Batch [100/499] Loss: 0.2365 Dice: 0.6717 LR: 0.000500
  Batch [150/499] Loss: 0.2297 Dice: 0.6826 LR: 0.000500
  Batch [200/499] Loss: 0.2776 Dice: 0.6077 LR: 0.000500
  Batch [250/499] Loss: 0.2819 Dice: 0.6015 LR: 0.000500
  Batch [300/499] Loss: 0.1913 Dice: 0.7366 LR: 0.000500
  Batch [350/499] Loss: 0.2246 Dice: 0.6924 LR: 0.000500
  Batch [400/499] Loss: 0.3175 Dice: 0.5558 LR: 0.000500
  Batch [450/499] Loss: 0.1789 Dice: 0.7513 LR: 0.000500
Epoch 144 Train: 	[total] 0.2608	[shape] 0.2601	[dice] 0.6351	[l2] 6.6829
  Training time: 87.87s
  Total epoch time: 87.87s (no validation)

Epoch 145/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2817 Dice: 0.6039 LR: 0.000500
  Batch [50/499] Loss: 0.1923 Dice: 0.7351 LR: 0.000500
  Batch [100/499] Loss: 0.2283 Dice: 0.6839 LR: 0.000500
  Batch [150/499] Loss: 0.2295 Dice: 0.6852 LR: 0.000500
  Batch [200/499] Loss: 0.2688 Dice: 0.6231 LR: 0.000500
  Batch [250/499] Loss: 0.1969 Dice: 0.7234 LR: 0.000500
  Batch [300/499] Loss: 0.2858 Dice: 0.6019 LR: 0.000500
  Batch [350/499] Loss: 0.2287 Dice: 0.6795 LR: 0.000500
  Batch [400/499] Loss: 0.2629 Dice: 0.6330 LR: 0.000500
  Batch [450/499] Loss: 0.2325 Dice: 0.6754 LR: 0.000500
Epoch 145 Train: 	[total] 0.2607	[shape] 0.2600	[dice] 0.6350	[l2] 6.6838
  Training time: 88.23s
  Total epoch time: 88.23s (no validation)

Epoch 146/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3002 Dice: 0.5788 LR: 0.000500
  Batch [50/499] Loss: 0.2761 Dice: 0.6082 LR: 0.000500
  Batch [100/499] Loss: 0.2761 Dice: 0.6111 LR: 0.000500
  Batch [150/499] Loss: 0.2868 Dice: 0.6019 LR: 0.000500
  Batch [200/499] Loss: 0.2277 Dice: 0.6783 LR: 0.000500
  Batch [250/499] Loss: 0.2493 Dice: 0.6557 LR: 0.000500
  Batch [300/499] Loss: 0.2498 Dice: 0.6535 LR: 0.000500
  Batch [350/499] Loss: 0.2258 Dice: 0.6847 LR: 0.000500
  Batch [400/499] Loss: 0.2961 Dice: 0.5851 LR: 0.000500
  Batch [450/499] Loss: 0.2542 Dice: 0.6475 LR: 0.000500
Epoch 146 Train: 	[total] 0.2592	[shape] 0.2585	[dice] 0.6373	[l2] 6.6847
  Training time: 88.00s
  Total epoch time: 88.00s (no validation)

Epoch 147/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2171 Dice: 0.6948 LR: 0.000500
  Batch [50/499] Loss: 0.2001 Dice: 0.7194 LR: 0.000500
  Batch [100/499] Loss: 0.2336 Dice: 0.6807 LR: 0.000500
  Batch [150/499] Loss: 0.3531 Dice: 0.5059 LR: 0.000500
  Batch [200/499] Loss: 0.2375 Dice: 0.6704 LR: 0.000500
  Batch [250/499] Loss: 0.1743 Dice: 0.7587 LR: 0.000500
  Batch [300/499] Loss: 0.1747 Dice: 0.7551 LR: 0.000500
  Batch [350/499] Loss: 0.3206 Dice: 0.5527 LR: 0.000500
  Batch [400/499] Loss: 0.2137 Dice: 0.7032 LR: 0.000500
  Batch [450/499] Loss: 0.3381 Dice: 0.5262 LR: 0.000500
Epoch 147 Train: 	[total] 0.2606	[shape] 0.2600	[dice] 0.6352	[l2] 6.6855
  Training time: 87.83s
  Total epoch time: 87.83s (no validation)

Epoch 148/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2342 Dice: 0.6704 LR: 0.000500
  Batch [50/499] Loss: 0.3067 Dice: 0.5688 LR: 0.000500
  Batch [100/499] Loss: 0.2572 Dice: 0.6395 LR: 0.000500
  Batch [150/499] Loss: 0.2990 Dice: 0.5809 LR: 0.000500
  Batch [200/499] Loss: 0.3748 Dice: 0.4745 LR: 0.000500
  Batch [250/499] Loss: 0.3487 Dice: 0.5064 LR: 0.000500
  Batch [300/499] Loss: 0.3084 Dice: 0.5672 LR: 0.000500
  Batch [350/499] Loss: 0.4352 Dice: 0.3824 LR: 0.000500
  Batch [400/499] Loss: 0.3426 Dice: 0.5179 LR: 0.000500
  Batch [450/499] Loss: 0.3411 Dice: 0.5207 LR: 0.000500
Epoch 148 Train: 	[total] 0.2608	[shape] 0.2602	[dice] 0.6352	[l2] 6.6863
  Training time: 87.71s
  Total epoch time: 87.71s (no validation)

Epoch 149/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2168 Dice: 0.6954 LR: 0.000500
  Batch [50/499] Loss: 0.3762 Dice: 0.4737 LR: 0.000500
  Batch [100/499] Loss: 0.2963 Dice: 0.5886 LR: 0.000500
  Batch [150/499] Loss: 0.2689 Dice: 0.6219 LR: 0.000500
  Batch [200/499] Loss: 0.1933 Dice: 0.7309 LR: 0.000500
  Batch [250/499] Loss: 0.2148 Dice: 0.7006 LR: 0.000500
  Batch [300/499] Loss: 0.2914 Dice: 0.5896 LR: 0.000500
  Batch [350/499] Loss: 0.2263 Dice: 0.6864 LR: 0.000500
  Batch [400/499] Loss: 0.2753 Dice: 0.6193 LR: 0.000500
  Batch [450/499] Loss: 0.2388 Dice: 0.6640 LR: 0.000500
Epoch 149 Train: 	[total] 0.2598	[shape] 0.2592	[dice] 0.6364	[l2] 6.6873
  Training time: 87.95s
  Total epoch time: 87.95s (no validation)

Epoch 150/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.4125 Dice: 0.4249 LR: 0.000500
  Batch [50/499] Loss: 0.2520 Dice: 0.6483 LR: 0.000500
  Batch [100/499] Loss: 0.2727 Dice: 0.6199 LR: 0.000500
  Batch [150/499] Loss: 0.2817 Dice: 0.6052 LR: 0.000500
  Batch [200/499] Loss: 0.3251 Dice: 0.5446 LR: 0.000500
  Batch [250/499] Loss: 0.2811 Dice: 0.6105 LR: 0.000500
  Batch [300/499] Loss: 0.2550 Dice: 0.6396 LR: 0.000500
  Batch [350/499] Loss: 0.3209 Dice: 0.5502 LR: 0.000500
  Batch [400/499] Loss: 0.2637 Dice: 0.6315 LR: 0.000500
  Batch [450/499] Loss: 0.3583 Dice: 0.4927 LR: 0.000500
Epoch 150 Train: 	[total] 0.2582	[shape] 0.2575	[dice] 0.6389	[l2] 6.6881
  Training time: 88.23s
Epoch 150 Eval:  	[total] 0.6377	[shape] 0.6370	[dice] 0.0961	[l2] 6.6889
  Validation time: 202.39s
  Total epoch time: 290.62s

Epoch 151/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3104 Dice: 0.5594 LR: 0.000500
  Batch [50/499] Loss: 0.3087 Dice: 0.5683 LR: 0.000500
  Batch [100/499] Loss: 0.2921 Dice: 0.5933 LR: 0.000500
  Batch [150/499] Loss: 0.2530 Dice: 0.6402 LR: 0.000500
  Batch [200/499] Loss: 0.2686 Dice: 0.6233 LR: 0.000500
  Batch [250/499] Loss: 0.1555 Dice: 0.7813 LR: 0.000500
  Batch [300/499] Loss: 0.1673 Dice: 0.7672 LR: 0.000500
  Batch [350/499] Loss: 0.2395 Dice: 0.6703 LR: 0.000500
  Batch [400/499] Loss: 0.2859 Dice: 0.6011 LR: 0.000500
  Batch [450/499] Loss: 0.1517 Dice: 0.7864 LR: 0.000500
Epoch 151 Train: 	[total] 0.2596	[shape] 0.2589	[dice] 0.6366	[l2] 6.6889
  Training time: 87.82s
  Total epoch time: 87.82s (no validation)

Epoch 152/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1987 Dice: 0.7143 LR: 0.000500
  Batch [50/499] Loss: 0.4010 Dice: 0.4377 LR: 0.000500
  Batch [100/499] Loss: 0.1505 Dice: 0.7911 LR: 0.000500
  Batch [150/499] Loss: 0.3049 Dice: 0.5738 LR: 0.000500
  Batch [200/499] Loss: 0.2373 Dice: 0.6747 LR: 0.000500
  Batch [250/499] Loss: 0.2736 Dice: 0.6209 LR: 0.000500
  Batch [300/499] Loss: 0.3023 Dice: 0.5736 LR: 0.000500
  Batch [350/499] Loss: 0.2120 Dice: 0.6985 LR: 0.000500
  Batch [400/499] Loss: 0.2689 Dice: 0.6190 LR: 0.000500
  Batch [450/499] Loss: 0.2537 Dice: 0.6459 LR: 0.000500
Epoch 152 Train: 	[total] 0.2599	[shape] 0.2593	[dice] 0.6363	[l2] 6.6898
  Training time: 87.84s
  Total epoch time: 87.84s (no validation)

Epoch 153/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3109 Dice: 0.5611 LR: 0.000500
  Batch [50/499] Loss: 0.2344 Dice: 0.6697 LR: 0.000500
  Batch [100/499] Loss: 0.2750 Dice: 0.6170 LR: 0.000500
  Batch [150/499] Loss: 0.3496 Dice: 0.5087 LR: 0.000500
  Batch [200/499] Loss: 0.1790 Dice: 0.7514 LR: 0.000500
  Batch [250/499] Loss: 0.3351 Dice: 0.5285 LR: 0.000500
  Batch [300/499] Loss: 0.2491 Dice: 0.6526 LR: 0.000500
  Batch [350/499] Loss: 0.2979 Dice: 0.5820 LR: 0.000500
  Batch [400/499] Loss: 0.2038 Dice: 0.7130 LR: 0.000500
  Batch [450/499] Loss: 0.2087 Dice: 0.7060 LR: 0.000500
Epoch 153 Train: 	[total] 0.2606	[shape] 0.2600	[dice] 0.6352	[l2] 6.6906
  Training time: 87.73s
  Total epoch time: 87.73s (no validation)

Epoch 154/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2492 Dice: 0.6562 LR: 0.000500
  Batch [50/499] Loss: 0.2053 Dice: 0.7116 LR: 0.000500
  Batch [100/499] Loss: 0.2237 Dice: 0.6910 LR: 0.000500
  Batch [150/499] Loss: 0.3035 Dice: 0.5784 LR: 0.000500
  Batch [200/499] Loss: 0.3490 Dice: 0.5082 LR: 0.000500
  Batch [250/499] Loss: 0.1993 Dice: 0.7184 LR: 0.000500
  Batch [300/499] Loss: 0.2765 Dice: 0.6137 LR: 0.000500
  Batch [350/499] Loss: 0.2352 Dice: 0.6714 LR: 0.000500
  Batch [400/499] Loss: 0.2288 Dice: 0.6783 LR: 0.000500
  Batch [450/499] Loss: 0.2719 Dice: 0.6161 LR: 0.000500
Epoch 154 Train: 	[total] 0.2590	[shape] 0.2583	[dice] 0.6375	[l2] 6.6914
  Training time: 87.72s
  Total epoch time: 87.72s (no validation)

Epoch 155/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2673 Dice: 0.6260 LR: 0.000500
  Batch [50/499] Loss: 0.3439 Dice: 0.5168 LR: 0.000500
  Batch [100/499] Loss: 0.1752 Dice: 0.7585 LR: 0.000500
  Batch [150/499] Loss: 0.2337 Dice: 0.6745 LR: 0.000500
  Batch [200/499] Loss: 0.2946 Dice: 0.5870 LR: 0.000500
  Batch [250/499] Loss: 0.3241 Dice: 0.5462 LR: 0.000500
  Batch [300/499] Loss: 0.3160 Dice: 0.5617 LR: 0.000500
  Batch [350/499] Loss: 0.2434 Dice: 0.6622 LR: 0.000500
  Batch [400/499] Loss: 0.1007 Dice: 0.8639 LR: 0.000500
  Batch [450/499] Loss: 0.1833 Dice: 0.7437 LR: 0.000500
Epoch 155 Train: 	[total] 0.2592	[shape] 0.2585	[dice] 0.6373	[l2] 6.6924
  Training time: 88.07s
  Total epoch time: 88.07s (no validation)

Epoch 156/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3056 Dice: 0.5732 LR: 0.000500
  Batch [50/499] Loss: 0.2604 Dice: 0.6330 LR: 0.000500
  Batch [100/499] Loss: 0.2877 Dice: 0.6004 LR: 0.000500
  Batch [150/499] Loss: 0.2063 Dice: 0.7109 LR: 0.000500
  Batch [200/499] Loss: 0.1690 Dice: 0.7591 LR: 0.000500
  Batch [250/499] Loss: 0.2828 Dice: 0.6011 LR: 0.000500
  Batch [300/499] Loss: 0.2508 Dice: 0.6454 LR: 0.000500
  Batch [350/499] Loss: 0.2390 Dice: 0.6651 LR: 0.000500
  Batch [400/499] Loss: 0.3907 Dice: 0.4491 LR: 0.000500
  Batch [450/499] Loss: 0.2205 Dice: 0.6924 LR: 0.000500
Epoch 156 Train: 	[total] 0.2579	[shape] 0.2572	[dice] 0.6393	[l2] 6.6931
  Training time: 87.94s
  Total epoch time: 87.94s (no validation)

Epoch 157/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2565 Dice: 0.6427 LR: 0.000500
  Batch [50/499] Loss: 0.2882 Dice: 0.5936 LR: 0.000500
  Batch [100/499] Loss: 0.2065 Dice: 0.7147 LR: 0.000500
  Batch [150/499] Loss: 0.2874 Dice: 0.5952 LR: 0.000500
  Batch [200/499] Loss: 0.2940 Dice: 0.5887 LR: 0.000500
  Batch [250/499] Loss: 0.2556 Dice: 0.6442 LR: 0.000500
  Batch [300/499] Loss: 0.2915 Dice: 0.5957 LR: 0.000500
  Batch [350/499] Loss: 0.2677 Dice: 0.6268 LR: 0.000500
  Batch [400/499] Loss: 0.3239 Dice: 0.5425 LR: 0.000500
  Batch [450/499] Loss: 0.3276 Dice: 0.5435 LR: 0.000500
Epoch 157 Train: 	[total] 0.2592	[shape] 0.2586	[dice] 0.6373	[l2] 6.6939
  Training time: 87.89s
  Total epoch time: 87.89s (no validation)

Epoch 158/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2321 Dice: 0.6722 LR: 0.000500
  Batch [50/499] Loss: 0.2259 Dice: 0.6834 LR: 0.000500
  Batch [100/499] Loss: 0.2099 Dice: 0.7059 LR: 0.000500
  Batch [150/499] Loss: 0.2578 Dice: 0.6410 LR: 0.000500
  Batch [200/499] Loss: 0.2694 Dice: 0.6192 LR: 0.000500
  Batch [250/499] Loss: 0.2838 Dice: 0.6023 LR: 0.000500
  Batch [300/499] Loss: 0.3144 Dice: 0.5548 LR: 0.000500
  Batch [350/499] Loss: 0.2878 Dice: 0.5947 LR: 0.000500
  Batch [400/499] Loss: 0.2532 Dice: 0.6442 LR: 0.000500
  Batch [450/499] Loss: 0.2656 Dice: 0.6316 LR: 0.000500
Epoch 158 Train: 	[total] 0.2595	[shape] 0.2588	[dice] 0.6369	[l2] 6.6947
  Training time: 87.80s
  Total epoch time: 87.80s (no validation)

Epoch 159/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2353 Dice: 0.6782 LR: 0.000500
  Batch [50/499] Loss: 0.3051 Dice: 0.5691 LR: 0.000500
  Batch [100/499] Loss: 0.3061 Dice: 0.5678 LR: 0.000500
  Batch [150/499] Loss: 0.2220 Dice: 0.6878 LR: 0.000500
  Batch [200/499] Loss: 0.1980 Dice: 0.7263 LR: 0.000500
  Batch [250/499] Loss: 0.2442 Dice: 0.6619 LR: 0.000500
  Batch [300/499] Loss: 0.2427 Dice: 0.6603 LR: 0.000500
  Batch [350/499] Loss: 0.1384 Dice: 0.8129 LR: 0.000500
  Batch [400/499] Loss: 0.1947 Dice: 0.7273 LR: 0.000500
  Batch [450/499] Loss: 0.2765 Dice: 0.6153 LR: 0.000500
Epoch 159 Train: 	[total] 0.2584	[shape] 0.2577	[dice] 0.6385	[l2] 6.6956
  Training time: 87.87s
  Total epoch time: 87.87s (no validation)

Epoch 160/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3467 Dice: 0.5171 LR: 0.000500
  Batch [50/499] Loss: 0.2412 Dice: 0.6595 LR: 0.000500
  Batch [100/499] Loss: 0.2657 Dice: 0.6285 LR: 0.000500
  Batch [150/499] Loss: 0.2481 Dice: 0.6508 LR: 0.000500
  Batch [200/499] Loss: 0.2586 Dice: 0.6371 LR: 0.000500
  Batch [250/499] Loss: 0.3053 Dice: 0.5711 LR: 0.000500
  Batch [300/499] Loss: 0.2297 Dice: 0.6772 LR: 0.000500
  Batch [350/499] Loss: 0.3053 Dice: 0.5768 LR: 0.000500
  Batch [400/499] Loss: 0.2190 Dice: 0.6931 LR: 0.000500
  Batch [450/499] Loss: 0.2935 Dice: 0.5894 LR: 0.000500
Epoch 160 Train: 	[total] 0.2579	[shape] 0.2572	[dice] 0.6390	[l2] 6.6964
  Training time: 87.98s
Epoch 160 Eval:  	[total] 0.6389	[shape] 0.6382	[dice] 0.0945	[l2] 6.6972
  Validation time: 202.98s
  Total epoch time: 290.96s

Epoch 161/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1842 Dice: 0.7521 LR: 0.000500
  Batch [50/499] Loss: 0.3058 Dice: 0.5701 LR: 0.000500
  Batch [100/499] Loss: 0.3213 Dice: 0.5482 LR: 0.000500
  Batch [150/499] Loss: 0.2889 Dice: 0.5969 LR: 0.000500
  Batch [200/499] Loss: 0.2933 Dice: 0.5904 LR: 0.000500
  Batch [250/499] Loss: 0.2536 Dice: 0.6468 LR: 0.000500
  Batch [300/499] Loss: 0.1971 Dice: 0.7245 LR: 0.000500
  Batch [350/499] Loss: 0.2454 Dice: 0.6579 LR: 0.000500
  Batch [400/499] Loss: 0.2732 Dice: 0.6158 LR: 0.000500
  Batch [450/499] Loss: 0.2341 Dice: 0.6720 LR: 0.000500
Epoch 161 Train: 	[total] 0.2585	[shape] 0.2578	[dice] 0.6384	[l2] 6.6972
  Training time: 88.02s
  Total epoch time: 88.02s (no validation)

Epoch 162/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1897 Dice: 0.7336 LR: 0.000500
  Batch [50/499] Loss: 0.2712 Dice: 0.6220 LR: 0.000500
  Batch [100/499] Loss: 0.2006 Dice: 0.7203 LR: 0.000500
  Batch [150/499] Loss: 0.2557 Dice: 0.6452 LR: 0.000500
  Batch [200/499] Loss: 0.2026 Dice: 0.7205 LR: 0.000500
  Batch [250/499] Loss: 0.2365 Dice: 0.6715 LR: 0.000500
  Batch [300/499] Loss: 0.3317 Dice: 0.5362 LR: 0.000500
  Batch [350/499] Loss: 0.2702 Dice: 0.6220 LR: 0.000500
  Batch [400/499] Loss: 0.2136 Dice: 0.7038 LR: 0.000500
  Batch [450/499] Loss: 0.2362 Dice: 0.6685 LR: 0.000500
Epoch 162 Train: 	[total] 0.2579	[shape] 0.2572	[dice] 0.6390	[l2] 6.6982
  Training time: 87.77s
  Total epoch time: 87.77s (no validation)

Epoch 163/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3225 Dice: 0.5462 LR: 0.000500
  Batch [50/499] Loss: 0.2719 Dice: 0.6162 LR: 0.000500
  Batch [100/499] Loss: 0.3601 Dice: 0.4919 LR: 0.000500
  Batch [150/499] Loss: 0.1814 Dice: 0.7485 LR: 0.000500
  Batch [200/499] Loss: 0.3323 Dice: 0.5296 LR: 0.000500
  Batch [250/499] Loss: 0.2203 Dice: 0.6915 LR: 0.000500
  Batch [300/499] Loss: 0.1672 Dice: 0.7723 LR: 0.000500
  Batch [350/499] Loss: 0.2925 Dice: 0.5895 LR: 0.000500
  Batch [400/499] Loss: 0.1547 Dice: 0.7857 LR: 0.000500
  Batch [450/499] Loss: 0.1818 Dice: 0.7478 LR: 0.000500
Epoch 163 Train: 	[total] 0.2571	[shape] 0.2565	[dice] 0.6402	[l2] 6.6990
  Training time: 87.88s
  Total epoch time: 87.88s (no validation)

Epoch 164/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2928 Dice: 0.5894 LR: 0.000500
  Batch [50/499] Loss: 0.3607 Dice: 0.4912 LR: 0.000500
  Batch [100/499] Loss: 0.3112 Dice: 0.5639 LR: 0.000500
  Batch [150/499] Loss: 0.2933 Dice: 0.5857 LR: 0.000500
  Batch [200/499] Loss: 0.2331 Dice: 0.6740 LR: 0.000500
  Batch [250/499] Loss: 0.2086 Dice: 0.7089 LR: 0.000500
  Batch [300/499] Loss: 0.2501 Dice: 0.6485 LR: 0.000500
  Batch [350/499] Loss: 0.4353 Dice: 0.3861 LR: 0.000500
  Batch [400/499] Loss: 0.2947 Dice: 0.5865 LR: 0.000500
  Batch [450/499] Loss: 0.3663 Dice: 0.4835 LR: 0.000500
Epoch 164 Train: 	[total] 0.2577	[shape] 0.2570	[dice] 0.6394	[l2] 6.6998
  Training time: 87.96s
  Total epoch time: 87.96s (no validation)

Epoch 165/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2878 Dice: 0.5962 LR: 0.000500
  Batch [50/499] Loss: 0.2847 Dice: 0.6029 LR: 0.000500
  Batch [100/499] Loss: 0.2825 Dice: 0.6059 LR: 0.000500
  Batch [150/499] Loss: 0.3252 Dice: 0.5484 LR: 0.000500
  Batch [200/499] Loss: 0.1906 Dice: 0.7306 LR: 0.000500
  Batch [250/499] Loss: 0.2362 Dice: 0.6643 LR: 0.000500
  Batch [300/499] Loss: 0.3010 Dice: 0.5771 LR: 0.000500
  Batch [350/499] Loss: 0.2487 Dice: 0.6471 LR: 0.000500
  Batch [400/499] Loss: 0.2037 Dice: 0.7159 LR: 0.000500
  Batch [450/499] Loss: 0.2383 Dice: 0.6620 LR: 0.000500
Epoch 165 Train: 	[total] 0.2580	[shape] 0.2573	[dice] 0.6390	[l2] 6.7005
  Training time: 87.71s
  Total epoch time: 87.71s (no validation)

Epoch 166/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2619 Dice: 0.6306 LR: 0.000500
  Batch [50/499] Loss: 0.3290 Dice: 0.5429 LR: 0.000500
  Batch [100/499] Loss: 0.2839 Dice: 0.6074 LR: 0.000500
  Batch [150/499] Loss: 0.2537 Dice: 0.6451 LR: 0.000500
  Batch [200/499] Loss: 0.1834 Dice: 0.7485 LR: 0.000500
  Batch [250/499] Loss: 0.3009 Dice: 0.5797 LR: 0.000500
  Batch [300/499] Loss: 0.2118 Dice: 0.7043 LR: 0.000500
  Batch [350/499] Loss: 0.2981 Dice: 0.5821 LR: 0.000500
  Batch [400/499] Loss: 0.2791 Dice: 0.6073 LR: 0.000500
  Batch [450/499] Loss: 0.2325 Dice: 0.6811 LR: 0.000500
Epoch 166 Train: 	[total] 0.2586	[shape] 0.2579	[dice] 0.6382	[l2] 6.7012
  Training time: 87.82s
  Total epoch time: 87.82s (no validation)

Epoch 167/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2651 Dice: 0.6298 LR: 0.000500
  Batch [50/499] Loss: 0.2476 Dice: 0.6529 LR: 0.000500
  Batch [100/499] Loss: 0.3387 Dice: 0.5248 LR: 0.000500
  Batch [150/499] Loss: 0.2215 Dice: 0.6849 LR: 0.000500
  Batch [200/499] Loss: 0.3189 Dice: 0.5467 LR: 0.000500
  Batch [250/499] Loss: 0.3562 Dice: 0.5010 LR: 0.000500
  Batch [300/499] Loss: 0.2756 Dice: 0.6146 LR: 0.000500
  Batch [350/499] Loss: 0.1760 Dice: 0.7573 LR: 0.000500
  Batch [400/499] Loss: 0.3592 Dice: 0.4954 LR: 0.000500
  Batch [450/499] Loss: 0.3438 Dice: 0.5164 LR: 0.000500
Epoch 167 Train: 	[total] 0.2571	[shape] 0.2564	[dice] 0.6403	[l2] 6.7020
  Training time: 87.96s
  Total epoch time: 87.96s (no validation)

Epoch 168/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2342 Dice: 0.6732 LR: 0.000500
  Batch [50/499] Loss: 0.2504 Dice: 0.6472 LR: 0.000500
  Batch [100/499] Loss: 0.2749 Dice: 0.6182 LR: 0.000500
  Batch [150/499] Loss: 0.3005 Dice: 0.5790 LR: 0.000500
  Batch [200/499] Loss: 0.2167 Dice: 0.6987 LR: 0.000500
  Batch [250/499] Loss: 0.2666 Dice: 0.6251 LR: 0.000500
  Batch [300/499] Loss: 0.2416 Dice: 0.6626 LR: 0.000500
  Batch [350/499] Loss: 0.2938 Dice: 0.5898 LR: 0.000500
  Batch [400/499] Loss: 0.2047 Dice: 0.7199 LR: 0.000500
  Batch [450/499] Loss: 0.3030 Dice: 0.5712 LR: 0.000500
Epoch 168 Train: 	[total] 0.2576	[shape] 0.2569	[dice] 0.6395	[l2] 6.7030
  Training time: 87.65s
  Total epoch time: 87.65s (no validation)

Epoch 169/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2580 Dice: 0.6413 LR: 0.000500
  Batch [50/499] Loss: 0.3446 Dice: 0.5148 LR: 0.000500
  Batch [100/499] Loss: 0.1774 Dice: 0.7537 LR: 0.000500
  Batch [150/499] Loss: 0.2735 Dice: 0.6184 LR: 0.000500
  Batch [200/499] Loss: 0.2110 Dice: 0.7020 LR: 0.000500
  Batch [250/499] Loss: 0.3218 Dice: 0.5503 LR: 0.000500
  Batch [300/499] Loss: 0.2322 Dice: 0.6722 LR: 0.000500
  Batch [350/499] Loss: 0.3187 Dice: 0.5520 LR: 0.000500
  Batch [400/499] Loss: 0.2730 Dice: 0.6154 LR: 0.000500
  Batch [450/499] Loss: 0.3210 Dice: 0.5476 LR: 0.000500
Epoch 169 Train: 	[total] 0.2570	[shape] 0.2563	[dice] 0.6403	[l2] 6.7038
  Training time: 88.17s
  Total epoch time: 88.17s (no validation)

Epoch 170/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1974 Dice: 0.7232 LR: 0.000500
  Batch [50/499] Loss: 0.2690 Dice: 0.6264 LR: 0.000500
  Batch [100/499] Loss: 0.2919 Dice: 0.5900 LR: 0.000500
  Batch [150/499] Loss: 0.2081 Dice: 0.7128 LR: 0.000500
  Batch [200/499] Loss: 0.3311 Dice: 0.5333 LR: 0.000500
  Batch [250/499] Loss: 0.2707 Dice: 0.6204 LR: 0.000500
  Batch [300/499] Loss: 0.1960 Dice: 0.7270 LR: 0.000500
  Batch [350/499] Loss: 0.3295 Dice: 0.5450 LR: 0.000500
  Batch [400/499] Loss: 0.2076 Dice: 0.7089 LR: 0.000500
  Batch [450/499] Loss: 0.3868 Dice: 0.4533 LR: 0.000500
Epoch 170 Train: 	[total] 0.2566	[shape] 0.2559	[dice] 0.6409	[l2] 6.7045
  Training time: 87.70s
Epoch 170 Eval:  	[total] 0.6379	[shape] 0.6372	[dice] 0.0961	[l2] 6.7053
  Validation time: 202.61s
  Total epoch time: 290.31s

Epoch 171/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3628 Dice: 0.4909 LR: 0.000500
  Batch [50/499] Loss: 0.2513 Dice: 0.6481 LR: 0.000500
  Batch [100/499] Loss: 0.2786 Dice: 0.6097 LR: 0.000500
  Batch [150/499] Loss: 0.1985 Dice: 0.7271 LR: 0.000500
  Batch [200/499] Loss: 0.3476 Dice: 0.5106 LR: 0.000500
  Batch [250/499] Loss: 0.2013 Dice: 0.7188 LR: 0.000500
  Batch [300/499] Loss: 0.2719 Dice: 0.6241 LR: 0.000500
  Batch [350/499] Loss: 0.2168 Dice: 0.6991 LR: 0.000500
  Batch [400/499] Loss: 0.2616 Dice: 0.6299 LR: 0.000500
  Batch [450/499] Loss: 0.1788 Dice: 0.7506 LR: 0.000500
Epoch 171 Train: 	[total] 0.2560	[shape] 0.2553	[dice] 0.6420	[l2] 6.7054
  Training time: 87.87s
  Total epoch time: 87.87s (no validation)

Epoch 172/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1533 Dice: 0.7870 LR: 0.000500
  Batch [50/499] Loss: 0.2724 Dice: 0.6171 LR: 0.000500
  Batch [100/499] Loss: 0.2722 Dice: 0.6169 LR: 0.000500
  Batch [150/499] Loss: 0.2838 Dice: 0.6033 LR: 0.000500
  Batch [200/499] Loss: 0.3171 Dice: 0.5580 LR: 0.000500
  Batch [250/499] Loss: 0.2020 Dice: 0.7203 LR: 0.000500
  Batch [300/499] Loss: 0.2095 Dice: 0.7053 LR: 0.000500
  Batch [350/499] Loss: 0.2052 Dice: 0.7122 LR: 0.000500
  Batch [400/499] Loss: 0.2115 Dice: 0.7064 LR: 0.000500
  Batch [450/499] Loss: 0.3020 Dice: 0.5725 LR: 0.000500
Epoch 172 Train: 	[total] 0.2560	[shape] 0.2554	[dice] 0.6418	[l2] 6.7062
  Training time: 87.88s
  Total epoch time: 87.88s (no validation)

Epoch 173/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2411 Dice: 0.6618 LR: 0.000500
  Batch [50/499] Loss: 0.2409 Dice: 0.6643 LR: 0.000500
  Batch [100/499] Loss: 0.2725 Dice: 0.6189 LR: 0.000500
  Batch [150/499] Loss: 0.2881 Dice: 0.5962 LR: 0.000500
  Batch [200/499] Loss: 0.2144 Dice: 0.7037 LR: 0.000500
  Batch [250/499] Loss: 0.2799 Dice: 0.6092 LR: 0.000500
  Batch [300/499] Loss: 0.2049 Dice: 0.7146 LR: 0.000500
  Batch [350/499] Loss: 0.2711 Dice: 0.6239 LR: 0.000500
  Batch [400/499] Loss: 0.2763 Dice: 0.6179 LR: 0.000500
  Batch [450/499] Loss: 0.2236 Dice: 0.6860 LR: 0.000500
Epoch 173 Train: 	[total] 0.2568	[shape] 0.2562	[dice] 0.6408	[l2] 6.7072
  Training time: 87.74s
  Total epoch time: 87.74s (no validation)

Epoch 174/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2785 Dice: 0.6065 LR: 0.000500
  Batch [50/499] Loss: 0.2146 Dice: 0.7043 LR: 0.000500
  Batch [100/499] Loss: 0.2717 Dice: 0.6187 LR: 0.000500
  Batch [150/499] Loss: 0.2323 Dice: 0.6704 LR: 0.000500
  Batch [200/499] Loss: 0.2780 Dice: 0.6046 LR: 0.000500
  Batch [250/499] Loss: 0.3212 Dice: 0.5500 LR: 0.000500
  Batch [300/499] Loss: 0.1993 Dice: 0.7194 LR: 0.000500
  Batch [350/499] Loss: 0.3254 Dice: 0.5458 LR: 0.000500
  Batch [400/499] Loss: 0.2080 Dice: 0.7080 LR: 0.000500
  Batch [450/499] Loss: 0.2565 Dice: 0.6396 LR: 0.000500
Epoch 174 Train: 	[total] 0.2559	[shape] 0.2552	[dice] 0.6420	[l2] 6.7081
  Training time: 87.89s
  Total epoch time: 87.89s (no validation)

Epoch 175/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2853 Dice: 0.5995 LR: 0.000500
  Batch [50/499] Loss: 0.2345 Dice: 0.6695 LR: 0.000500
  Batch [100/499] Loss: 0.2471 Dice: 0.6533 LR: 0.000500
  Batch [150/499] Loss: 0.2762 Dice: 0.6149 LR: 0.000500
  Batch [200/499] Loss: 0.2120 Dice: 0.7047 LR: 0.000500
  Batch [250/499] Loss: 0.2397 Dice: 0.6637 LR: 0.000500
  Batch [300/499] Loss: 0.1749 Dice: 0.7565 LR: 0.000500
  Batch [350/499] Loss: 0.2314 Dice: 0.6767 LR: 0.000500
  Batch [400/499] Loss: 0.2548 Dice: 0.6442 LR: 0.000500
  Batch [450/499] Loss: 0.2550 Dice: 0.6432 LR: 0.000500
Epoch 175 Train: 	[total] 0.2570	[shape] 0.2563	[dice] 0.6404	[l2] 6.7089
  Training time: 88.01s
  Total epoch time: 88.01s (no validation)

Epoch 176/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1657 Dice: 0.7706 LR: 0.000500
  Batch [50/499] Loss: 0.2206 Dice: 0.6887 LR: 0.000500
  Batch [100/499] Loss: 0.1975 Dice: 0.7264 LR: 0.000500
  Batch [150/499] Loss: 0.2558 Dice: 0.6413 LR: 0.000500
  Batch [200/499] Loss: 0.2415 Dice: 0.6603 LR: 0.000500
  Batch [250/499] Loss: 0.1740 Dice: 0.7578 LR: 0.000500
  Batch [300/499] Loss: 0.3193 Dice: 0.5490 LR: 0.000500
  Batch [350/499] Loss: 0.2425 Dice: 0.6598 LR: 0.000500
  Batch [400/499] Loss: 0.2171 Dice: 0.6986 LR: 0.000500
  Batch [450/499] Loss: 0.2685 Dice: 0.6199 LR: 0.000500
Epoch 176 Train: 	[total] 0.2561	[shape] 0.2554	[dice] 0.6416	[l2] 6.7096
  Training time: 87.95s
  Total epoch time: 87.95s (no validation)

Epoch 177/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2598 Dice: 0.6346 LR: 0.000500
  Batch [50/499] Loss: 0.3876 Dice: 0.4582 LR: 0.000500
  Batch [100/499] Loss: 0.1477 Dice: 0.7958 LR: 0.000500
  Batch [150/499] Loss: 0.2657 Dice: 0.6248 LR: 0.000500
  Batch [200/499] Loss: 0.3189 Dice: 0.5511 LR: 0.000500
  Batch [250/499] Loss: 0.2340 Dice: 0.6760 LR: 0.000500
  Batch [300/499] Loss: 0.2535 Dice: 0.6421 LR: 0.000500
  Batch [350/499] Loss: 0.2261 Dice: 0.6844 LR: 0.000500
  Batch [400/499] Loss: 0.2729 Dice: 0.6156 LR: 0.000500
  Batch [450/499] Loss: 0.2489 Dice: 0.6517 LR: 0.000500
Epoch 177 Train: 	[total] 0.2565	[shape] 0.2558	[dice] 0.6410	[l2] 6.7105
  Training time: 87.88s
  Total epoch time: 87.88s (no validation)

Epoch 178/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2500 Dice: 0.6538 LR: 0.000500
  Batch [50/499] Loss: 0.1755 Dice: 0.7591 LR: 0.000500
  Batch [100/499] Loss: 0.3356 Dice: 0.5259 LR: 0.000500
  Batch [150/499] Loss: 0.2813 Dice: 0.6055 LR: 0.000500
  Batch [200/499] Loss: 0.2253 Dice: 0.6847 LR: 0.000500
  Batch [250/499] Loss: 0.1902 Dice: 0.7355 LR: 0.000500
  Batch [300/499] Loss: 0.3066 Dice: 0.5674 LR: 0.000500
  Batch [350/499] Loss: 0.2562 Dice: 0.6381 LR: 0.000500
  Batch [400/499] Loss: 0.3369 Dice: 0.5320 LR: 0.000500
  Batch [450/499] Loss: 0.2072 Dice: 0.7149 LR: 0.000500
Epoch 178 Train: 	[total] 0.2550	[shape] 0.2544	[dice] 0.6433	[l2] 6.7114
  Training time: 87.60s
  Total epoch time: 87.60s (no validation)

Epoch 179/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2387 Dice: 0.6690 LR: 0.000500
  Batch [50/499] Loss: 0.3572 Dice: 0.4990 LR: 0.000500
  Batch [100/499] Loss: 0.2654 Dice: 0.6344 LR: 0.000500
  Batch [150/499] Loss: 0.2582 Dice: 0.6381 LR: 0.000500
  Batch [200/499] Loss: 0.2283 Dice: 0.6768 LR: 0.000500
  Batch [250/499] Loss: 0.3279 Dice: 0.5394 LR: 0.000500
  Batch [300/499] Loss: 0.3139 Dice: 0.5606 LR: 0.000500
  Batch [350/499] Loss: 0.2351 Dice: 0.6681 LR: 0.000500
  Batch [400/499] Loss: 0.2464 Dice: 0.6576 LR: 0.000500
  Batch [450/499] Loss: 0.3054 Dice: 0.5688 LR: 0.000500
Epoch 179 Train: 	[total] 0.2549	[shape] 0.2542	[dice] 0.6434	[l2] 6.7122
  Training time: 87.80s
  Total epoch time: 87.80s (no validation)

Epoch 180/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2632 Dice: 0.6250 LR: 0.000500
  Batch [50/499] Loss: 0.2871 Dice: 0.5996 LR: 0.000500
  Batch [100/499] Loss: 0.2629 Dice: 0.6296 LR: 0.000500
  Batch [150/499] Loss: 0.1778 Dice: 0.7549 LR: 0.000500
  Batch [200/499] Loss: 0.2925 Dice: 0.5887 LR: 0.000500
  Batch [250/499] Loss: 0.2332 Dice: 0.6738 LR: 0.000500
  Batch [300/499] Loss: 0.2732 Dice: 0.6241 LR: 0.000500
  Batch [350/499] Loss: 0.2698 Dice: 0.6195 LR: 0.000500
  Batch [400/499] Loss: 0.3123 Dice: 0.5621 LR: 0.000500
  Batch [450/499] Loss: 0.2659 Dice: 0.6252 LR: 0.000500
Epoch 180 Train: 	[total] 0.2548	[shape] 0.2542	[dice] 0.6433	[l2] 6.7130
  Training time: 87.80s
Epoch 180 Eval:  	[total] 0.6355	[shape] 0.6348	[dice] 0.0994	[l2] 6.7139
  Validation time: 202.02s
======================================================================
Found a new best model! Shape Loss: 0.634806
======================================================================
  Total epoch time: 289.81s

Epoch 181/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2248 Dice: 0.6872 LR: 0.000500
  Batch [50/499] Loss: 0.2292 Dice: 0.6762 LR: 0.000500
  Batch [100/499] Loss: 0.3352 Dice: 0.5301 LR: 0.000500
  Batch [150/499] Loss: 0.3283 Dice: 0.5375 LR: 0.000500
  Batch [200/499] Loss: 0.2658 Dice: 0.6287 LR: 0.000500
  Batch [250/499] Loss: 0.4070 Dice: 0.4246 LR: 0.000500
  Batch [300/499] Loss: 0.2346 Dice: 0.6766 LR: 0.000500
  Batch [350/499] Loss: 0.3308 Dice: 0.5384 LR: 0.000500
  Batch [400/499] Loss: 0.4422 Dice: 0.3758 LR: 0.000500
  Batch [450/499] Loss: 0.2457 Dice: 0.6574 LR: 0.000500
Epoch 181 Train: 	[total] 0.2550	[shape] 0.2543	[dice] 0.6433	[l2] 6.7139
  Training time: 87.67s
  Total epoch time: 87.67s (no validation)

Epoch 182/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2039 Dice: 0.7162 LR: 0.000500
  Batch [50/499] Loss: 0.2043 Dice: 0.7118 LR: 0.000500
  Batch [100/499] Loss: 0.2850 Dice: 0.5998 LR: 0.000500
  Batch [150/499] Loss: 0.2146 Dice: 0.7008 LR: 0.000500
  Batch [200/499] Loss: 0.3482 Dice: 0.5097 LR: 0.000500
  Batch [250/499] Loss: 0.2492 Dice: 0.6519 LR: 0.000500
  Batch [300/499] Loss: 0.2327 Dice: 0.6739 LR: 0.000500
  Batch [350/499] Loss: 0.2202 Dice: 0.6913 LR: 0.000500
  Batch [400/499] Loss: 0.2406 Dice: 0.6665 LR: 0.000500
  Batch [450/499] Loss: 0.1834 Dice: 0.7442 LR: 0.000500
Epoch 182 Train: 	[total] 0.2547	[shape] 0.2541	[dice] 0.6436	[l2] 6.7148
  Training time: 87.80s
  Total epoch time: 87.80s (no validation)

Epoch 183/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1905 Dice: 0.7366 LR: 0.000500
  Batch [50/499] Loss: 0.3159 Dice: 0.5517 LR: 0.000500
  Batch [100/499] Loss: 0.2825 Dice: 0.6078 LR: 0.000500
  Batch [150/499] Loss: 0.3236 Dice: 0.5514 LR: 0.000500
  Batch [200/499] Loss: 0.3076 Dice: 0.5640 LR: 0.000500
  Batch [250/499] Loss: 0.2908 Dice: 0.5911 LR: 0.000500
  Batch [300/499] Loss: 0.3750 Dice: 0.4707 LR: 0.000500
  Batch [350/499] Loss: 0.2133 Dice: 0.7044 LR: 0.000500
  Batch [400/499] Loss: 0.2082 Dice: 0.7074 LR: 0.000500
  Batch [450/499] Loss: 0.2976 Dice: 0.5822 LR: 0.000500
Epoch 183 Train: 	[total] 0.2551	[shape] 0.2545	[dice] 0.6431	[l2] 6.7155
  Training time: 87.87s
  Total epoch time: 87.87s (no validation)

Epoch 184/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2605 Dice: 0.6365 LR: 0.000500
  Batch [50/499] Loss: 0.2566 Dice: 0.6406 LR: 0.000500
  Batch [100/499] Loss: 0.2629 Dice: 0.6310 LR: 0.000500
  Batch [150/499] Loss: 0.2500 Dice: 0.6511 LR: 0.000500
  Batch [200/499] Loss: 0.2316 Dice: 0.6792 LR: 0.000500
  Batch [250/499] Loss: 0.2501 Dice: 0.6498 LR: 0.000500
  Batch [300/499] Loss: 0.2402 Dice: 0.6636 LR: 0.000500
  Batch [350/499] Loss: 0.2151 Dice: 0.6994 LR: 0.000500
  Batch [400/499] Loss: 0.2694 Dice: 0.6209 LR: 0.000500
  Batch [450/499] Loss: 0.2992 Dice: 0.5810 LR: 0.000500
Epoch 184 Train: 	[total] 0.2554	[shape] 0.2548	[dice] 0.6426	[l2] 6.7163
  Training time: 87.88s
  Total epoch time: 87.88s (no validation)

Epoch 185/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3016 Dice: 0.5734 LR: 0.000500
  Batch [50/499] Loss: 0.3244 Dice: 0.5479 LR: 0.000500
  Batch [100/499] Loss: 0.3675 Dice: 0.4841 LR: 0.000500
  Batch [150/499] Loss: 0.2337 Dice: 0.6690 LR: 0.000500
  Batch [200/499] Loss: 0.1885 Dice: 0.7402 LR: 0.000500
  Batch [250/499] Loss: 0.2385 Dice: 0.6677 LR: 0.000500
  Batch [300/499] Loss: 0.1930 Dice: 0.7324 LR: 0.000500
  Batch [350/499] Loss: 0.2713 Dice: 0.6182 LR: 0.000500
  Batch [400/499] Loss: 0.2399 Dice: 0.6672 LR: 0.000500
  Batch [450/499] Loss: 0.2194 Dice: 0.6967 LR: 0.000500
Epoch 185 Train: 	[total] 0.2557	[shape] 0.2551	[dice] 0.6422	[l2] 6.7172
  Training time: 87.62s
  Total epoch time: 87.62s (no validation)

Epoch 186/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1899 Dice: 0.7338 LR: 0.000500
  Batch [50/499] Loss: 0.2366 Dice: 0.6728 LR: 0.000500
  Batch [100/499] Loss: 0.2753 Dice: 0.6121 LR: 0.000500
  Batch [150/499] Loss: 0.2582 Dice: 0.6371 LR: 0.000500
  Batch [200/499] Loss: 0.3064 Dice: 0.5684 LR: 0.000500
  Batch [250/499] Loss: 0.2330 Dice: 0.6749 LR: 0.000500
  Batch [300/499] Loss: 0.3217 Dice: 0.5469 LR: 0.000500
  Batch [350/499] Loss: 0.2229 Dice: 0.6920 LR: 0.000500
  Batch [400/499] Loss: 0.2892 Dice: 0.5904 LR: 0.000500
  Batch [450/499] Loss: 0.3237 Dice: 0.5447 LR: 0.000500
Epoch 186 Train: 	[total] 0.2548	[shape] 0.2541	[dice] 0.6433	[l2] 6.7180
  Training time: 87.47s
  Total epoch time: 87.47s (no validation)

Epoch 187/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1734 Dice: 0.7560 LR: 0.000500
  Batch [50/499] Loss: 0.1917 Dice: 0.7328 LR: 0.000500
  Batch [100/499] Loss: 0.2096 Dice: 0.7054 LR: 0.000500
  Batch [150/499] Loss: 0.2880 Dice: 0.5997 LR: 0.000500
  Batch [200/499] Loss: 0.2399 Dice: 0.6685 LR: 0.000500
  Batch [250/499] Loss: 0.1851 Dice: 0.7385 LR: 0.000500
  Batch [300/499] Loss: 0.1950 Dice: 0.7240 LR: 0.000500
  Batch [350/499] Loss: 0.2298 Dice: 0.6791 LR: 0.000500
  Batch [400/499] Loss: 0.2912 Dice: 0.5855 LR: 0.000500
  Batch [450/499] Loss: 0.2714 Dice: 0.6346 LR: 0.000500
Epoch 187 Train: 	[total] 0.2546	[shape] 0.2539	[dice] 0.6439	[l2] 6.7189
  Training time: 87.78s
  Total epoch time: 87.78s (no validation)

Epoch 188/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2329 Dice: 0.6768 LR: 0.000500
  Batch [50/499] Loss: 0.2976 Dice: 0.5851 LR: 0.000500
  Batch [100/499] Loss: 0.3154 Dice: 0.5565 LR: 0.000500
  Batch [150/499] Loss: 0.2378 Dice: 0.6718 LR: 0.000500
  Batch [200/499] Loss: 0.2266 Dice: 0.6850 LR: 0.000500
  Batch [250/499] Loss: 0.1618 Dice: 0.7785 LR: 0.000500
  Batch [300/499] Loss: 0.1534 Dice: 0.7883 LR: 0.000500
  Batch [350/499] Loss: 0.2240 Dice: 0.6828 LR: 0.000500
  Batch [400/499] Loss: 0.1755 Dice: 0.7555 LR: 0.000500
  Batch [450/499] Loss: 0.1955 Dice: 0.7337 LR: 0.000500
Epoch 188 Train: 	[total] 0.2542	[shape] 0.2536	[dice] 0.6443	[l2] 6.7198
  Training time: 87.82s
  Total epoch time: 87.82s (no validation)

Epoch 189/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1834 Dice: 0.7446 LR: 0.000500
  Batch [50/499] Loss: 0.2552 Dice: 0.6444 LR: 0.000500
  Batch [100/499] Loss: 0.2102 Dice: 0.7093 LR: 0.000500
  Batch [150/499] Loss: 0.1965 Dice: 0.7239 LR: 0.000500
  Batch [200/499] Loss: 0.2198 Dice: 0.6930 LR: 0.000500
  Batch [250/499] Loss: 0.2513 Dice: 0.6484 LR: 0.000500
  Batch [300/499] Loss: 0.2144 Dice: 0.7033 LR: 0.000500
  Batch [350/499] Loss: 0.2768 Dice: 0.6109 LR: 0.000500
  Batch [400/499] Loss: 0.3317 Dice: 0.5342 LR: 0.000500
  Batch [450/499] Loss: 0.2133 Dice: 0.6968 LR: 0.000500
Epoch 189 Train: 	[total] 0.2537	[shape] 0.2531	[dice] 0.6451	[l2] 6.7205
  Training time: 87.98s
  Total epoch time: 87.98s (no validation)

Epoch 190/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2440 Dice: 0.6571 LR: 0.000500
  Batch [50/499] Loss: 0.3669 Dice: 0.4805 LR: 0.000500
  Batch [100/499] Loss: 0.3248 Dice: 0.5405 LR: 0.000500
  Batch [150/499] Loss: 0.2837 Dice: 0.6055 LR: 0.000500
  Batch [200/499] Loss: 0.2056 Dice: 0.7122 LR: 0.000500
  Batch [250/499] Loss: 0.2176 Dice: 0.6955 LR: 0.000500
  Batch [300/499] Loss: 0.1520 Dice: 0.7881 LR: 0.000500
  Batch [350/499] Loss: 0.2744 Dice: 0.6140 LR: 0.000500
  Batch [400/499] Loss: 0.2145 Dice: 0.7033 LR: 0.000500
  Batch [450/499] Loss: 0.3010 Dice: 0.5790 LR: 0.000500
Epoch 190 Train: 	[total] 0.2538	[shape] 0.2531	[dice] 0.6450	[l2] 6.7214
  Training time: 87.76s
Epoch 190 Eval:  	[total] 0.6375	[shape] 0.6369	[dice] 0.0965	[l2] 6.7222
  Validation time: 202.64s
  Total epoch time: 290.40s

Epoch 191/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2494 Dice: 0.6542 LR: 0.000500
  Batch [50/499] Loss: 0.2161 Dice: 0.6937 LR: 0.000500
  Batch [100/499] Loss: 0.2443 Dice: 0.6603 LR: 0.000500
  Batch [150/499] Loss: 0.2059 Dice: 0.7139 LR: 0.000500
  Batch [200/499] Loss: 0.2155 Dice: 0.6974 LR: 0.000500
  Batch [250/499] Loss: 0.2240 Dice: 0.6870 LR: 0.000500
  Batch [300/499] Loss: 0.2164 Dice: 0.6976 LR: 0.000500
  Batch [350/499] Loss: 0.2249 Dice: 0.6855 LR: 0.000500
  Batch [400/499] Loss: 0.2719 Dice: 0.6219 LR: 0.000500
  Batch [450/499] Loss: 0.2536 Dice: 0.6470 LR: 0.000500
Epoch 191 Train: 	[total] 0.2535	[shape] 0.2528	[dice] 0.6453	[l2] 6.7222
  Training time: 88.05s
  Total epoch time: 88.05s (no validation)

Epoch 192/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2668 Dice: 0.6302 LR: 0.000500
  Batch [50/499] Loss: 0.2095 Dice: 0.7109 LR: 0.000500
  Batch [100/499] Loss: 0.2196 Dice: 0.6947 LR: 0.000500
  Batch [150/499] Loss: 0.2263 Dice: 0.6844 LR: 0.000500
  Batch [200/499] Loss: 0.2373 Dice: 0.6661 LR: 0.000500
  Batch [250/499] Loss: 0.2933 Dice: 0.5881 LR: 0.000500
  Batch [300/499] Loss: 0.4043 Dice: 0.4319 LR: 0.000500
  Batch [350/499] Loss: 0.1522 Dice: 0.7899 LR: 0.000500
  Batch [400/499] Loss: 0.1976 Dice: 0.7228 LR: 0.000500
  Batch [450/499] Loss: 0.2581 Dice: 0.6403 LR: 0.000500
Epoch 192 Train: 	[total] 0.2538	[shape] 0.2531	[dice] 0.6450	[l2] 6.7229
  Training time: 87.96s
  Total epoch time: 87.96s (no validation)

Epoch 193/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2208 Dice: 0.6903 LR: 0.000500
  Batch [50/499] Loss: 0.2368 Dice: 0.6725 LR: 0.000500
  Batch [100/499] Loss: 0.3731 Dice: 0.4729 LR: 0.000500
  Batch [150/499] Loss: 0.2319 Dice: 0.6769 LR: 0.000500
  Batch [200/499] Loss: 0.2495 Dice: 0.6520 LR: 0.000500
  Batch [250/499] Loss: 0.2332 Dice: 0.6778 LR: 0.000500
  Batch [300/499] Loss: 0.2234 Dice: 0.6868 LR: 0.000500
  Batch [350/499] Loss: 0.2161 Dice: 0.7005 LR: 0.000500
  Batch [400/499] Loss: 0.2322 Dice: 0.6747 LR: 0.000500
  Batch [450/499] Loss: 0.3003 Dice: 0.5790 LR: 0.000500
Epoch 193 Train: 	[total] 0.2548	[shape] 0.2541	[dice] 0.6436	[l2] 6.7238
  Training time: 87.67s
  Total epoch time: 87.67s (no validation)

Epoch 194/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2399 Dice: 0.6680 LR: 0.000500
  Batch [50/499] Loss: 0.3054 Dice: 0.5692 LR: 0.000500
  Batch [100/499] Loss: 0.3323 Dice: 0.5304 LR: 0.000500
  Batch [150/499] Loss: 0.1852 Dice: 0.7400 LR: 0.000500
  Batch [200/499] Loss: 0.3085 Dice: 0.5675 LR: 0.000500
  Batch [250/499] Loss: 0.2683 Dice: 0.6242 LR: 0.000500
  Batch [300/499] Loss: 0.2049 Dice: 0.7146 LR: 0.000500
  Batch [350/499] Loss: 0.2597 Dice: 0.6371 LR: 0.000500
  Batch [400/499] Loss: 0.2209 Dice: 0.6943 LR: 0.000500
  Batch [450/499] Loss: 0.3276 Dice: 0.5376 LR: 0.000500
Epoch 194 Train: 	[total] 0.2538	[shape] 0.2532	[dice] 0.6450	[l2] 6.7246
  Training time: 87.59s
  Total epoch time: 87.59s (no validation)

Epoch 195/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3523 Dice: 0.5050 LR: 0.000500
  Batch [50/499] Loss: 0.4091 Dice: 0.4240 LR: 0.000500
  Batch [100/499] Loss: 0.2588 Dice: 0.6371 LR: 0.000500
  Batch [150/499] Loss: 0.2381 Dice: 0.6702 LR: 0.000500
  Batch [200/499] Loss: 0.2695 Dice: 0.6211 LR: 0.000500
  Batch [250/499] Loss: 0.2307 Dice: 0.6770 LR: 0.000500
  Batch [300/499] Loss: 0.1674 Dice: 0.7708 LR: 0.000500
  Batch [350/499] Loss: 0.1927 Dice: 0.7291 LR: 0.000500
  Batch [400/499] Loss: 0.1894 Dice: 0.7348 LR: 0.000500
  Batch [450/499] Loss: 0.2897 Dice: 0.5914 LR: 0.000500
Epoch 195 Train: 	[total] 0.2545	[shape] 0.2538	[dice] 0.6439	[l2] 6.7254
  Training time: 87.65s
  Total epoch time: 87.65s (no validation)

Epoch 196/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.1840 Dice: 0.7411 LR: 0.000500
  Batch [50/499] Loss: 0.2872 Dice: 0.5983 LR: 0.000500
  Batch [100/499] Loss: 0.2290 Dice: 0.6772 LR: 0.000500
  Batch [150/499] Loss: 0.2527 Dice: 0.6475 LR: 0.000500
  Batch [200/499] Loss: 0.2228 Dice: 0.6959 LR: 0.000500
  Batch [250/499] Loss: 0.2637 Dice: 0.6295 LR: 0.000500
  Batch [300/499] Loss: 0.2033 Dice: 0.7212 LR: 0.000500
  Batch [350/499] Loss: 0.3123 Dice: 0.5668 LR: 0.000500
  Batch [400/499] Loss: 0.2174 Dice: 0.6971 LR: 0.000500
  Batch [450/499] Loss: 0.2067 Dice: 0.7142 LR: 0.000500
Epoch 196 Train: 	[total] 0.2542	[shape] 0.2535	[dice] 0.6442	[l2] 6.7262
  Training time: 87.78s
  Total epoch time: 87.78s (no validation)

Epoch 197/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2490 Dice: 0.6449 LR: 0.000500
  Batch [50/499] Loss: 0.2915 Dice: 0.5928 LR: 0.000500
  Batch [100/499] Loss: 0.2005 Dice: 0.7204 LR: 0.000500
  Batch [150/499] Loss: 0.2705 Dice: 0.6209 LR: 0.000500
  Batch [200/499] Loss: 0.2575 Dice: 0.6385 LR: 0.000500
  Batch [250/499] Loss: 0.2292 Dice: 0.6804 LR: 0.000500
  Batch [300/499] Loss: 0.2850 Dice: 0.5998 LR: 0.000500
  Batch [350/499] Loss: 0.2029 Dice: 0.7244 LR: 0.000500
  Batch [400/499] Loss: 0.2329 Dice: 0.6733 LR: 0.000500
  Batch [450/499] Loss: 0.2539 Dice: 0.6430 LR: 0.000500
Epoch 197 Train: 	[total] 0.2535	[shape] 0.2528	[dice] 0.6452	[l2] 6.7271
  Training time: 87.91s
  Total epoch time: 87.91s (no validation)

Epoch 198/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2227 Dice: 0.6903 LR: 0.000500
  Batch [50/499] Loss: 0.2776 Dice: 0.6104 LR: 0.000500
  Batch [100/499] Loss: 0.1426 Dice: 0.8026 LR: 0.000500
  Batch [150/499] Loss: 0.2492 Dice: 0.6469 LR: 0.000500
  Batch [200/499] Loss: 0.2925 Dice: 0.5894 LR: 0.000500
  Batch [250/499] Loss: 0.2422 Dice: 0.6643 LR: 0.000500
  Batch [300/499] Loss: 0.3040 Dice: 0.5724 LR: 0.000500
  Batch [350/499] Loss: 0.2885 Dice: 0.5975 LR: 0.000500
  Batch [400/499] Loss: 0.2019 Dice: 0.7202 LR: 0.000500
  Batch [450/499] Loss: 0.2999 Dice: 0.5780 LR: 0.000500
Epoch 198 Train: 	[total] 0.2531	[shape] 0.2525	[dice] 0.6458	[l2] 6.7280
  Training time: 88.08s
  Total epoch time: 88.08s (no validation)

Epoch 199/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.3136 Dice: 0.5589 LR: 0.000500
  Batch [50/499] Loss: 0.1586 Dice: 0.7818 LR: 0.000500
  Batch [100/499] Loss: 0.3128 Dice: 0.5641 LR: 0.000500
  Batch [150/499] Loss: 0.4079 Dice: 0.4210 LR: 0.000500
  Batch [200/499] Loss: 0.1937 Dice: 0.7340 LR: 0.000500
  Batch [250/499] Loss: 0.2203 Dice: 0.6965 LR: 0.000500
  Batch [300/499] Loss: 0.2073 Dice: 0.7135 LR: 0.000500
  Batch [350/499] Loss: 0.3619 Dice: 0.4908 LR: 0.000500
  Batch [400/499] Loss: 0.2208 Dice: 0.6918 LR: 0.000500
  Batch [450/499] Loss: 0.2479 Dice: 0.6532 LR: 0.000500
Epoch 199 Train: 	[total] 0.2541	[shape] 0.2534	[dice] 0.6444	[l2] 6.7288
  Training time: 87.72s
  Total epoch time: 87.72s (no validation)

Epoch 200/300 (boundary_bias=30%)
  Batch [0/499] Loss: 0.2798 Dice: 0.6037 LR: 0.000500
  Batch [50/499] Loss: 0.2253 Dice: 0.6847 LR: 0.000500
  Batch [100/499] Loss: 0.1655 Dice: 0.7659 LR: 0.000500
  Batch [150/499] Loss: 0.3258 Dice: 0.5424 LR: 0.000500
  Batch [200/499] Loss: 0.2433 Dice: 0.6617 LR: 0.000500
  Batch [250/499] Loss: 0.2735 Dice: 0.6197 LR: 0.000500
  Batch [300/499] Loss: 0.2718 Dice: 0.6208 LR: 0.000500
  Batch [350/499] Loss: 0.1979 Dice: 0.7218 LR: 0.000500
  Batch [400/499] Loss: 0.2225 Dice: 0.6899 LR: 0.000500
  Batch [450/499] Loss: 0.2245 Dice: 0.6850 LR: 0.000500
Epoch 200 Train: 	[total] 0.2530	[shape] 0.2523	[dice] 0.6460	[l2] 6.7296
  Training time: 87.72s
Epoch 200 Eval:  	[total] 0.6353	[shape] 0.6346	[dice] 0.0998	[l2] 6.7304
  Validation time: 202.48s
======================================================================
Found a new best model! Shape Loss: 0.634630
======================================================================
  Total epoch time: 290.20s

Epoch 201/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.4434 Dice: 0.3725 LR: 0.000250
  Batch [50/499] Loss: 0.5181 Dice: 0.2651 LR: 0.000250
  Batch [100/499] Loss: 0.3546 Dice: 0.4997 LR: 0.000250
  Batch [150/499] Loss: 0.3629 Dice: 0.4880 LR: 0.000250
  Batch [200/499] Loss: 0.4030 Dice: 0.4325 LR: 0.000250
  Batch [250/499] Loss: 0.4565 Dice: 0.3535 LR: 0.000250
  Batch [300/499] Loss: 0.3590 Dice: 0.4934 LR: 0.000250
  Batch [350/499] Loss: 0.4037 Dice: 0.4320 LR: 0.000250
  Batch [400/499] Loss: 0.3140 Dice: 0.5572 LR: 0.000250
  Batch [450/499] Loss: 0.3971 Dice: 0.4375 LR: 0.000250
Epoch 201 Train: 	[total] 0.3955	[shape] 0.3948	[dice] 0.4413	[l2] 6.7304
  Training time: 87.75s
  Total epoch time: 87.75s (no validation)

Epoch 202/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3523 Dice: 0.5081 LR: 0.000250
  Batch [50/499] Loss: 0.3173 Dice: 0.5512 LR: 0.000250
  Batch [100/499] Loss: 0.3405 Dice: 0.5150 LR: 0.000250
  Batch [150/499] Loss: 0.4473 Dice: 0.3671 LR: 0.000250
  Batch [200/499] Loss: 0.4012 Dice: 0.4320 LR: 0.000250
  Batch [250/499] Loss: 0.3034 Dice: 0.5730 LR: 0.000250
  Batch [300/499] Loss: 0.3760 Dice: 0.4699 LR: 0.000250
  Batch [350/499] Loss: 0.4299 Dice: 0.3954 LR: 0.000250
  Batch [400/499] Loss: 0.3824 Dice: 0.4578 LR: 0.000250
  Batch [450/499] Loss: 0.4196 Dice: 0.4099 LR: 0.000250
Epoch 202 Train: 	[total] 0.3698	[shape] 0.3692	[dice] 0.4770	[l2] 6.7306
  Training time: 87.45s
  Total epoch time: 87.45s (no validation)

Epoch 203/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2930 Dice: 0.5881 LR: 0.000250
  Batch [50/499] Loss: 0.4340 Dice: 0.3885 LR: 0.000250
  Batch [100/499] Loss: 0.3660 Dice: 0.4869 LR: 0.000250
  Batch [150/499] Loss: 0.3711 Dice: 0.4729 LR: 0.000250
  Batch [200/499] Loss: 0.2929 Dice: 0.5888 LR: 0.000250
  Batch [250/499] Loss: 0.3159 Dice: 0.5504 LR: 0.000250
  Batch [300/499] Loss: 0.4282 Dice: 0.3929 LR: 0.000250
  Batch [350/499] Loss: 0.3619 Dice: 0.4905 LR: 0.000250
  Batch [400/499] Loss: 0.4352 Dice: 0.3861 LR: 0.000250
  Batch [450/499] Loss: 0.3534 Dice: 0.5027 LR: 0.000250
Epoch 203 Train: 	[total] 0.3614	[shape] 0.3607	[dice] 0.4890	[l2] 6.7307
  Training time: 87.76s
  Total epoch time: 87.76s (no validation)

Epoch 204/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3480 Dice: 0.5095 LR: 0.000250
  Batch [50/499] Loss: 0.4154 Dice: 0.4126 LR: 0.000250
  Batch [100/499] Loss: 0.4483 Dice: 0.3627 LR: 0.000250
  Batch [150/499] Loss: 0.4696 Dice: 0.3329 LR: 0.000250
  Batch [200/499] Loss: 0.3932 Dice: 0.4436 LR: 0.000250
  Batch [250/499] Loss: 0.3453 Dice: 0.5091 LR: 0.000250
  Batch [300/499] Loss: 0.3559 Dice: 0.4954 LR: 0.000250
  Batch [350/499] Loss: 0.2574 Dice: 0.6383 LR: 0.000250
  Batch [400/499] Loss: 0.4345 Dice: 0.3852 LR: 0.000250
  Batch [450/499] Loss: 0.2955 Dice: 0.5854 LR: 0.000250
Epoch 204 Train: 	[total] 0.3570	[shape] 0.3563	[dice] 0.4952	[l2] 6.7308
  Training time: 87.54s
  Total epoch time: 87.54s (no validation)

Epoch 205/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3428 Dice: 0.5183 LR: 0.000250
  Batch [50/499] Loss: 0.3296 Dice: 0.5324 LR: 0.000250
  Batch [100/499] Loss: 0.2809 Dice: 0.6024 LR: 0.000250
  Batch [150/499] Loss: 0.3387 Dice: 0.5244 LR: 0.000250
  Batch [200/499] Loss: 0.3228 Dice: 0.5469 LR: 0.000250
  Batch [250/499] Loss: 0.3143 Dice: 0.5575 LR: 0.000250
  Batch [300/499] Loss: 0.3670 Dice: 0.4825 LR: 0.000250
  Batch [350/499] Loss: 0.3306 Dice: 0.5356 LR: 0.000250
  Batch [400/499] Loss: 0.3580 Dice: 0.4928 LR: 0.000250
  Batch [450/499] Loss: 0.3466 Dice: 0.5110 LR: 0.000250
Epoch 205 Train: 	[total] 0.3541	[shape] 0.3535	[dice] 0.4993	[l2] 6.7308
  Training time: 87.58s
  Total epoch time: 87.58s (no validation)

Epoch 206/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3589 Dice: 0.4911 LR: 0.000250
  Batch [50/499] Loss: 0.3451 Dice: 0.5129 LR: 0.000250
  Batch [100/499] Loss: 0.2788 Dice: 0.6033 LR: 0.000250
  Batch [150/499] Loss: 0.3147 Dice: 0.5576 LR: 0.000250
  Batch [200/499] Loss: 0.2941 Dice: 0.5850 LR: 0.000250
  Batch [250/499] Loss: 0.3821 Dice: 0.4596 LR: 0.000250
  Batch [300/499] Loss: 0.4053 Dice: 0.4286 LR: 0.000250
  Batch [350/499] Loss: 0.3925 Dice: 0.4428 LR: 0.000250
  Batch [400/499] Loss: 0.3974 Dice: 0.4352 LR: 0.000250
  Batch [450/499] Loss: 0.3099 Dice: 0.5600 LR: 0.000250
Epoch 206 Train: 	[total] 0.3521	[shape] 0.3514	[dice] 0.5021	[l2] 6.7309
  Training time: 87.75s
  Total epoch time: 87.75s (no validation)

Epoch 207/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3986 Dice: 0.4358 LR: 0.000250
  Batch [50/499] Loss: 0.3108 Dice: 0.5614 LR: 0.000250
  Batch [100/499] Loss: 0.3243 Dice: 0.5401 LR: 0.000250
  Batch [150/499] Loss: 0.2902 Dice: 0.5886 LR: 0.000250
  Batch [200/499] Loss: 0.2987 Dice: 0.5820 LR: 0.000250
  Batch [250/499] Loss: 0.3377 Dice: 0.5222 LR: 0.000250
  Batch [300/499] Loss: 0.3770 Dice: 0.4648 LR: 0.000250
  Batch [350/499] Loss: 0.3406 Dice: 0.5173 LR: 0.000250
  Batch [400/499] Loss: 0.4064 Dice: 0.4223 LR: 0.000250
  Batch [450/499] Loss: 0.3223 Dice: 0.5442 LR: 0.000250
Epoch 207 Train: 	[total] 0.3500	[shape] 0.3494	[dice] 0.5051	[l2] 6.7311
  Training time: 87.39s
  Total epoch time: 87.39s (no validation)

Epoch 208/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3652 Dice: 0.4820 LR: 0.000250
  Batch [50/499] Loss: 0.2815 Dice: 0.6018 LR: 0.000250
  Batch [100/499] Loss: 0.4668 Dice: 0.3360 LR: 0.000250
  Batch [150/499] Loss: 0.3263 Dice: 0.5370 LR: 0.000250
  Batch [200/499] Loss: 0.2779 Dice: 0.6069 LR: 0.000250
  Batch [250/499] Loss: 0.3605 Dice: 0.4906 LR: 0.000250
  Batch [300/499] Loss: 0.3657 Dice: 0.4830 LR: 0.000250
  Batch [350/499] Loss: 0.3869 Dice: 0.4504 LR: 0.000250
  Batch [400/499] Loss: 0.4435 Dice: 0.3708 LR: 0.000250
  Batch [450/499] Loss: 0.3140 Dice: 0.5553 LR: 0.000250
Epoch 208 Train: 	[total] 0.3488	[shape] 0.3481	[dice] 0.5068	[l2] 6.7312
  Training time: 87.62s
  Total epoch time: 87.62s (no validation)

Epoch 209/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3176 Dice: 0.5487 LR: 0.000250
  Batch [50/499] Loss: 0.3027 Dice: 0.5705 LR: 0.000250
  Batch [100/499] Loss: 0.2993 Dice: 0.5770 LR: 0.000250
  Batch [150/499] Loss: 0.4663 Dice: 0.3380 LR: 0.000250
  Batch [200/499] Loss: 0.3785 Dice: 0.4664 LR: 0.000250
  Batch [250/499] Loss: 0.3614 Dice: 0.4871 LR: 0.000250
  Batch [300/499] Loss: 0.4031 Dice: 0.4268 LR: 0.000250
  Batch [350/499] Loss: 0.3039 Dice: 0.5730 LR: 0.000250
  Batch [400/499] Loss: 0.3402 Dice: 0.5192 LR: 0.000250
  Batch [450/499] Loss: 0.3322 Dice: 0.5336 LR: 0.000250
Epoch 209 Train: 	[total] 0.3484	[shape] 0.3477	[dice] 0.5074	[l2] 6.7313
  Training time: 87.69s
  Total epoch time: 87.69s (no validation)

Epoch 210/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3827 Dice: 0.4586 LR: 0.000250
  Batch [50/499] Loss: 0.3568 Dice: 0.4961 LR: 0.000250
  Batch [100/499] Loss: 0.3958 Dice: 0.4392 LR: 0.000250
  Batch [150/499] Loss: 0.4317 Dice: 0.3892 LR: 0.000250
  Batch [200/499] Loss: 0.2815 Dice: 0.6066 LR: 0.000250
  Batch [250/499] Loss: 0.3295 Dice: 0.5321 LR: 0.000250
  Batch [300/499] Loss: 0.2796 Dice: 0.6045 LR: 0.000250
  Batch [350/499] Loss: 0.3060 Dice: 0.5683 LR: 0.000250
  Batch [400/499] Loss: 0.3137 Dice: 0.5584 LR: 0.000250
  Batch [450/499] Loss: 0.2855 Dice: 0.5995 LR: 0.000250
Epoch 210 Train: 	[total] 0.3480	[shape] 0.3473	[dice] 0.5081	[l2] 6.7314
  Training time: 87.59s
Epoch 210 Eval:  	[total] 0.6103	[shape] 0.6097	[dice] 0.1350	[l2] 6.7315
  Validation time: 202.72s
======================================================================
Found a new best model! Shape Loss: 0.609666
======================================================================
  Total epoch time: 290.31s

Epoch 211/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3171 Dice: 0.5523 LR: 0.000250
  Batch [50/499] Loss: 0.2975 Dice: 0.5791 LR: 0.000250
  Batch [100/499] Loss: 0.3517 Dice: 0.5037 LR: 0.000250
  Batch [150/499] Loss: 0.4311 Dice: 0.3869 LR: 0.000250
  Batch [200/499] Loss: 0.3737 Dice: 0.4681 LR: 0.000250
  Batch [250/499] Loss: 0.3195 Dice: 0.5491 LR: 0.000250
  Batch [300/499] Loss: 0.3732 Dice: 0.4710 LR: 0.000250
  Batch [350/499] Loss: 0.3555 Dice: 0.4982 LR: 0.000250
  Batch [400/499] Loss: 0.2070 Dice: 0.7062 LR: 0.000250
  Batch [450/499] Loss: 0.3440 Dice: 0.5127 LR: 0.000250
Epoch 211 Train: 	[total] 0.3468	[shape] 0.3462	[dice] 0.5097	[l2] 6.7315
  Training time: 87.79s
  Total epoch time: 87.79s (no validation)

Epoch 212/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3432 Dice: 0.5127 LR: 0.000250
  Batch [50/499] Loss: 0.3396 Dice: 0.5206 LR: 0.000250
  Batch [100/499] Loss: 0.4108 Dice: 0.4156 LR: 0.000250
  Batch [150/499] Loss: 0.3063 Dice: 0.5637 LR: 0.000250
  Batch [200/499] Loss: 0.2855 Dice: 0.5985 LR: 0.000250
  Batch [250/499] Loss: 0.3795 Dice: 0.4625 LR: 0.000250
  Batch [300/499] Loss: 0.4131 Dice: 0.4128 LR: 0.000250
  Batch [350/499] Loss: 0.4695 Dice: 0.3345 LR: 0.000250
  Batch [400/499] Loss: 0.2719 Dice: 0.6163 LR: 0.000250
  Batch [450/499] Loss: 0.3764 Dice: 0.4710 LR: 0.000250
Epoch 212 Train: 	[total] 0.3466	[shape] 0.3460	[dice] 0.5099	[l2] 6.7316
  Training time: 87.50s
  Total epoch time: 87.50s (no validation)

Epoch 213/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3764 Dice: 0.4680 LR: 0.000250
  Batch [50/499] Loss: 0.3499 Dice: 0.5062 LR: 0.000250
  Batch [100/499] Loss: 0.2711 Dice: 0.6142 LR: 0.000250
  Batch [150/499] Loss: 0.2839 Dice: 0.5978 LR: 0.000250
  Batch [200/499] Loss: 0.3953 Dice: 0.4415 LR: 0.000250
  Batch [250/499] Loss: 0.3302 Dice: 0.5333 LR: 0.000250
  Batch [300/499] Loss: 0.3007 Dice: 0.5741 LR: 0.000250
  Batch [350/499] Loss: 0.2904 Dice: 0.5902 LR: 0.000250
  Batch [400/499] Loss: 0.3827 Dice: 0.4571 LR: 0.000250
  Batch [450/499] Loss: 0.4222 Dice: 0.4050 LR: 0.000250
Epoch 213 Train: 	[total] 0.3468	[shape] 0.3462	[dice] 0.5096	[l2] 6.7317
  Training time: 87.64s
  Total epoch time: 87.64s (no validation)

Epoch 214/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3856 Dice: 0.4552 LR: 0.000250
  Batch [50/499] Loss: 0.2721 Dice: 0.6156 LR: 0.000250
  Batch [100/499] Loss: 0.3831 Dice: 0.4572 LR: 0.000250
  Batch [150/499] Loss: 0.2937 Dice: 0.5830 LR: 0.000250
  Batch [200/499] Loss: 0.3657 Dice: 0.4815 LR: 0.000250
  Batch [250/499] Loss: 0.4236 Dice: 0.3995 LR: 0.000250
  Batch [300/499] Loss: 0.4570 Dice: 0.3525 LR: 0.000250
  Batch [350/499] Loss: 0.2862 Dice: 0.5961 LR: 0.000250
  Batch [400/499] Loss: 0.3589 Dice: 0.4901 LR: 0.000250
  Batch [450/499] Loss: 0.3570 Dice: 0.4932 LR: 0.000250
Epoch 214 Train: 	[total] 0.3463	[shape] 0.3457	[dice] 0.5103	[l2] 6.7318
  Training time: 87.72s
  Total epoch time: 87.72s (no validation)

Epoch 215/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3493 Dice: 0.5048 LR: 0.000250
  Batch [50/499] Loss: 0.2939 Dice: 0.5839 LR: 0.000250
  Batch [100/499] Loss: 0.3931 Dice: 0.4433 LR: 0.000250
  Batch [150/499] Loss: 0.3667 Dice: 0.4778 LR: 0.000250
  Batch [200/499] Loss: 0.3312 Dice: 0.5330 LR: 0.000250
  Batch [250/499] Loss: 0.3605 Dice: 0.4885 LR: 0.000250
  Batch [300/499] Loss: 0.3489 Dice: 0.5069 LR: 0.000250
  Batch [350/499] Loss: 0.3500 Dice: 0.5057 LR: 0.000250
  Batch [400/499] Loss: 0.3165 Dice: 0.5553 LR: 0.000250
  Batch [450/499] Loss: 0.3684 Dice: 0.4816 LR: 0.000250
Epoch 215 Train: 	[total] 0.3456	[shape] 0.3449	[dice] 0.5113	[l2] 6.7319
  Training time: 87.59s
  Total epoch time: 87.59s (no validation)

Epoch 216/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.4636 Dice: 0.3421 LR: 0.000250
  Batch [50/499] Loss: 0.2760 Dice: 0.6097 LR: 0.000250
  Batch [100/499] Loss: 0.2991 Dice: 0.5787 LR: 0.000250
  Batch [150/499] Loss: 0.4199 Dice: 0.4047 LR: 0.000250
  Batch [200/499] Loss: 0.3255 Dice: 0.5398 LR: 0.000250
  Batch [250/499] Loss: 0.3433 Dice: 0.5142 LR: 0.000250
  Batch [300/499] Loss: 0.3561 Dice: 0.4998 LR: 0.000250
  Batch [350/499] Loss: 0.3813 Dice: 0.4591 LR: 0.000250
  Batch [400/499] Loss: 0.4080 Dice: 0.4207 LR: 0.000250
  Batch [450/499] Loss: 0.3375 Dice: 0.5257 LR: 0.000250
Epoch 216 Train: 	[total] 0.3446	[shape] 0.3439	[dice] 0.5129	[l2] 6.7320
  Training time: 87.65s
  Total epoch time: 87.65s (no validation)

Epoch 217/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3352 Dice: 0.5248 LR: 0.000250
  Batch [50/499] Loss: 0.3745 Dice: 0.4690 LR: 0.000250
  Batch [100/499] Loss: 0.3320 Dice: 0.5304 LR: 0.000250
  Batch [150/499] Loss: 0.4008 Dice: 0.4312 LR: 0.000250
  Batch [200/499] Loss: 0.3337 Dice: 0.5272 LR: 0.000250
  Batch [250/499] Loss: 0.4495 Dice: 0.3588 LR: 0.000250
  Batch [300/499] Loss: 0.3562 Dice: 0.4967 LR: 0.000250
  Batch [350/499] Loss: 0.2877 Dice: 0.5928 LR: 0.000250
  Batch [400/499] Loss: 0.3161 Dice: 0.5552 LR: 0.000250
  Batch [450/499] Loss: 0.3521 Dice: 0.5006 LR: 0.000250
Epoch 217 Train: 	[total] 0.3435	[shape] 0.3428	[dice] 0.5143	[l2] 6.7322
  Training time: 87.70s
  Total epoch time: 87.70s (no validation)

Epoch 218/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.4039 Dice: 0.4302 LR: 0.000250
  Batch [50/499] Loss: 0.4099 Dice: 0.4196 LR: 0.000250
  Batch [100/499] Loss: 0.3032 Dice: 0.5686 LR: 0.000250
  Batch [150/499] Loss: 0.3302 Dice: 0.5348 LR: 0.000250
  Batch [200/499] Loss: 0.4084 Dice: 0.4198 LR: 0.000250
  Batch [250/499] Loss: 0.3386 Dice: 0.5204 LR: 0.000250
  Batch [300/499] Loss: 0.3708 Dice: 0.4757 LR: 0.000250
  Batch [350/499] Loss: 0.3747 Dice: 0.4697 LR: 0.000250
  Batch [400/499] Loss: 0.2806 Dice: 0.6072 LR: 0.000250
  Batch [450/499] Loss: 0.3611 Dice: 0.4901 LR: 0.000250
Epoch 218 Train: 	[total] 0.3439	[shape] 0.3432	[dice] 0.5138	[l2] 6.7323
  Training time: 87.52s
  Total epoch time: 87.52s (no validation)

Epoch 219/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3400 Dice: 0.5219 LR: 0.000250
  Batch [50/499] Loss: 0.3025 Dice: 0.5794 LR: 0.000250
  Batch [100/499] Loss: 0.3276 Dice: 0.5383 LR: 0.000250
  Batch [150/499] Loss: 0.3066 Dice: 0.5681 LR: 0.000250
  Batch [200/499] Loss: 0.3841 Dice: 0.4544 LR: 0.000250
  Batch [250/499] Loss: 0.2876 Dice: 0.5960 LR: 0.000250
  Batch [300/499] Loss: 0.3929 Dice: 0.4449 LR: 0.000250
  Batch [350/499] Loss: 0.3446 Dice: 0.5123 LR: 0.000250
  Batch [400/499] Loss: 0.4139 Dice: 0.4140 LR: 0.000250
  Batch [450/499] Loss: 0.3367 Dice: 0.5207 LR: 0.000250
Epoch 219 Train: 	[total] 0.3440	[shape] 0.3433	[dice] 0.5137	[l2] 6.7324
  Training time: 87.47s
  Total epoch time: 87.47s (no validation)

Epoch 220/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2628 Dice: 0.6324 LR: 0.000250
  Batch [50/499] Loss: 0.3017 Dice: 0.5736 LR: 0.000250
  Batch [100/499] Loss: 0.3037 Dice: 0.5704 LR: 0.000250
  Batch [150/499] Loss: 0.3021 Dice: 0.5752 LR: 0.000250
  Batch [200/499] Loss: 0.4304 Dice: 0.3912 LR: 0.000250
  Batch [250/499] Loss: 0.4458 Dice: 0.3685 LR: 0.000250
  Batch [300/499] Loss: 0.3218 Dice: 0.5442 LR: 0.000250
  Batch [350/499] Loss: 0.3406 Dice: 0.5180 LR: 0.000250
  Batch [400/499] Loss: 0.4016 Dice: 0.4313 LR: 0.000250
  Batch [450/499] Loss: 0.3112 Dice: 0.5596 LR: 0.000250
Epoch 220 Train: 	[total] 0.3430	[shape] 0.3424	[dice] 0.5152	[l2] 6.7326
  Training time: 87.64s
Epoch 220 Eval:  	[total] 0.6082	[shape] 0.6075	[dice] 0.1381	[l2] 6.7327
  Validation time: 202.94s
======================================================================
Found a new best model! Shape Loss: 0.607514
======================================================================
  Total epoch time: 290.58s

Epoch 221/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3757 Dice: 0.4682 LR: 0.000250
  Batch [50/499] Loss: 0.2829 Dice: 0.6022 LR: 0.000250
  Batch [100/499] Loss: 0.4627 Dice: 0.3397 LR: 0.000250
  Batch [150/499] Loss: 0.3077 Dice: 0.5656 LR: 0.000250
  Batch [200/499] Loss: 0.3739 Dice: 0.4716 LR: 0.000250
  Batch [250/499] Loss: 0.3925 Dice: 0.4454 LR: 0.000250
  Batch [300/499] Loss: 0.4302 Dice: 0.3936 LR: 0.000250
  Batch [350/499] Loss: 0.3736 Dice: 0.4690 LR: 0.000250
  Batch [400/499] Loss: 0.3079 Dice: 0.5670 LR: 0.000250
  Batch [450/499] Loss: 0.4149 Dice: 0.4109 LR: 0.000250
Epoch 221 Train: 	[total] 0.3430	[shape] 0.3423	[dice] 0.5150	[l2] 6.7327
  Training time: 87.79s
  Total epoch time: 87.79s (no validation)

Epoch 222/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3647 Dice: 0.4863 LR: 0.000250
  Batch [50/499] Loss: 0.3728 Dice: 0.4724 LR: 0.000250
  Batch [100/499] Loss: 0.3894 Dice: 0.4469 LR: 0.000250
  Batch [150/499] Loss: 0.3066 Dice: 0.5654 LR: 0.000250
  Batch [200/499] Loss: 0.2464 Dice: 0.6528 LR: 0.000250
  Batch [250/499] Loss: 0.3495 Dice: 0.5050 LR: 0.000250
  Batch [300/499] Loss: 0.2829 Dice: 0.6028 LR: 0.000250
  Batch [350/499] Loss: 0.3862 Dice: 0.4502 LR: 0.000250
  Batch [400/499] Loss: 0.3641 Dice: 0.4867 LR: 0.000250
  Batch [450/499] Loss: 0.3651 Dice: 0.4821 LR: 0.000250
Epoch 222 Train: 	[total] 0.3420	[shape] 0.3414	[dice] 0.5165	[l2] 6.7328
  Training time: 87.69s
  Total epoch time: 87.69s (no validation)

Epoch 223/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2906 Dice: 0.5891 LR: 0.000250
  Batch [50/499] Loss: 0.3637 Dice: 0.4868 LR: 0.000250
  Batch [100/499] Loss: 0.3143 Dice: 0.5561 LR: 0.000250
  Batch [150/499] Loss: 0.4099 Dice: 0.4179 LR: 0.000250
  Batch [200/499] Loss: 0.4514 Dice: 0.3610 LR: 0.000250
  Batch [250/499] Loss: 0.3158 Dice: 0.5540 LR: 0.000250
  Batch [300/499] Loss: 0.3733 Dice: 0.4717 LR: 0.000250
  Batch [350/499] Loss: 0.3296 Dice: 0.5326 LR: 0.000250
  Batch [400/499] Loss: 0.3244 Dice: 0.5452 LR: 0.000250
  Batch [450/499] Loss: 0.4098 Dice: 0.4163 LR: 0.000250
Epoch 223 Train: 	[total] 0.3426	[shape] 0.3420	[dice] 0.5157	[l2] 6.7330
  Training time: 87.41s
  Total epoch time: 87.41s (no validation)

Epoch 224/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3441 Dice: 0.5128 LR: 0.000250
  Batch [50/499] Loss: 0.3830 Dice: 0.4599 LR: 0.000250
  Batch [100/499] Loss: 0.4305 Dice: 0.3945 LR: 0.000250
  Batch [150/499] Loss: 0.3825 Dice: 0.4621 LR: 0.000250
  Batch [200/499] Loss: 0.3888 Dice: 0.4524 LR: 0.000250
  Batch [250/499] Loss: 0.3988 Dice: 0.4339 LR: 0.000250
  Batch [300/499] Loss: 0.3329 Dice: 0.5288 LR: 0.000250
  Batch [350/499] Loss: 0.2673 Dice: 0.6223 LR: 0.000250
  Batch [400/499] Loss: 0.3517 Dice: 0.5024 LR: 0.000250
  Batch [450/499] Loss: 0.3474 Dice: 0.5106 LR: 0.000250
Epoch 224 Train: 	[total] 0.3420	[shape] 0.3413	[dice] 0.5166	[l2] 6.7331
  Training time: 87.50s
  Total epoch time: 87.50s (no validation)

Epoch 225/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3461 Dice: 0.5112 LR: 0.000250
  Batch [50/499] Loss: 0.5096 Dice: 0.2784 LR: 0.000250
  Batch [100/499] Loss: 0.3588 Dice: 0.4936 LR: 0.000250
  Batch [150/499] Loss: 0.3431 Dice: 0.5167 LR: 0.000250
  Batch [200/499] Loss: 0.2827 Dice: 0.6014 LR: 0.000250
  Batch [250/499] Loss: 0.2664 Dice: 0.6233 LR: 0.000250
  Batch [300/499] Loss: 0.4098 Dice: 0.4188 LR: 0.000250
  Batch [350/499] Loss: 0.3802 Dice: 0.4619 LR: 0.000250
  Batch [400/499] Loss: 0.3426 Dice: 0.5177 LR: 0.000250
  Batch [450/499] Loss: 0.3161 Dice: 0.5536 LR: 0.000250
Epoch 225 Train: 	[total] 0.3414	[shape] 0.3408	[dice] 0.5175	[l2] 6.7333
  Training time: 87.89s
  Total epoch time: 87.89s (no validation)

Epoch 226/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3362 Dice: 0.5234 LR: 0.000250
  Batch [50/499] Loss: 0.2902 Dice: 0.5896 LR: 0.000250
  Batch [100/499] Loss: 0.3122 Dice: 0.5598 LR: 0.000250
  Batch [150/499] Loss: 0.3634 Dice: 0.4836 LR: 0.000250
  Batch [200/499] Loss: 0.3573 Dice: 0.5012 LR: 0.000250
  Batch [250/499] Loss: 0.2918 Dice: 0.5895 LR: 0.000250
  Batch [300/499] Loss: 0.2749 Dice: 0.6122 LR: 0.000250
  Batch [350/499] Loss: 0.2648 Dice: 0.6266 LR: 0.000250
  Batch [400/499] Loss: 0.3261 Dice: 0.5398 LR: 0.000250
  Batch [450/499] Loss: 0.2965 Dice: 0.5802 LR: 0.000250
Epoch 226 Train: 	[total] 0.3408	[shape] 0.3401	[dice] 0.5184	[l2] 6.7334
  Training time: 87.52s
  Total epoch time: 87.52s (no validation)

Epoch 227/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3147 Dice: 0.5559 LR: 0.000250
  Batch [50/499] Loss: 0.2723 Dice: 0.6136 LR: 0.000250
  Batch [100/499] Loss: 0.3568 Dice: 0.4934 LR: 0.000250
  Batch [150/499] Loss: 0.3487 Dice: 0.5078 LR: 0.000250
  Batch [200/499] Loss: 0.3423 Dice: 0.5148 LR: 0.000250
  Batch [250/499] Loss: 0.3409 Dice: 0.5159 LR: 0.000250
  Batch [300/499] Loss: 0.2961 Dice: 0.5810 LR: 0.000250
  Batch [350/499] Loss: 0.3386 Dice: 0.5223 LR: 0.000250
  Batch [400/499] Loss: 0.3506 Dice: 0.5064 LR: 0.000250
  Batch [450/499] Loss: 0.3508 Dice: 0.5045 LR: 0.000250
Epoch 227 Train: 	[total] 0.3409	[shape] 0.3403	[dice] 0.5181	[l2] 6.7335
  Training time: 87.47s
  Total epoch time: 87.47s (no validation)

Epoch 228/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3096 Dice: 0.5595 LR: 0.000250
  Batch [50/499] Loss: 0.3051 Dice: 0.5687 LR: 0.000250
  Batch [100/499] Loss: 0.2895 Dice: 0.5903 LR: 0.000250
  Batch [150/499] Loss: 0.3391 Dice: 0.5228 LR: 0.000250
  Batch [200/499] Loss: 0.3777 Dice: 0.4673 LR: 0.000250
  Batch [250/499] Loss: 0.4261 Dice: 0.3979 LR: 0.000250
  Batch [300/499] Loss: 0.3402 Dice: 0.5178 LR: 0.000250
  Batch [350/499] Loss: 0.3168 Dice: 0.5514 LR: 0.000250
  Batch [400/499] Loss: 0.4033 Dice: 0.4297 LR: 0.000250
  Batch [450/499] Loss: 0.4151 Dice: 0.4120 LR: 0.000250
Epoch 228 Train: 	[total] 0.3404	[shape] 0.3397	[dice] 0.5189	[l2] 6.7336
  Training time: 87.54s
  Total epoch time: 87.54s (no validation)

Epoch 229/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2969 Dice: 0.5821 LR: 0.000250
  Batch [50/499] Loss: 0.3900 Dice: 0.4483 LR: 0.000250
  Batch [100/499] Loss: 0.3560 Dice: 0.4989 LR: 0.000250
  Batch [150/499] Loss: 0.3860 Dice: 0.4524 LR: 0.000250
  Batch [200/499] Loss: 0.3321 Dice: 0.5302 LR: 0.000250
  Batch [250/499] Loss: 0.3729 Dice: 0.4709 LR: 0.000250
  Batch [300/499] Loss: 0.3369 Dice: 0.5221 LR: 0.000250
  Batch [350/499] Loss: 0.3499 Dice: 0.5063 LR: 0.000250
  Batch [400/499] Loss: 0.3032 Dice: 0.5742 LR: 0.000250
  Batch [450/499] Loss: 0.3198 Dice: 0.5522 LR: 0.000250
Epoch 229 Train: 	[total] 0.3411	[shape] 0.3405	[dice] 0.5178	[l2] 6.7338
  Training time: 87.68s
  Total epoch time: 87.68s (no validation)

Epoch 230/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2874 Dice: 0.5977 LR: 0.000250
  Batch [50/499] Loss: 0.3767 Dice: 0.4658 LR: 0.000250
  Batch [100/499] Loss: 0.2735 Dice: 0.6159 LR: 0.000250
  Batch [150/499] Loss: 0.3280 Dice: 0.5360 LR: 0.000250
  Batch [200/499] Loss: 0.3836 Dice: 0.4577 LR: 0.000250
  Batch [250/499] Loss: 0.3490 Dice: 0.5066 LR: 0.000250
  Batch [300/499] Loss: 0.4230 Dice: 0.4025 LR: 0.000250
  Batch [350/499] Loss: 0.2946 Dice: 0.5838 LR: 0.000250
  Batch [400/499] Loss: 0.3053 Dice: 0.5681 LR: 0.000250
  Batch [450/499] Loss: 0.3937 Dice: 0.4441 LR: 0.000250
Epoch 230 Train: 	[total] 0.3402	[shape] 0.3395	[dice] 0.5192	[l2] 6.7339
  Training time: 87.76s
Epoch 230 Eval:  	[total] 0.6033	[shape] 0.6026	[dice] 0.1453	[l2] 6.7341
  Validation time: 202.67s
======================================================================
Found a new best model! Shape Loss: 0.602643
======================================================================
  Total epoch time: 290.42s

Epoch 231/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3122 Dice: 0.5606 LR: 0.000250
  Batch [50/499] Loss: 0.3959 Dice: 0.4406 LR: 0.000250
  Batch [100/499] Loss: 0.4105 Dice: 0.4160 LR: 0.000250
  Batch [150/499] Loss: 0.3764 Dice: 0.4662 LR: 0.000250
  Batch [200/499] Loss: 0.3108 Dice: 0.5623 LR: 0.000250
  Batch [250/499] Loss: 0.4170 Dice: 0.4113 LR: 0.000250
  Batch [300/499] Loss: 0.3204 Dice: 0.5469 LR: 0.000250
  Batch [350/499] Loss: 0.3308 Dice: 0.5301 LR: 0.000250
  Batch [400/499] Loss: 0.2782 Dice: 0.6069 LR: 0.000250
  Batch [450/499] Loss: 0.2963 Dice: 0.5820 LR: 0.000250
Epoch 231 Train: 	[total] 0.3399	[shape] 0.3392	[dice] 0.5196	[l2] 6.7341
  Training time: 87.55s
  Total epoch time: 87.55s (no validation)

Epoch 232/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2981 Dice: 0.5773 LR: 0.000250
  Batch [50/499] Loss: 0.3202 Dice: 0.5476 LR: 0.000250
  Batch [100/499] Loss: 0.3834 Dice: 0.4586 LR: 0.000250
  Batch [150/499] Loss: 0.3692 Dice: 0.4748 LR: 0.000250
  Batch [200/499] Loss: 0.3061 Dice: 0.5692 LR: 0.000250
  Batch [250/499] Loss: 0.3091 Dice: 0.5639 LR: 0.000250
  Batch [300/499] Loss: 0.3517 Dice: 0.5015 LR: 0.000250
  Batch [350/499] Loss: 0.3875 Dice: 0.4509 LR: 0.000250
  Batch [400/499] Loss: 0.2826 Dice: 0.6018 LR: 0.000250
  Batch [450/499] Loss: 0.2757 Dice: 0.6110 LR: 0.000250
Epoch 232 Train: 	[total] 0.3393	[shape] 0.3386	[dice] 0.5205	[l2] 6.7343
  Training time: 87.44s
  Total epoch time: 87.44s (no validation)

Epoch 233/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3327 Dice: 0.5271 LR: 0.000250
  Batch [50/499] Loss: 0.3681 Dice: 0.4805 LR: 0.000250
  Batch [100/499] Loss: 0.2811 Dice: 0.6036 LR: 0.000250
  Batch [150/499] Loss: 0.3240 Dice: 0.5429 LR: 0.000250
  Batch [200/499] Loss: 0.3535 Dice: 0.5015 LR: 0.000250
  Batch [250/499] Loss: 0.3193 Dice: 0.5507 LR: 0.000250
  Batch [300/499] Loss: 0.2659 Dice: 0.6252 LR: 0.000250
  Batch [350/499] Loss: 0.3192 Dice: 0.5519 LR: 0.000250
  Batch [400/499] Loss: 0.3192 Dice: 0.5495 LR: 0.000250
  Batch [450/499] Loss: 0.2748 Dice: 0.6140 LR: 0.000250
Epoch 233 Train: 	[total] 0.3390	[shape] 0.3383	[dice] 0.5209	[l2] 6.7344
  Training time: 87.73s
  Total epoch time: 87.73s (no validation)

Epoch 234/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3379 Dice: 0.5212 LR: 0.000250
  Batch [50/499] Loss: 0.3280 Dice: 0.5378 LR: 0.000250
  Batch [100/499] Loss: 0.3519 Dice: 0.5034 LR: 0.000250
  Batch [150/499] Loss: 0.3265 Dice: 0.5383 LR: 0.000250
  Batch [200/499] Loss: 0.3486 Dice: 0.5067 LR: 0.000250
  Batch [250/499] Loss: 0.2736 Dice: 0.6147 LR: 0.000250
  Batch [300/499] Loss: 0.3427 Dice: 0.5190 LR: 0.000250
  Batch [350/499] Loss: 0.4090 Dice: 0.4193 LR: 0.000250
  Batch [400/499] Loss: 0.3173 Dice: 0.5496 LR: 0.000250
  Batch [450/499] Loss: 0.2801 Dice: 0.6055 LR: 0.000250
Epoch 234 Train: 	[total] 0.3398	[shape] 0.3392	[dice] 0.5197	[l2] 6.7345
  Training time: 87.76s
  Total epoch time: 87.76s (no validation)

Epoch 235/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3554 Dice: 0.5011 LR: 0.000250
  Batch [50/499] Loss: 0.3327 Dice: 0.5301 LR: 0.000250
  Batch [100/499] Loss: 0.3330 Dice: 0.5285 LR: 0.000250
  Batch [150/499] Loss: 0.4273 Dice: 0.3959 LR: 0.000250
  Batch [200/499] Loss: 0.3374 Dice: 0.5230 LR: 0.000250
  Batch [250/499] Loss: 0.3260 Dice: 0.5385 LR: 0.000250
  Batch [300/499] Loss: 0.2750 Dice: 0.6112 LR: 0.000250
  Batch [350/499] Loss: 0.3468 Dice: 0.5092 LR: 0.000250
  Batch [400/499] Loss: 0.3484 Dice: 0.5121 LR: 0.000250
  Batch [450/499] Loss: 0.3962 Dice: 0.4376 LR: 0.000250
Epoch 235 Train: 	[total] 0.3384	[shape] 0.3377	[dice] 0.5218	[l2] 6.7346
  Training time: 87.75s
  Total epoch time: 87.75s (no validation)

Epoch 236/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2415 Dice: 0.6562 LR: 0.000250
  Batch [50/499] Loss: 0.3687 Dice: 0.4799 LR: 0.000250
  Batch [100/499] Loss: 0.4443 Dice: 0.3700 LR: 0.000250
  Batch [150/499] Loss: 0.3599 Dice: 0.4899 LR: 0.000250
  Batch [200/499] Loss: 0.3268 Dice: 0.5374 LR: 0.000250
  Batch [250/499] Loss: 0.3453 Dice: 0.5105 LR: 0.000250
  Batch [300/499] Loss: 0.3327 Dice: 0.5290 LR: 0.000250
  Batch [350/499] Loss: 0.3328 Dice: 0.5278 LR: 0.000250
  Batch [400/499] Loss: 0.4128 Dice: 0.4183 LR: 0.000250
  Batch [450/499] Loss: 0.2892 Dice: 0.5922 LR: 0.000250
Epoch 236 Train: 	[total] 0.3382	[shape] 0.3375	[dice] 0.5221	[l2] 6.7348
  Training time: 87.48s
  Total epoch time: 87.48s (no validation)

Epoch 237/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2658 Dice: 0.6236 LR: 0.000250
  Batch [50/499] Loss: 0.2799 Dice: 0.6053 LR: 0.000250
  Batch [100/499] Loss: 0.2834 Dice: 0.6027 LR: 0.000250
  Batch [150/499] Loss: 0.3838 Dice: 0.4538 LR: 0.000250
  Batch [200/499] Loss: 0.3016 Dice: 0.5756 LR: 0.000250
  Batch [250/499] Loss: 0.3187 Dice: 0.5533 LR: 0.000250
  Batch [300/499] Loss: 0.4089 Dice: 0.4219 LR: 0.000250
  Batch [350/499] Loss: 0.4176 Dice: 0.4078 LR: 0.000250
  Batch [400/499] Loss: 0.3337 Dice: 0.5279 LR: 0.000250
  Batch [450/499] Loss: 0.3180 Dice: 0.5530 LR: 0.000250
Epoch 237 Train: 	[total] 0.3386	[shape] 0.3380	[dice] 0.5213	[l2] 6.7349
  Training time: 87.56s
  Total epoch time: 87.56s (no validation)

Epoch 238/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2988 Dice: 0.5809 LR: 0.000250
  Batch [50/499] Loss: 0.3267 Dice: 0.5371 LR: 0.000250
  Batch [100/499] Loss: 0.3574 Dice: 0.4932 LR: 0.000250
  Batch [150/499] Loss: 0.3062 Dice: 0.5639 LR: 0.000250
  Batch [200/499] Loss: 0.3363 Dice: 0.5227 LR: 0.000250
  Batch [250/499] Loss: 0.3278 Dice: 0.5354 LR: 0.000250
  Batch [300/499] Loss: 0.4289 Dice: 0.3934 LR: 0.000250
  Batch [350/499] Loss: 0.3334 Dice: 0.5303 LR: 0.000250
  Batch [400/499] Loss: 0.4151 Dice: 0.4130 LR: 0.000250
  Batch [450/499] Loss: 0.2976 Dice: 0.5830 LR: 0.000250
Epoch 238 Train: 	[total] 0.3375	[shape] 0.3368	[dice] 0.5230	[l2] 6.7351
  Training time: 87.51s
  Total epoch time: 87.51s (no validation)

Epoch 239/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.4556 Dice: 0.3527 LR: 0.000250
  Batch [50/499] Loss: 0.3475 Dice: 0.5081 LR: 0.000250
  Batch [100/499] Loss: 0.3277 Dice: 0.5357 LR: 0.000250
  Batch [150/499] Loss: 0.3104 Dice: 0.5645 LR: 0.000250
  Batch [200/499] Loss: 0.4155 Dice: 0.4110 LR: 0.000250
  Batch [250/499] Loss: 0.3905 Dice: 0.4477 LR: 0.000250
  Batch [300/499] Loss: 0.3536 Dice: 0.5024 LR: 0.000250
  Batch [350/499] Loss: 0.3149 Dice: 0.5545 LR: 0.000250
  Batch [400/499] Loss: 0.2851 Dice: 0.5993 LR: 0.000250
  Batch [450/499] Loss: 0.3153 Dice: 0.5556 LR: 0.000250
Epoch 239 Train: 	[total] 0.3384	[shape] 0.3377	[dice] 0.5218	[l2] 6.7352
  Training time: 87.73s
  Total epoch time: 87.73s (no validation)

Epoch 240/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3878 Dice: 0.4506 LR: 0.000250
  Batch [50/499] Loss: 0.3600 Dice: 0.4928 LR: 0.000250
  Batch [100/499] Loss: 0.4010 Dice: 0.4319 LR: 0.000250
  Batch [150/499] Loss: 0.3655 Dice: 0.4830 LR: 0.000250
  Batch [200/499] Loss: 0.2887 Dice: 0.5923 LR: 0.000250
  Batch [250/499] Loss: 0.3501 Dice: 0.5094 LR: 0.000250
  Batch [300/499] Loss: 0.3740 Dice: 0.4731 LR: 0.000250
  Batch [350/499] Loss: 0.3931 Dice: 0.4463 LR: 0.000250
  Batch [400/499] Loss: 0.2674 Dice: 0.6230 LR: 0.000250
  Batch [450/499] Loss: 0.3776 Dice: 0.4663 LR: 0.000250
Epoch 240 Train: 	[total] 0.3377	[shape] 0.3370	[dice] 0.5228	[l2] 6.7353
  Training time: 87.66s
Epoch 240 Eval:  	[total] 0.6026	[shape] 0.6020	[dice] 0.1463	[l2] 6.7355
  Validation time: 202.88s
======================================================================
Found a new best model! Shape Loss: 0.601963
======================================================================
  Total epoch time: 290.55s

Epoch 241/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3394 Dice: 0.5179 LR: 0.000250
  Batch [50/499] Loss: 0.3335 Dice: 0.5333 LR: 0.000250
  Batch [100/499] Loss: 0.2962 Dice: 0.5797 LR: 0.000250
  Batch [150/499] Loss: 0.2969 Dice: 0.5824 LR: 0.000250
  Batch [200/499] Loss: 0.3296 Dice: 0.5338 LR: 0.000250
  Batch [250/499] Loss: 0.3278 Dice: 0.5396 LR: 0.000250
  Batch [300/499] Loss: 0.3212 Dice: 0.5447 LR: 0.000250
  Batch [350/499] Loss: 0.3907 Dice: 0.4463 LR: 0.000250
  Batch [400/499] Loss: 0.3683 Dice: 0.4786 LR: 0.000250
  Batch [450/499] Loss: 0.3513 Dice: 0.5052 LR: 0.000250
Epoch 241 Train: 	[total] 0.3375	[shape] 0.3368	[dice] 0.5230	[l2] 6.7355
  Training time: 87.64s
  Total epoch time: 87.64s (no validation)

Epoch 242/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3546 Dice: 0.5005 LR: 0.000250
  Batch [50/499] Loss: 0.3257 Dice: 0.5418 LR: 0.000250
  Batch [100/499] Loss: 0.3369 Dice: 0.5207 LR: 0.000250
  Batch [150/499] Loss: 0.3685 Dice: 0.4789 LR: 0.000250
  Batch [200/499] Loss: 0.3376 Dice: 0.5234 LR: 0.000250
  Batch [250/499] Loss: 0.3084 Dice: 0.5645 LR: 0.000250
  Batch [300/499] Loss: 0.2932 Dice: 0.5868 LR: 0.000250
  Batch [350/499] Loss: 0.3408 Dice: 0.5200 LR: 0.000250
  Batch [400/499] Loss: 0.3166 Dice: 0.5520 LR: 0.000250
  Batch [450/499] Loss: 0.4099 Dice: 0.4180 LR: 0.000250
Epoch 242 Train: 	[total] 0.3371	[shape] 0.3364	[dice] 0.5237	[l2] 6.7356
  Training time: 87.46s
  Total epoch time: 87.46s (no validation)

Epoch 243/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3333 Dice: 0.5303 LR: 0.000250
  Batch [50/499] Loss: 0.3404 Dice: 0.5194 LR: 0.000250
  Batch [100/499] Loss: 0.2973 Dice: 0.5816 LR: 0.000250
  Batch [150/499] Loss: 0.2639 Dice: 0.6289 LR: 0.000250
  Batch [200/499] Loss: 0.2718 Dice: 0.6214 LR: 0.000250
  Batch [250/499] Loss: 0.3636 Dice: 0.4835 LR: 0.000250
  Batch [300/499] Loss: 0.3486 Dice: 0.5046 LR: 0.000250
  Batch [350/499] Loss: 0.2799 Dice: 0.6034 LR: 0.000250
  Batch [400/499] Loss: 0.3792 Dice: 0.4636 LR: 0.000250
  Batch [450/499] Loss: 0.3893 Dice: 0.4482 LR: 0.000250
Epoch 243 Train: 	[total] 0.3368	[shape] 0.3361	[dice] 0.5241	[l2] 6.7358
  Training time: 87.74s
  Total epoch time: 87.74s (no validation)

Epoch 244/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3240 Dice: 0.5423 LR: 0.000250
  Batch [50/499] Loss: 0.4514 Dice: 0.3593 LR: 0.000250
  Batch [100/499] Loss: 0.4181 Dice: 0.4100 LR: 0.000250
  Batch [150/499] Loss: 0.2897 Dice: 0.5890 LR: 0.000250
  Batch [200/499] Loss: 0.4481 Dice: 0.3639 LR: 0.000250
  Batch [250/499] Loss: 0.3985 Dice: 0.4343 LR: 0.000250
  Batch [300/499] Loss: 0.3279 Dice: 0.5346 LR: 0.000250
  Batch [350/499] Loss: 0.3472 Dice: 0.5123 LR: 0.000250
  Batch [400/499] Loss: 0.2873 Dice: 0.5916 LR: 0.000250
  Batch [450/499] Loss: 0.3106 Dice: 0.5620 LR: 0.000250
Epoch 244 Train: 	[total] 0.3365	[shape] 0.3358	[dice] 0.5245	[l2] 6.7359
  Training time: 87.74s
  Total epoch time: 87.74s (no validation)

Epoch 245/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3515 Dice: 0.5024 LR: 0.000250
  Batch [50/499] Loss: 0.3157 Dice: 0.5515 LR: 0.000250
  Batch [100/499] Loss: 0.3905 Dice: 0.4490 LR: 0.000250
  Batch [150/499] Loss: 0.2786 Dice: 0.6069 LR: 0.000250
  Batch [200/499] Loss: 0.2835 Dice: 0.6009 LR: 0.000250
  Batch [250/499] Loss: 0.4030 Dice: 0.4270 LR: 0.000250
  Batch [300/499] Loss: 0.3769 Dice: 0.4654 LR: 0.000250
  Batch [350/499] Loss: 0.3436 Dice: 0.5153 LR: 0.000250
  Batch [400/499] Loss: 0.2996 Dice: 0.5758 LR: 0.000250
  Batch [450/499] Loss: 0.4073 Dice: 0.4206 LR: 0.000250
Epoch 245 Train: 	[total] 0.3360	[shape] 0.3354	[dice] 0.5251	[l2] 6.7360
  Training time: 87.64s
  Total epoch time: 87.64s (no validation)

Epoch 246/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3425 Dice: 0.5162 LR: 0.000250
  Batch [50/499] Loss: 0.2671 Dice: 0.6248 LR: 0.000250
  Batch [100/499] Loss: 0.2594 Dice: 0.6380 LR: 0.000250
  Batch [150/499] Loss: 0.3787 Dice: 0.4639 LR: 0.000250
  Batch [200/499] Loss: 0.3591 Dice: 0.4939 LR: 0.000250
  Batch [250/499] Loss: 0.3133 Dice: 0.5606 LR: 0.000250
  Batch [300/499] Loss: 0.3114 Dice: 0.5606 LR: 0.000250
  Batch [350/499] Loss: 0.3369 Dice: 0.5237 LR: 0.000250
  Batch [400/499] Loss: 0.3812 Dice: 0.4603 LR: 0.000250
  Batch [450/499] Loss: 0.3434 Dice: 0.5166 LR: 0.000250
Epoch 246 Train: 	[total] 0.3371	[shape] 0.3364	[dice] 0.5236	[l2] 6.7362
  Training time: 87.79s
  Total epoch time: 87.79s (no validation)

Epoch 247/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2955 Dice: 0.5808 LR: 0.000250
  Batch [50/499] Loss: 0.3681 Dice: 0.4794 LR: 0.000250
  Batch [100/499] Loss: 0.3679 Dice: 0.4818 LR: 0.000250
  Batch [150/499] Loss: 0.3522 Dice: 0.5034 LR: 0.000250
  Batch [200/499] Loss: 0.2986 Dice: 0.5780 LR: 0.000250
  Batch [250/499] Loss: 0.4605 Dice: 0.3478 LR: 0.000250
  Batch [300/499] Loss: 0.3276 Dice: 0.5370 LR: 0.000250
  Batch [350/499] Loss: 0.3252 Dice: 0.5398 LR: 0.000250
  Batch [400/499] Loss: 0.3277 Dice: 0.5388 LR: 0.000250
  Batch [450/499] Loss: 0.2174 Dice: 0.6921 LR: 0.000250
Epoch 247 Train: 	[total] 0.3369	[shape] 0.3362	[dice] 0.5240	[l2] 6.7363
  Training time: 87.88s
  Total epoch time: 87.88s (no validation)

Epoch 248/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3810 Dice: 0.4596 LR: 0.000250
  Batch [50/499] Loss: 0.3212 Dice: 0.5471 LR: 0.000250
  Batch [100/499] Loss: 0.2569 Dice: 0.6365 LR: 0.000250
  Batch [150/499] Loss: 0.2981 Dice: 0.5754 LR: 0.000250
  Batch [200/499] Loss: 0.3454 Dice: 0.5158 LR: 0.000250
  Batch [250/499] Loss: 0.3677 Dice: 0.4810 LR: 0.000250
  Batch [300/499] Loss: 0.3417 Dice: 0.5168 LR: 0.000250
  Batch [350/499] Loss: 0.3742 Dice: 0.4675 LR: 0.000250
  Batch [400/499] Loss: 0.3657 Dice: 0.4824 LR: 0.000250
  Batch [450/499] Loss: 0.3428 Dice: 0.5181 LR: 0.000250
Epoch 248 Train: 	[total] 0.3358	[shape] 0.3351	[dice] 0.5255	[l2] 6.7365
  Training time: 87.63s
  Total epoch time: 87.63s (no validation)

Epoch 249/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3588 Dice: 0.4949 LR: 0.000250
  Batch [50/499] Loss: 0.2473 Dice: 0.6480 LR: 0.000250
  Batch [100/499] Loss: 0.2529 Dice: 0.6427 LR: 0.000250
  Batch [150/499] Loss: 0.3055 Dice: 0.5720 LR: 0.000250
  Batch [200/499] Loss: 0.3323 Dice: 0.5312 LR: 0.000250
  Batch [250/499] Loss: 0.3384 Dice: 0.5215 LR: 0.000250
  Batch [300/499] Loss: 0.3751 Dice: 0.4688 LR: 0.000250
  Batch [350/499] Loss: 0.3711 Dice: 0.4754 LR: 0.000250
  Batch [400/499] Loss: 0.3387 Dice: 0.5231 LR: 0.000250
  Batch [450/499] Loss: 0.3172 Dice: 0.5521 LR: 0.000250
Epoch 249 Train: 	[total] 0.3354	[shape] 0.3348	[dice] 0.5260	[l2] 6.7366
  Training time: 87.51s
  Total epoch time: 87.51s (no validation)

Epoch 250/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3447 Dice: 0.5100 LR: 0.000250
  Batch [50/499] Loss: 0.3488 Dice: 0.5048 LR: 0.000250
  Batch [100/499] Loss: 0.3442 Dice: 0.5138 LR: 0.000250
  Batch [150/499] Loss: 0.3760 Dice: 0.4652 LR: 0.000250
  Batch [200/499] Loss: 0.2823 Dice: 0.6018 LR: 0.000250
  Batch [250/499] Loss: 0.2993 Dice: 0.5766 LR: 0.000250
  Batch [300/499] Loss: 0.2821 Dice: 0.6039 LR: 0.000250
  Batch [350/499] Loss: 0.3416 Dice: 0.5159 LR: 0.000250
  Batch [400/499] Loss: 0.3387 Dice: 0.5216 LR: 0.000250
  Batch [450/499] Loss: 0.3033 Dice: 0.5694 LR: 0.000250
Epoch 250 Train: 	[total] 0.3351	[shape] 0.3344	[dice] 0.5265	[l2] 6.7367
  Training time: 87.72s
Epoch 250 Eval:  	[total] 0.5992	[shape] 0.5985	[dice] 0.1515	[l2] 6.7368
  Validation time: 201.99s
======================================================================
Found a new best model! Shape Loss: 0.598479
======================================================================
  Total epoch time: 289.72s

Epoch 251/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3496 Dice: 0.5036 LR: 0.000250
  Batch [50/499] Loss: 0.3700 Dice: 0.4761 LR: 0.000250
  Batch [100/499] Loss: 0.2990 Dice: 0.5806 LR: 0.000250
  Batch [150/499] Loss: 0.3521 Dice: 0.5045 LR: 0.000250
  Batch [200/499] Loss: 0.3981 Dice: 0.4381 LR: 0.000250
  Batch [250/499] Loss: 0.3555 Dice: 0.4957 LR: 0.000250
  Batch [300/499] Loss: 0.3019 Dice: 0.5732 LR: 0.000250
  Batch [350/499] Loss: 0.3433 Dice: 0.5155 LR: 0.000250
  Batch [400/499] Loss: 0.3625 Dice: 0.4876 LR: 0.000250
  Batch [450/499] Loss: 0.3810 Dice: 0.4604 LR: 0.000250
Epoch 251 Train: 	[total] 0.3350	[shape] 0.3343	[dice] 0.5267	[l2] 6.7368
  Training time: 88.23s
  Total epoch time: 88.23s (no validation)

Epoch 252/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2695 Dice: 0.6209 LR: 0.000250
  Batch [50/499] Loss: 0.3601 Dice: 0.4900 LR: 0.000250
  Batch [100/499] Loss: 0.3630 Dice: 0.4866 LR: 0.000250
  Batch [150/499] Loss: 0.3031 Dice: 0.5691 LR: 0.000250
  Batch [200/499] Loss: 0.3722 Dice: 0.4720 LR: 0.000250
  Batch [250/499] Loss: 0.3173 Dice: 0.5530 LR: 0.000250
  Batch [300/499] Loss: 0.3022 Dice: 0.5735 LR: 0.000250
  Batch [350/499] Loss: 0.3436 Dice: 0.5124 LR: 0.000250
  Batch [400/499] Loss: 0.3191 Dice: 0.5508 LR: 0.000250
  Batch [450/499] Loss: 0.3533 Dice: 0.5010 LR: 0.000250
Epoch 252 Train: 	[total] 0.3348	[shape] 0.3341	[dice] 0.5270	[l2] 6.7370
  Training time: 87.85s
  Total epoch time: 87.85s (no validation)

Epoch 253/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3795 Dice: 0.4642 LR: 0.000250
  Batch [50/499] Loss: 0.3450 Dice: 0.5111 LR: 0.000250
  Batch [100/499] Loss: 0.3090 Dice: 0.5610 LR: 0.000250
  Batch [150/499] Loss: 0.2454 Dice: 0.6536 LR: 0.000250
  Batch [200/499] Loss: 0.3303 Dice: 0.5342 LR: 0.000250
  Batch [250/499] Loss: 0.3563 Dice: 0.4946 LR: 0.000250
  Batch [300/499] Loss: 0.2745 Dice: 0.6120 LR: 0.000250
  Batch [350/499] Loss: 0.2956 Dice: 0.5835 LR: 0.000250
  Batch [400/499] Loss: 0.3440 Dice: 0.5133 LR: 0.000250
  Batch [450/499] Loss: 0.3807 Dice: 0.4604 LR: 0.000250
Epoch 253 Train: 	[total] 0.3343	[shape] 0.3337	[dice] 0.5277	[l2] 6.7371
  Training time: 87.71s
  Total epoch time: 87.71s (no validation)

Epoch 254/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3017 Dice: 0.5747 LR: 0.000250
  Batch [50/499] Loss: 0.4110 Dice: 0.4176 LR: 0.000250
  Batch [100/499] Loss: 0.2817 Dice: 0.6049 LR: 0.000250
  Batch [150/499] Loss: 0.2893 Dice: 0.5911 LR: 0.000250
  Batch [200/499] Loss: 0.2585 Dice: 0.6382 LR: 0.000250
  Batch [250/499] Loss: 0.2876 Dice: 0.5958 LR: 0.000250
  Batch [300/499] Loss: 0.4217 Dice: 0.4043 LR: 0.000250
  Batch [350/499] Loss: 0.3616 Dice: 0.4886 LR: 0.000250
  Batch [400/499] Loss: 0.3400 Dice: 0.5187 LR: 0.000250
  Batch [450/499] Loss: 0.3272 Dice: 0.5385 LR: 0.000250
Epoch 254 Train: 	[total] 0.3345	[shape] 0.3338	[dice] 0.5275	[l2] 6.7373
  Training time: 87.66s
  Total epoch time: 87.66s (no validation)

Epoch 255/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3335 Dice: 0.5298 LR: 0.000250
  Batch [50/499] Loss: 0.2748 Dice: 0.6100 LR: 0.000250
  Batch [100/499] Loss: 0.3344 Dice: 0.5285 LR: 0.000250
  Batch [150/499] Loss: 0.3185 Dice: 0.5496 LR: 0.000250
  Batch [200/499] Loss: 0.2917 Dice: 0.5877 LR: 0.000250
  Batch [250/499] Loss: 0.2981 Dice: 0.5804 LR: 0.000250
  Batch [300/499] Loss: 0.3021 Dice: 0.5714 LR: 0.000250
  Batch [350/499] Loss: 0.4746 Dice: 0.3268 LR: 0.000250
  Batch [400/499] Loss: 0.3870 Dice: 0.4515 LR: 0.000250
  Batch [450/499] Loss: 0.4239 Dice: 0.4010 LR: 0.000250
Epoch 255 Train: 	[total] 0.3343	[shape] 0.3336	[dice] 0.5277	[l2] 6.7374
  Training time: 87.59s
  Total epoch time: 87.59s (no validation)

Epoch 256/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2867 Dice: 0.5985 LR: 0.000250
  Batch [50/499] Loss: 0.2668 Dice: 0.6242 LR: 0.000250
  Batch [100/499] Loss: 0.4031 Dice: 0.4295 LR: 0.000250
  Batch [150/499] Loss: 0.2744 Dice: 0.6133 LR: 0.000250
  Batch [200/499] Loss: 0.4063 Dice: 0.4250 LR: 0.000250
  Batch [250/499] Loss: 0.3432 Dice: 0.5144 LR: 0.000250
  Batch [300/499] Loss: 0.3335 Dice: 0.5253 LR: 0.000250
  Batch [350/499] Loss: 0.3135 Dice: 0.5582 LR: 0.000250
  Batch [400/499] Loss: 0.3296 Dice: 0.5343 LR: 0.000250
  Batch [450/499] Loss: 0.3345 Dice: 0.5269 LR: 0.000250
Epoch 256 Train: 	[total] 0.3343	[shape] 0.3336	[dice] 0.5276	[l2] 6.7375
  Training time: 87.72s
  Total epoch time: 87.72s (no validation)

Epoch 257/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3881 Dice: 0.4498 LR: 0.000250
  Batch [50/499] Loss: 0.4161 Dice: 0.4107 LR: 0.000250
  Batch [100/499] Loss: 0.3667 Dice: 0.4804 LR: 0.000250
  Batch [150/499] Loss: 0.4574 Dice: 0.3513 LR: 0.000250
  Batch [200/499] Loss: 0.3179 Dice: 0.5531 LR: 0.000250
  Batch [250/499] Loss: 0.3346 Dice: 0.5302 LR: 0.000250
  Batch [300/499] Loss: 0.3318 Dice: 0.5268 LR: 0.000250
  Batch [350/499] Loss: 0.2848 Dice: 0.5980 LR: 0.000250
  Batch [400/499] Loss: 0.3798 Dice: 0.4640 LR: 0.000250
  Batch [450/499] Loss: 0.3123 Dice: 0.5589 LR: 0.000250
Epoch 257 Train: 	[total] 0.3343	[shape] 0.3336	[dice] 0.5278	[l2] 6.7377
  Training time: 87.75s
  Total epoch time: 87.75s (no validation)

Epoch 258/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2934 Dice: 0.5854 LR: 0.000250
  Batch [50/499] Loss: 0.2802 Dice: 0.6049 LR: 0.000250
  Batch [100/499] Loss: 0.3019 Dice: 0.5722 LR: 0.000250
  Batch [150/499] Loss: 0.4237 Dice: 0.4027 LR: 0.000250
  Batch [200/499] Loss: 0.3475 Dice: 0.5103 LR: 0.000250
  Batch [250/499] Loss: 0.2776 Dice: 0.6067 LR: 0.000250
  Batch [300/499] Loss: 0.3466 Dice: 0.5101 LR: 0.000250
  Batch [350/499] Loss: 0.2332 Dice: 0.6705 LR: 0.000250
  Batch [400/499] Loss: 0.3248 Dice: 0.5436 LR: 0.000250
  Batch [450/499] Loss: 0.2407 Dice: 0.6618 LR: 0.000250
Epoch 258 Train: 	[total] 0.3332	[shape] 0.3325	[dice] 0.5292	[l2] 6.7379
  Training time: 87.66s
  Total epoch time: 87.66s (no validation)

Epoch 259/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3186 Dice: 0.5490 LR: 0.000250
  Batch [50/499] Loss: 0.3472 Dice: 0.5100 LR: 0.000250
  Batch [100/499] Loss: 0.3070 Dice: 0.5679 LR: 0.000250
  Batch [150/499] Loss: 0.2664 Dice: 0.6256 LR: 0.000250
  Batch [200/499] Loss: 0.3325 Dice: 0.5292 LR: 0.000250
  Batch [250/499] Loss: 0.3870 Dice: 0.4514 LR: 0.000250
  Batch [300/499] Loss: 0.3416 Dice: 0.5180 LR: 0.000250
  Batch [350/499] Loss: 0.3240 Dice: 0.5390 LR: 0.000250
  Batch [400/499] Loss: 0.3851 Dice: 0.4561 LR: 0.000250
  Batch [450/499] Loss: 0.3115 Dice: 0.5596 LR: 0.000250
Epoch 259 Train: 	[total] 0.3339	[shape] 0.3332	[dice] 0.5282	[l2] 6.7380
  Training time: 87.98s
  Total epoch time: 87.98s (no validation)

Epoch 260/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2848 Dice: 0.5985 LR: 0.000250
  Batch [50/499] Loss: 0.2681 Dice: 0.6186 LR: 0.000250
  Batch [100/499] Loss: 0.2719 Dice: 0.6156 LR: 0.000250
  Batch [150/499] Loss: 0.3034 Dice: 0.5727 LR: 0.000250
  Batch [200/499] Loss: 0.3010 Dice: 0.5743 LR: 0.000250
  Batch [250/499] Loss: 0.3838 Dice: 0.4543 LR: 0.000250
  Batch [300/499] Loss: 0.4400 Dice: 0.3769 LR: 0.000250
  Batch [350/499] Loss: 0.3398 Dice: 0.5180 LR: 0.000250
  Batch [400/499] Loss: 0.2558 Dice: 0.6385 LR: 0.000250
  Batch [450/499] Loss: 0.4142 Dice: 0.4152 LR: 0.000250
Epoch 260 Train: 	[total] 0.3342	[shape] 0.3336	[dice] 0.5277	[l2] 6.7381
  Training time: 87.76s
Epoch 260 Eval:  	[total] 0.5986	[shape] 0.5979	[dice] 0.1523	[l2] 6.7382
  Validation time: 202.44s
======================================================================
Found a new best model! Shape Loss: 0.597891
======================================================================
  Total epoch time: 290.20s

Epoch 261/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2908 Dice: 0.5887 LR: 0.000250
  Batch [50/499] Loss: 0.3544 Dice: 0.4985 LR: 0.000250
  Batch [100/499] Loss: 0.2894 Dice: 0.5938 LR: 0.000250
  Batch [150/499] Loss: 0.3622 Dice: 0.4887 LR: 0.000250
  Batch [200/499] Loss: 0.2770 Dice: 0.6080 LR: 0.000250
  Batch [250/499] Loss: 0.2458 Dice: 0.6546 LR: 0.000250
  Batch [300/499] Loss: 0.3371 Dice: 0.5274 LR: 0.000250
  Batch [350/499] Loss: 0.2888 Dice: 0.5939 LR: 0.000250
  Batch [400/499] Loss: 0.4044 Dice: 0.4269 LR: 0.000250
  Batch [450/499] Loss: 0.2579 Dice: 0.6393 LR: 0.000250
Epoch 261 Train: 	[total] 0.3333	[shape] 0.3326	[dice] 0.5292	[l2] 6.7382
  Training time: 87.63s
  Total epoch time: 87.63s (no validation)

Epoch 262/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2902 Dice: 0.5913 LR: 0.000250
  Batch [50/499] Loss: 0.3166 Dice: 0.5545 LR: 0.000250
  Batch [100/499] Loss: 0.2952 Dice: 0.5854 LR: 0.000250
  Batch [150/499] Loss: 0.2826 Dice: 0.6022 LR: 0.000250
  Batch [200/499] Loss: 0.3046 Dice: 0.5678 LR: 0.000250
  Batch [250/499] Loss: 0.3092 Dice: 0.5638 LR: 0.000250
  Batch [300/499] Loss: 0.3947 Dice: 0.4440 LR: 0.000250
  Batch [350/499] Loss: 0.3003 Dice: 0.5759 LR: 0.000250
  Batch [400/499] Loss: 0.3470 Dice: 0.5090 LR: 0.000250
  Batch [450/499] Loss: 0.2876 Dice: 0.5936 LR: 0.000250
Epoch 262 Train: 	[total] 0.3329	[shape] 0.3323	[dice] 0.5296	[l2] 6.7384
  Training time: 87.67s
  Total epoch time: 87.67s (no validation)

Epoch 263/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3076 Dice: 0.5647 LR: 0.000250
  Batch [50/499] Loss: 0.2679 Dice: 0.6271 LR: 0.000250
  Batch [100/499] Loss: 0.2641 Dice: 0.6296 LR: 0.000250
  Batch [150/499] Loss: 0.3138 Dice: 0.5581 LR: 0.000250
  Batch [200/499] Loss: 0.3793 Dice: 0.4628 LR: 0.000250
  Batch [250/499] Loss: 0.2839 Dice: 0.5973 LR: 0.000250
  Batch [300/499] Loss: 0.4675 Dice: 0.3352 LR: 0.000250
  Batch [350/499] Loss: 0.3413 Dice: 0.5173 LR: 0.000250
  Batch [400/499] Loss: 0.3636 Dice: 0.4856 LR: 0.000250
  Batch [450/499] Loss: 0.4003 Dice: 0.4327 LR: 0.000250
Epoch 263 Train: 	[total] 0.3335	[shape] 0.3328	[dice] 0.5289	[l2] 6.7386
  Training time: 87.62s
  Total epoch time: 87.62s (no validation)

Epoch 264/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2673 Dice: 0.6212 LR: 0.000250
  Batch [50/499] Loss: 0.2577 Dice: 0.6354 LR: 0.000250
  Batch [100/499] Loss: 0.3276 Dice: 0.5399 LR: 0.000250
  Batch [150/499] Loss: 0.2870 Dice: 0.5958 LR: 0.000250
  Batch [200/499] Loss: 0.3594 Dice: 0.4908 LR: 0.000250
  Batch [250/499] Loss: 0.3189 Dice: 0.5493 LR: 0.000250
  Batch [300/499] Loss: 0.3119 Dice: 0.5644 LR: 0.000250
  Batch [350/499] Loss: 0.3683 Dice: 0.4795 LR: 0.000250
  Batch [400/499] Loss: 0.4042 Dice: 0.4286 LR: 0.000250
  Batch [450/499] Loss: 0.3007 Dice: 0.5756 LR: 0.000250
Epoch 264 Train: 	[total] 0.3333	[shape] 0.3327	[dice] 0.5290	[l2] 6.7387
  Training time: 87.92s
  Total epoch time: 87.92s (no validation)

Epoch 265/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3372 Dice: 0.5233 LR: 0.000250
  Batch [50/499] Loss: 0.2957 Dice: 0.5825 LR: 0.000250
  Batch [100/499] Loss: 0.2793 Dice: 0.6037 LR: 0.000250
  Batch [150/499] Loss: 0.3404 Dice: 0.5183 LR: 0.000250
  Batch [200/499] Loss: 0.2724 Dice: 0.6155 LR: 0.000250
  Batch [250/499] Loss: 0.2648 Dice: 0.6292 LR: 0.000250
  Batch [300/499] Loss: 0.3760 Dice: 0.4683 LR: 0.000250
  Batch [350/499] Loss: 0.2614 Dice: 0.6328 LR: 0.000250
  Batch [400/499] Loss: 0.2979 Dice: 0.5818 LR: 0.000250
  Batch [450/499] Loss: 0.2782 Dice: 0.6092 LR: 0.000250
Epoch 265 Train: 	[total] 0.3324	[shape] 0.3317	[dice] 0.5304	[l2] 6.7388
  Training time: 87.87s
  Total epoch time: 87.87s (no validation)

Epoch 266/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3084 Dice: 0.5649 LR: 0.000250
  Batch [50/499] Loss: 0.3355 Dice: 0.5265 LR: 0.000250
  Batch [100/499] Loss: 0.3199 Dice: 0.5476 LR: 0.000250
  Batch [150/499] Loss: 0.2883 Dice: 0.5920 LR: 0.000250
  Batch [200/499] Loss: 0.3146 Dice: 0.5558 LR: 0.000250
  Batch [250/499] Loss: 0.2776 Dice: 0.6062 LR: 0.000250
  Batch [300/499] Loss: 0.3463 Dice: 0.5118 LR: 0.000250
  Batch [350/499] Loss: 0.3435 Dice: 0.5173 LR: 0.000250
  Batch [400/499] Loss: 0.3372 Dice: 0.5210 LR: 0.000250
  Batch [450/499] Loss: 0.3501 Dice: 0.5071 LR: 0.000250
Epoch 266 Train: 	[total] 0.3323	[shape] 0.3316	[dice] 0.5306	[l2] 6.7390
  Training time: 87.54s
  Total epoch time: 87.54s (no validation)

Epoch 267/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3895 Dice: 0.4485 LR: 0.000250
  Batch [50/499] Loss: 0.3034 Dice: 0.5728 LR: 0.000250
  Batch [100/499] Loss: 0.4147 Dice: 0.4144 LR: 0.000250
  Batch [150/499] Loss: 0.3645 Dice: 0.4836 LR: 0.000250
  Batch [200/499] Loss: 0.3069 Dice: 0.5671 LR: 0.000250
  Batch [250/499] Loss: 0.3922 Dice: 0.4460 LR: 0.000250
  Batch [300/499] Loss: 0.3861 Dice: 0.4536 LR: 0.000250
  Batch [350/499] Loss: 0.3515 Dice: 0.5034 LR: 0.000250
  Batch [400/499] Loss: 0.4606 Dice: 0.3478 LR: 0.000250
  Batch [450/499] Loss: 0.3531 Dice: 0.5002 LR: 0.000250
Epoch 267 Train: 	[total] 0.3329	[shape] 0.3322	[dice] 0.5296	[l2] 6.7391
  Training time: 87.73s
  Total epoch time: 87.73s (no validation)

Epoch 268/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3153 Dice: 0.5531 LR: 0.000250
  Batch [50/499] Loss: 0.3528 Dice: 0.4998 LR: 0.000250
  Batch [100/499] Loss: 0.3004 Dice: 0.5742 LR: 0.000250
  Batch [150/499] Loss: 0.2779 Dice: 0.6082 LR: 0.000250
  Batch [200/499] Loss: 0.2958 Dice: 0.5820 LR: 0.000250
  Batch [250/499] Loss: 0.3517 Dice: 0.5022 LR: 0.000250
  Batch [300/499] Loss: 0.3196 Dice: 0.5464 LR: 0.000250
  Batch [350/499] Loss: 0.3458 Dice: 0.5113 LR: 0.000250
  Batch [400/499] Loss: 0.3796 Dice: 0.4624 LR: 0.000250
  Batch [450/499] Loss: 0.4257 Dice: 0.3973 LR: 0.000250
Epoch 268 Train: 	[total] 0.3329	[shape] 0.3322	[dice] 0.5297	[l2] 6.7393
  Training time: 88.17s
  Total epoch time: 88.17s (no validation)

Epoch 269/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3123 Dice: 0.5591 LR: 0.000250
  Batch [50/499] Loss: 0.2712 Dice: 0.6156 LR: 0.000250
  Batch [100/499] Loss: 0.3662 Dice: 0.4823 LR: 0.000250
  Batch [150/499] Loss: 0.3090 Dice: 0.5634 LR: 0.000250
  Batch [200/499] Loss: 0.3299 Dice: 0.5318 LR: 0.000250
  Batch [250/499] Loss: 0.2820 Dice: 0.6023 LR: 0.000250
  Batch [300/499] Loss: 0.2805 Dice: 0.6026 LR: 0.000250
  Batch [350/499] Loss: 0.2831 Dice: 0.6024 LR: 0.000250
  Batch [400/499] Loss: 0.3454 Dice: 0.5118 LR: 0.000250
  Batch [450/499] Loss: 0.3464 Dice: 0.5117 LR: 0.000250
Epoch 269 Train: 	[total] 0.3317	[shape] 0.3310	[dice] 0.5314	[l2] 6.7394
  Training time: 87.58s
  Total epoch time: 87.58s (no validation)

Epoch 270/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2994 Dice: 0.5769 LR: 0.000250
  Batch [50/499] Loss: 0.3871 Dice: 0.4497 LR: 0.000250
  Batch [100/499] Loss: 0.3035 Dice: 0.5721 LR: 0.000250
  Batch [150/499] Loss: 0.2894 Dice: 0.5906 LR: 0.000250
  Batch [200/499] Loss: 0.3236 Dice: 0.5420 LR: 0.000250
  Batch [250/499] Loss: 0.3429 Dice: 0.5145 LR: 0.000250
  Batch [300/499] Loss: 0.3447 Dice: 0.5124 LR: 0.000250
  Batch [350/499] Loss: 0.3740 Dice: 0.4714 LR: 0.000250
  Batch [400/499] Loss: 0.3499 Dice: 0.5061 LR: 0.000250
  Batch [450/499] Loss: 0.2975 Dice: 0.5808 LR: 0.000250
Epoch 270 Train: 	[total] 0.3318	[shape] 0.3311	[dice] 0.5312	[l2] 6.7396
  Training time: 87.60s
Epoch 270 Eval:  	[total] 0.5955	[shape] 0.5948	[dice] 0.1568	[l2] 6.7397
  Validation time: 202.41s
======================================================================
Found a new best model! Shape Loss: 0.594794
======================================================================
  Total epoch time: 290.01s

Epoch 271/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2576 Dice: 0.6371 LR: 0.000250
  Batch [50/499] Loss: 0.4107 Dice: 0.4204 LR: 0.000250
  Batch [100/499] Loss: 0.3746 Dice: 0.4702 LR: 0.000250
  Batch [150/499] Loss: 0.2992 Dice: 0.5752 LR: 0.000250
  Batch [200/499] Loss: 0.3983 Dice: 0.4387 LR: 0.000250
  Batch [250/499] Loss: 0.3573 Dice: 0.4918 LR: 0.000250
  Batch [300/499] Loss: 0.3060 Dice: 0.5718 LR: 0.000250
  Batch [350/499] Loss: 0.3582 Dice: 0.4965 LR: 0.000250
  Batch [400/499] Loss: 0.3560 Dice: 0.4965 LR: 0.000250
  Batch [450/499] Loss: 0.3304 Dice: 0.5306 LR: 0.000250
Epoch 271 Train: 	[total] 0.3319	[shape] 0.3312	[dice] 0.5312	[l2] 6.7397
  Training time: 87.57s
  Total epoch time: 87.57s (no validation)

Epoch 272/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3434 Dice: 0.5161 LR: 0.000250
  Batch [50/499] Loss: 0.3384 Dice: 0.5193 LR: 0.000250
  Batch [100/499] Loss: 0.3068 Dice: 0.5690 LR: 0.000250
  Batch [150/499] Loss: 0.3162 Dice: 0.5546 LR: 0.000250
  Batch [200/499] Loss: 0.4255 Dice: 0.3962 LR: 0.000250
  Batch [250/499] Loss: 0.2820 Dice: 0.6006 LR: 0.000250
  Batch [300/499] Loss: 0.3284 Dice: 0.5357 LR: 0.000250
  Batch [350/499] Loss: 0.3295 Dice: 0.5355 LR: 0.000250
  Batch [400/499] Loss: 0.3629 Dice: 0.4876 LR: 0.000250
  Batch [450/499] Loss: 0.3474 Dice: 0.5135 LR: 0.000250
Epoch 272 Train: 	[total] 0.3320	[shape] 0.3313	[dice] 0.5309	[l2] 6.7400
  Training time: 87.32s
  Total epoch time: 87.32s (no validation)

Epoch 273/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3443 Dice: 0.5141 LR: 0.000250
  Batch [50/499] Loss: 0.4065 Dice: 0.4235 LR: 0.000250
  Batch [100/499] Loss: 0.3272 Dice: 0.5397 LR: 0.000250
  Batch [150/499] Loss: 0.3241 Dice: 0.5401 LR: 0.000250
  Batch [200/499] Loss: 0.3334 Dice: 0.5273 LR: 0.000250
  Batch [250/499] Loss: 0.2483 Dice: 0.6476 LR: 0.000250
  Batch [300/499] Loss: 0.3577 Dice: 0.4938 LR: 0.000250
  Batch [350/499] Loss: 0.3052 Dice: 0.5707 LR: 0.000250
  Batch [400/499] Loss: 0.2970 Dice: 0.5815 LR: 0.000250
  Batch [450/499] Loss: 0.3936 Dice: 0.4433 LR: 0.000250
Epoch 273 Train: 	[total] 0.3311	[shape] 0.3304	[dice] 0.5322	[l2] 6.7400
  Training time: 87.47s
  Total epoch time: 87.47s (no validation)

Epoch 274/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3973 Dice: 0.4350 LR: 0.000250
  Batch [50/499] Loss: 0.3545 Dice: 0.4985 LR: 0.000250
  Batch [100/499] Loss: 0.2764 Dice: 0.6112 LR: 0.000250
  Batch [150/499] Loss: 0.4093 Dice: 0.4212 LR: 0.000250
  Batch [200/499] Loss: 0.3657 Dice: 0.4840 LR: 0.000250
  Batch [250/499] Loss: 0.3067 Dice: 0.5708 LR: 0.000250
  Batch [300/499] Loss: 0.4049 Dice: 0.4275 LR: 0.000250
  Batch [350/499] Loss: 0.4185 Dice: 0.4080 LR: 0.000250
  Batch [400/499] Loss: 0.3461 Dice: 0.5118 LR: 0.000250
  Batch [450/499] Loss: 0.3556 Dice: 0.4954 LR: 0.000250
Epoch 274 Train: 	[total] 0.3309	[shape] 0.3302	[dice] 0.5326	[l2] 6.7401
  Training time: 87.78s
  Total epoch time: 87.78s (no validation)

Epoch 275/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2700 Dice: 0.6167 LR: 0.000250
  Batch [50/499] Loss: 0.4030 Dice: 0.4309 LR: 0.000250
  Batch [100/499] Loss: 0.3007 Dice: 0.5763 LR: 0.000250
  Batch [150/499] Loss: 0.3467 Dice: 0.5089 LR: 0.000250
  Batch [200/499] Loss: 0.3163 Dice: 0.5543 LR: 0.000250
  Batch [250/499] Loss: 0.2950 Dice: 0.5849 LR: 0.000250
  Batch [300/499] Loss: 0.3414 Dice: 0.5200 LR: 0.000250
  Batch [350/499] Loss: 0.2687 Dice: 0.6224 LR: 0.000250
  Batch [400/499] Loss: 0.4038 Dice: 0.4305 LR: 0.000250
  Batch [450/499] Loss: 0.2887 Dice: 0.5968 LR: 0.000250
Epoch 275 Train: 	[total] 0.3316	[shape] 0.3310	[dice] 0.5315	[l2] 6.7403
  Training time: 87.66s
  Total epoch time: 87.66s (no validation)

Epoch 276/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3149 Dice: 0.5563 LR: 0.000250
  Batch [50/499] Loss: 0.2407 Dice: 0.6596 LR: 0.000250
  Batch [100/499] Loss: 0.3541 Dice: 0.5009 LR: 0.000250
  Batch [150/499] Loss: 0.3229 Dice: 0.5450 LR: 0.000250
  Batch [200/499] Loss: 0.3495 Dice: 0.5055 LR: 0.000250
  Batch [250/499] Loss: 0.3513 Dice: 0.5051 LR: 0.000250
  Batch [300/499] Loss: 0.3750 Dice: 0.4709 LR: 0.000250
  Batch [350/499] Loss: 0.2685 Dice: 0.6206 LR: 0.000250
  Batch [400/499] Loss: 0.3087 Dice: 0.5661 LR: 0.000250
  Batch [450/499] Loss: 0.2985 Dice: 0.5772 LR: 0.000250
Epoch 276 Train: 	[total] 0.3311	[shape] 0.3305	[dice] 0.5322	[l2] 6.7404
  Training time: 87.67s
  Total epoch time: 87.67s (no validation)

Epoch 277/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.4031 Dice: 0.4297 LR: 0.000250
  Batch [50/499] Loss: 0.3318 Dice: 0.5333 LR: 0.000250
  Batch [100/499] Loss: 0.4058 Dice: 0.4237 LR: 0.000250
  Batch [150/499] Loss: 0.2646 Dice: 0.6263 LR: 0.000250
  Batch [200/499] Loss: 0.2879 Dice: 0.5909 LR: 0.000250
  Batch [250/499] Loss: 0.3051 Dice: 0.5668 LR: 0.000250
  Batch [300/499] Loss: 0.2557 Dice: 0.6395 LR: 0.000250
  Batch [350/499] Loss: 0.3772 Dice: 0.4674 LR: 0.000250
  Batch [400/499] Loss: 0.2757 Dice: 0.6114 LR: 0.000250
  Batch [450/499] Loss: 0.3033 Dice: 0.5711 LR: 0.000250
Epoch 277 Train: 	[total] 0.3302	[shape] 0.3295	[dice] 0.5335	[l2] 6.7405
  Training time: 87.75s
  Total epoch time: 87.75s (no validation)

Epoch 278/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2889 Dice: 0.5911 LR: 0.000250
  Batch [50/499] Loss: 0.2817 Dice: 0.6046 LR: 0.000250
  Batch [100/499] Loss: 0.3271 Dice: 0.5389 LR: 0.000250
  Batch [150/499] Loss: 0.3229 Dice: 0.5419 LR: 0.000250
  Batch [200/499] Loss: 0.3515 Dice: 0.5025 LR: 0.000250
  Batch [250/499] Loss: 0.3441 Dice: 0.5163 LR: 0.000250
  Batch [300/499] Loss: 0.3666 Dice: 0.4797 LR: 0.000250
  Batch [350/499] Loss: 0.3194 Dice: 0.5506 LR: 0.000250
  Batch [400/499] Loss: 0.2871 Dice: 0.5950 LR: 0.000250
  Batch [450/499] Loss: 0.3325 Dice: 0.5277 LR: 0.000250
Epoch 278 Train: 	[total] 0.3299	[shape] 0.3292	[dice] 0.5340	[l2] 6.7406
  Training time: 87.58s
  Total epoch time: 87.58s (no validation)

Epoch 279/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3279 Dice: 0.5371 LR: 0.000250
  Batch [50/499] Loss: 0.2816 Dice: 0.6025 LR: 0.000250
  Batch [100/499] Loss: 0.3417 Dice: 0.5170 LR: 0.000250
  Batch [150/499] Loss: 0.2662 Dice: 0.6263 LR: 0.000250
  Batch [200/499] Loss: 0.3307 Dice: 0.5313 LR: 0.000250
  Batch [250/499] Loss: 0.3322 Dice: 0.5309 LR: 0.000250
  Batch [300/499] Loss: 0.3295 Dice: 0.5342 LR: 0.000250
  Batch [350/499] Loss: 0.3226 Dice: 0.5422 LR: 0.000250
  Batch [400/499] Loss: 0.3586 Dice: 0.4912 LR: 0.000250
  Batch [450/499] Loss: 0.4046 Dice: 0.4250 LR: 0.000250
Epoch 279 Train: 	[total] 0.3307	[shape] 0.3300	[dice] 0.5329	[l2] 6.7408
  Training time: 87.66s
  Total epoch time: 87.66s (no validation)

Epoch 280/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3196 Dice: 0.5468 LR: 0.000250
  Batch [50/499] Loss: 0.2243 Dice: 0.6820 LR: 0.000250
  Batch [100/499] Loss: 0.3121 Dice: 0.5573 LR: 0.000250
  Batch [150/499] Loss: 0.3095 Dice: 0.5611 LR: 0.000250
  Batch [200/499] Loss: 0.3895 Dice: 0.4456 LR: 0.000250
  Batch [250/499] Loss: 0.3465 Dice: 0.5090 LR: 0.000250
  Batch [300/499] Loss: 0.2857 Dice: 0.5985 LR: 0.000250
  Batch [350/499] Loss: 0.2828 Dice: 0.6049 LR: 0.000250
  Batch [400/499] Loss: 0.3112 Dice: 0.5632 LR: 0.000250
  Batch [450/499] Loss: 0.3468 Dice: 0.5131 LR: 0.000250
Epoch 280 Train: 	[total] 0.3303	[shape] 0.3296	[dice] 0.5334	[l2] 6.7409
  Training time: 87.62s
Epoch 280 Eval:  	[total] 0.5936	[shape] 0.5930	[dice] 0.1597	[l2] 6.7410
  Validation time: 202.90s
======================================================================
Found a new best model! Shape Loss: 0.592975
======================================================================
  Total epoch time: 290.52s

Epoch 281/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3997 Dice: 0.4347 LR: 0.000250
  Batch [50/499] Loss: 0.2979 Dice: 0.5796 LR: 0.000250
  Batch [100/499] Loss: 0.3911 Dice: 0.4452 LR: 0.000250
  Batch [150/499] Loss: 0.3593 Dice: 0.4946 LR: 0.000250
  Batch [200/499] Loss: 0.3726 Dice: 0.4738 LR: 0.000250
  Batch [250/499] Loss: 0.3127 Dice: 0.5576 LR: 0.000250
  Batch [300/499] Loss: 0.3336 Dice: 0.5301 LR: 0.000250
  Batch [350/499] Loss: 0.3679 Dice: 0.4771 LR: 0.000250
  Batch [400/499] Loss: 0.2528 Dice: 0.6446 LR: 0.000250
  Batch [450/499] Loss: 0.3431 Dice: 0.5152 LR: 0.000250
Epoch 281 Train: 	[total] 0.3304	[shape] 0.3297	[dice] 0.5332	[l2] 6.7410
  Training time: 87.56s
  Total epoch time: 87.56s (no validation)

Epoch 282/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3014 Dice: 0.5753 LR: 0.000250
  Batch [50/499] Loss: 0.2808 Dice: 0.6018 LR: 0.000250
  Batch [100/499] Loss: 0.4103 Dice: 0.4199 LR: 0.000250
  Batch [150/499] Loss: 0.3453 Dice: 0.5143 LR: 0.000250
  Batch [200/499] Loss: 0.3445 Dice: 0.5098 LR: 0.000250
  Batch [250/499] Loss: 0.4434 Dice: 0.3710 LR: 0.000250
  Batch [300/499] Loss: 0.2325 Dice: 0.6714 LR: 0.000250
  Batch [350/499] Loss: 0.2854 Dice: 0.6013 LR: 0.000250
  Batch [400/499] Loss: 0.2710 Dice: 0.6195 LR: 0.000250
  Batch [450/499] Loss: 0.2709 Dice: 0.6204 LR: 0.000250
Epoch 282 Train: 	[total] 0.3302	[shape] 0.3295	[dice] 0.5335	[l2] 6.7411
  Training time: 87.73s
  Total epoch time: 87.73s (no validation)

Epoch 283/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3460 Dice: 0.5096 LR: 0.000250
  Batch [50/499] Loss: 0.3797 Dice: 0.4607 LR: 0.000250
  Batch [100/499] Loss: 0.3476 Dice: 0.5094 LR: 0.000250
  Batch [150/499] Loss: 0.3100 Dice: 0.5616 LR: 0.000250
  Batch [200/499] Loss: 0.3684 Dice: 0.4791 LR: 0.000250
  Batch [250/499] Loss: 0.2512 Dice: 0.6444 LR: 0.000250
  Batch [300/499] Loss: 0.2935 Dice: 0.5843 LR: 0.000250
  Batch [350/499] Loss: 0.3692 Dice: 0.4744 LR: 0.000250
  Batch [400/499] Loss: 0.3528 Dice: 0.5025 LR: 0.000250
  Batch [450/499] Loss: 0.3782 Dice: 0.4645 LR: 0.000250
Epoch 283 Train: 	[total] 0.3300	[shape] 0.3294	[dice] 0.5338	[l2] 6.7412
  Training time: 87.55s
  Total epoch time: 87.55s (no validation)

Epoch 284/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3769 Dice: 0.4670 LR: 0.000250
  Batch [50/499] Loss: 0.3439 Dice: 0.5160 LR: 0.000250
  Batch [100/499] Loss: 0.4303 Dice: 0.3948 LR: 0.000250
  Batch [150/499] Loss: 0.2498 Dice: 0.6510 LR: 0.000250
  Batch [200/499] Loss: 0.3586 Dice: 0.4921 LR: 0.000250
  Batch [250/499] Loss: 0.2834 Dice: 0.6019 LR: 0.000250
  Batch [300/499] Loss: 0.3541 Dice: 0.5001 LR: 0.000250
  Batch [350/499] Loss: 0.4089 Dice: 0.4195 LR: 0.000250
  Batch [400/499] Loss: 0.2896 Dice: 0.5919 LR: 0.000250
  Batch [450/499] Loss: 0.3572 Dice: 0.4964 LR: 0.000250
Epoch 284 Train: 	[total] 0.3297	[shape] 0.3290	[dice] 0.5342	[l2] 6.7414
  Training time: 87.38s
  Total epoch time: 87.38s (no validation)

Epoch 285/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3574 Dice: 0.4944 LR: 0.000250
  Batch [50/499] Loss: 0.3372 Dice: 0.5239 LR: 0.000250
  Batch [100/499] Loss: 0.4838 Dice: 0.3144 LR: 0.000250
  Batch [150/499] Loss: 0.3348 Dice: 0.5232 LR: 0.000250
  Batch [200/499] Loss: 0.3605 Dice: 0.4897 LR: 0.000250
  Batch [250/499] Loss: 0.3623 Dice: 0.4877 LR: 0.000250
  Batch [300/499] Loss: 0.2498 Dice: 0.6479 LR: 0.000250
  Batch [350/499] Loss: 0.2818 Dice: 0.6040 LR: 0.000250
  Batch [400/499] Loss: 0.2519 Dice: 0.6471 LR: 0.000250
  Batch [450/499] Loss: 0.3293 Dice: 0.5353 LR: 0.000250
Epoch 285 Train: 	[total] 0.3303	[shape] 0.3296	[dice] 0.5334	[l2] 6.7416
  Training time: 87.52s
  Total epoch time: 87.52s (no validation)

Epoch 286/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3034 Dice: 0.5736 LR: 0.000250
  Batch [50/499] Loss: 0.4851 Dice: 0.3120 LR: 0.000250
  Batch [100/499] Loss: 0.3606 Dice: 0.4891 LR: 0.000250
  Batch [150/499] Loss: 0.2921 Dice: 0.5881 LR: 0.000250
  Batch [200/499] Loss: 0.3053 Dice: 0.5701 LR: 0.000250
  Batch [250/499] Loss: 0.4441 Dice: 0.3685 LR: 0.000250
  Batch [300/499] Loss: 0.2726 Dice: 0.6145 LR: 0.000250
  Batch [350/499] Loss: 0.2550 Dice: 0.6381 LR: 0.000250
  Batch [400/499] Loss: 0.3528 Dice: 0.5033 LR: 0.000250
  Batch [450/499] Loss: 0.2768 Dice: 0.6117 LR: 0.000250
Epoch 286 Train: 	[total] 0.3294	[shape] 0.3287	[dice] 0.5346	[l2] 6.7417
  Training time: 87.46s
  Total epoch time: 87.46s (no validation)

Epoch 287/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3700 Dice: 0.4785 LR: 0.000250
  Batch [50/499] Loss: 0.3394 Dice: 0.5204 LR: 0.000250
  Batch [100/499] Loss: 0.3015 Dice: 0.5761 LR: 0.000250
  Batch [150/499] Loss: 0.3424 Dice: 0.5168 LR: 0.000250
  Batch [200/499] Loss: 0.2746 Dice: 0.6134 LR: 0.000250
  Batch [250/499] Loss: 0.3256 Dice: 0.5391 LR: 0.000250
  Batch [300/499] Loss: 0.3199 Dice: 0.5507 LR: 0.000250
  Batch [350/499] Loss: 0.3756 Dice: 0.4671 LR: 0.000250
  Batch [400/499] Loss: 0.3799 Dice: 0.4610 LR: 0.000250
  Batch [450/499] Loss: 0.3609 Dice: 0.4873 LR: 0.000250
Epoch 287 Train: 	[total] 0.3287	[shape] 0.3281	[dice] 0.5356	[l2] 6.7418
  Training time: 87.74s
  Total epoch time: 87.74s (no validation)

Epoch 288/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3381 Dice: 0.5241 LR: 0.000250
  Batch [50/499] Loss: 0.3122 Dice: 0.5579 LR: 0.000250
  Batch [100/499] Loss: 0.3672 Dice: 0.4786 LR: 0.000250
  Batch [150/499] Loss: 0.3437 Dice: 0.5128 LR: 0.000250
  Batch [200/499] Loss: 0.2587 Dice: 0.6353 LR: 0.000250
  Batch [250/499] Loss: 0.3558 Dice: 0.4966 LR: 0.000250
  Batch [300/499] Loss: 0.5032 Dice: 0.2857 LR: 0.000250
  Batch [350/499] Loss: 0.3629 Dice: 0.4871 LR: 0.000250
  Batch [400/499] Loss: 0.3433 Dice: 0.5149 LR: 0.000250
  Batch [450/499] Loss: 0.3738 Dice: 0.4737 LR: 0.000250
Epoch 288 Train: 	[total] 0.3293	[shape] 0.3286	[dice] 0.5349	[l2] 6.7421
  Training time: 87.74s
  Total epoch time: 87.74s (no validation)

Epoch 289/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3724 Dice: 0.4740 LR: 0.000250
  Batch [50/499] Loss: 0.3646 Dice: 0.4846 LR: 0.000250
  Batch [100/499] Loss: 0.2912 Dice: 0.5909 LR: 0.000250
  Batch [150/499] Loss: 0.3101 Dice: 0.5625 LR: 0.000250
  Batch [200/499] Loss: 0.2949 Dice: 0.5841 LR: 0.000250
  Batch [250/499] Loss: 0.4130 Dice: 0.4154 LR: 0.000250
  Batch [300/499] Loss: 0.3456 Dice: 0.5133 LR: 0.000250
  Batch [350/499] Loss: 0.3262 Dice: 0.5382 LR: 0.000250
  Batch [400/499] Loss: 0.4048 Dice: 0.4269 LR: 0.000250
  Batch [450/499] Loss: 0.4144 Dice: 0.4128 LR: 0.000250
Epoch 289 Train: 	[total] 0.3288	[shape] 0.3281	[dice] 0.5355	[l2] 6.7422
  Training time: 87.41s
  Total epoch time: 87.41s (no validation)

Epoch 290/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3097 Dice: 0.5610 LR: 0.000250
  Batch [50/499] Loss: 0.3781 Dice: 0.4652 LR: 0.000250
  Batch [100/499] Loss: 0.3248 Dice: 0.5394 LR: 0.000250
  Batch [150/499] Loss: 0.3446 Dice: 0.5126 LR: 0.000250
  Batch [200/499] Loss: 0.3153 Dice: 0.5566 LR: 0.000250
  Batch [250/499] Loss: 0.2936 Dice: 0.5854 LR: 0.000250
  Batch [300/499] Loss: 0.4220 Dice: 0.4017 LR: 0.000250
  Batch [350/499] Loss: 0.2956 Dice: 0.5821 LR: 0.000250
  Batch [400/499] Loss: 0.2687 Dice: 0.6200 LR: 0.000250
  Batch [450/499] Loss: 0.3916 Dice: 0.4449 LR: 0.000250
Epoch 290 Train: 	[total] 0.3289	[shape] 0.3282	[dice] 0.5354	[l2] 6.7424
  Training time: 87.57s
Epoch 290 Eval:  	[total] 0.5894	[shape] 0.5887	[dice] 0.1659	[l2] 6.7425
  Validation time: 202.64s
======================================================================
Found a new best model! Shape Loss: 0.588734
======================================================================
  Total epoch time: 290.22s

Epoch 291/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2422 Dice: 0.6607 LR: 0.000250
  Batch [50/499] Loss: 0.3645 Dice: 0.4853 LR: 0.000250
  Batch [100/499] Loss: 0.3685 Dice: 0.4803 LR: 0.000250
  Batch [150/499] Loss: 0.2876 Dice: 0.5918 LR: 0.000250
  Batch [200/499] Loss: 0.3179 Dice: 0.5524 LR: 0.000250
  Batch [250/499] Loss: 0.3383 Dice: 0.5201 LR: 0.000250
  Batch [300/499] Loss: 0.3506 Dice: 0.5027 LR: 0.000250
  Batch [350/499] Loss: 0.2833 Dice: 0.5998 LR: 0.000250
  Batch [400/499] Loss: 0.3731 Dice: 0.4705 LR: 0.000250
  Batch [450/499] Loss: 0.3739 Dice: 0.4713 LR: 0.000250
Epoch 291 Train: 	[total] 0.3291	[shape] 0.3285	[dice] 0.5351	[l2] 6.7425
  Training time: 87.36s
  Total epoch time: 87.36s (no validation)

Epoch 292/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3999 Dice: 0.4312 LR: 0.000250
  Batch [50/499] Loss: 0.4151 Dice: 0.4115 LR: 0.000250
  Batch [100/499] Loss: 0.3726 Dice: 0.4726 LR: 0.000250
  Batch [150/499] Loss: 0.2999 Dice: 0.5766 LR: 0.000250
  Batch [200/499] Loss: 0.3391 Dice: 0.5203 LR: 0.000250
  Batch [250/499] Loss: 0.2834 Dice: 0.5999 LR: 0.000250
  Batch [300/499] Loss: 0.3409 Dice: 0.5175 LR: 0.000250
  Batch [350/499] Loss: 0.3322 Dice: 0.5294 LR: 0.000250
  Batch [400/499] Loss: 0.3271 Dice: 0.5356 LR: 0.000250
  Batch [450/499] Loss: 0.2516 Dice: 0.6470 LR: 0.000250
Epoch 292 Train: 	[total] 0.3290	[shape] 0.3284	[dice] 0.5353	[l2] 6.7427
  Training time: 87.63s
  Total epoch time: 87.63s (no validation)

Epoch 293/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2721 Dice: 0.6186 LR: 0.000250
  Batch [50/499] Loss: 0.3093 Dice: 0.5656 LR: 0.000250
  Batch [100/499] Loss: 0.3383 Dice: 0.5211 LR: 0.000250
  Batch [150/499] Loss: 0.3991 Dice: 0.4364 LR: 0.000250
  Batch [200/499] Loss: 0.3294 Dice: 0.5325 LR: 0.000250
  Batch [250/499] Loss: 0.2606 Dice: 0.6334 LR: 0.000250
  Batch [300/499] Loss: 0.3380 Dice: 0.5220 LR: 0.000250
  Batch [350/499] Loss: 0.3810 Dice: 0.4634 LR: 0.000250
  Batch [400/499] Loss: 0.3310 Dice: 0.5327 LR: 0.000250
  Batch [450/499] Loss: 0.2761 Dice: 0.6103 LR: 0.000250
Epoch 293 Train: 	[total] 0.3280	[shape] 0.3273	[dice] 0.5366	[l2] 6.7428
  Training time: 87.35s
  Total epoch time: 87.35s (no validation)

Epoch 294/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3279 Dice: 0.5366 LR: 0.000250
  Batch [50/499] Loss: 0.3500 Dice: 0.5049 LR: 0.000250
  Batch [100/499] Loss: 0.3349 Dice: 0.5271 LR: 0.000250
  Batch [150/499] Loss: 0.2991 Dice: 0.5802 LR: 0.000250
  Batch [200/499] Loss: 0.3639 Dice: 0.4862 LR: 0.000250
  Batch [250/499] Loss: 0.4327 Dice: 0.3866 LR: 0.000250
  Batch [300/499] Loss: 0.3815 Dice: 0.4602 LR: 0.000250
  Batch [350/499] Loss: 0.3564 Dice: 0.4950 LR: 0.000250
  Batch [400/499] Loss: 0.2757 Dice: 0.6152 LR: 0.000250
  Batch [450/499] Loss: 0.3874 Dice: 0.4493 LR: 0.000250
Epoch 294 Train: 	[total] 0.3287	[shape] 0.3280	[dice] 0.5356	[l2] 6.7429
  Training time: 87.57s
  Total epoch time: 87.57s (no validation)

Epoch 295/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2834 Dice: 0.6024 LR: 0.000250
  Batch [50/499] Loss: 0.3475 Dice: 0.5087 LR: 0.000250
  Batch [100/499] Loss: 0.3369 Dice: 0.5235 LR: 0.000250
  Batch [150/499] Loss: 0.3236 Dice: 0.5463 LR: 0.000250
  Batch [200/499] Loss: 0.3458 Dice: 0.5086 LR: 0.000250
  Batch [250/499] Loss: 0.2552 Dice: 0.6433 LR: 0.000250
  Batch [300/499] Loss: 0.3341 Dice: 0.5288 LR: 0.000250
  Batch [350/499] Loss: 0.3095 Dice: 0.5647 LR: 0.000250
  Batch [400/499] Loss: 0.3242 Dice: 0.5418 LR: 0.000250
  Batch [450/499] Loss: 0.4110 Dice: 0.4169 LR: 0.000250
Epoch 295 Train: 	[total] 0.3287	[shape] 0.3280	[dice] 0.5358	[l2] 6.7431
  Training time: 87.44s
  Total epoch time: 87.44s (no validation)

Epoch 296/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3002 Dice: 0.5739 LR: 0.000250
  Batch [50/499] Loss: 0.3627 Dice: 0.4886 LR: 0.000250
  Batch [100/499] Loss: 0.3494 Dice: 0.5093 LR: 0.000250
  Batch [150/499] Loss: 0.3184 Dice: 0.5494 LR: 0.000250
  Batch [200/499] Loss: 0.2845 Dice: 0.5972 LR: 0.000250
  Batch [250/499] Loss: 0.3125 Dice: 0.5583 LR: 0.000250
  Batch [300/499] Loss: 0.2673 Dice: 0.6260 LR: 0.000250
  Batch [350/499] Loss: 0.3279 Dice: 0.5380 LR: 0.000250
  Batch [400/499] Loss: 0.2869 Dice: 0.5944 LR: 0.000250
  Batch [450/499] Loss: 0.2466 Dice: 0.6526 LR: 0.000250
Epoch 296 Train: 	[total] 0.3290	[shape] 0.3283	[dice] 0.5353	[l2] 6.7432
  Training time: 87.67s
  Total epoch time: 87.67s (no validation)

Epoch 297/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3758 Dice: 0.4679 LR: 0.000250
  Batch [50/499] Loss: 0.3432 Dice: 0.5165 LR: 0.000250
  Batch [100/499] Loss: 0.3079 Dice: 0.5654 LR: 0.000250
  Batch [150/499] Loss: 0.2868 Dice: 0.5974 LR: 0.000250
  Batch [200/499] Loss: 0.3133 Dice: 0.5611 LR: 0.000250
  Batch [250/499] Loss: 0.4038 Dice: 0.4284 LR: 0.000250
  Batch [300/499] Loss: 0.2809 Dice: 0.6035 LR: 0.000250
  Batch [350/499] Loss: 0.3554 Dice: 0.4977 LR: 0.000250
  Batch [400/499] Loss: 0.3964 Dice: 0.4377 LR: 0.000250
  Batch [450/499] Loss: 0.3181 Dice: 0.5537 LR: 0.000250
Epoch 297 Train: 	[total] 0.3291	[shape] 0.3284	[dice] 0.5351	[l2] 6.7434
  Training time: 87.77s
  Total epoch time: 87.77s (no validation)

Epoch 298/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.2874 Dice: 0.5954 LR: 0.000250
  Batch [50/499] Loss: 0.3272 Dice: 0.5408 LR: 0.000250
  Batch [100/499] Loss: 0.3707 Dice: 0.4733 LR: 0.000250
  Batch [150/499] Loss: 0.3405 Dice: 0.5226 LR: 0.000250
  Batch [200/499] Loss: 0.3188 Dice: 0.5512 LR: 0.000250
  Batch [250/499] Loss: 0.3356 Dice: 0.5257 LR: 0.000250
  Batch [300/499] Loss: 0.2852 Dice: 0.6014 LR: 0.000250
  Batch [350/499] Loss: 0.3045 Dice: 0.5710 LR: 0.000250
  Batch [400/499] Loss: 0.2385 Dice: 0.6622 LR: 0.000250
  Batch [450/499] Loss: 0.2989 Dice: 0.5816 LR: 0.000250
Epoch 298 Train: 	[total] 0.3276	[shape] 0.3270	[dice] 0.5372	[l2] 6.7435
  Training time: 87.48s
  Total epoch time: 87.48s (no validation)

Epoch 299/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.3380 Dice: 0.5222 LR: 0.000250
  Batch [50/499] Loss: 0.3745 Dice: 0.4695 LR: 0.000250
  Batch [100/499] Loss: 0.3823 Dice: 0.4583 LR: 0.000250
  Batch [150/499] Loss: 0.3432 Dice: 0.5135 LR: 0.000250
  Batch [200/499] Loss: 0.2815 Dice: 0.6033 LR: 0.000250
  Batch [250/499] Loss: 0.3524 Dice: 0.5008 LR: 0.000250
  Batch [300/499] Loss: 0.3562 Dice: 0.4967 LR: 0.000250
  Batch [350/499] Loss: 0.3182 Dice: 0.5500 LR: 0.000250
  Batch [400/499] Loss: 0.3956 Dice: 0.4391 LR: 0.000250
  Batch [450/499] Loss: 0.3518 Dice: 0.5036 LR: 0.000250
Epoch 299 Train: 	[total] 0.3278	[shape] 0.3271	[dice] 0.5369	[l2] 6.7437
  Training time: 87.52s
  Total epoch time: 87.52s (no validation)

Epoch 300/300 (boundary_bias=10%)
  Batch [0/499] Loss: 0.4171 Dice: 0.4070 LR: 0.000250
  Batch [50/499] Loss: 0.2991 Dice: 0.5765 LR: 0.000250
  Batch [100/499] Loss: 0.2871 Dice: 0.5961 LR: 0.000250
  Batch [150/499] Loss: 0.3369 Dice: 0.5242 LR: 0.000250
  Batch [200/499] Loss: 0.3343 Dice: 0.5321 LR: 0.000250
  Batch [250/499] Loss: 0.2920 Dice: 0.5900 LR: 0.000250
  Batch [300/499] Loss: 0.3451 Dice: 0.5155 LR: 0.000250
  Batch [350/499] Loss: 0.2944 Dice: 0.5841 LR: 0.000250
  Batch [400/499] Loss: 0.3669 Dice: 0.4818 LR: 0.000250
  Batch [450/499] Loss: 0.3248 Dice: 0.5438 LR: 0.000250
Epoch 300 Train: 	[total] 0.3272	[shape] 0.3266	[dice] 0.5378	[l2] 6.7438
  Training time: 87.49s
Epoch 300 Eval:  	[total] 0.5904	[shape] 0.5897	[dice] 0.1645	[l2] 6.7439
  Validation time: 202.06s
  Total epoch time: 289.55s


======================================================================
Training completed!
Checkpoints saved to: ./checkpoints/Coronary_class9_shape_only_251109_092755
======================================================================

