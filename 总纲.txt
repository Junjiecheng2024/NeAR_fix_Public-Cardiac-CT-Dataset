我们现在要做的项目叫NeAR_fix_Public-Cardiac-CT-Dataset，
我有一个修复工具near，还有一个数据集Public-Cardiac-CT-Dataset，
我要用这个修复工具near去修复这个数据集，利用隐式表征的天然光滑性，让每个类更加的光滑（除了0背景类）。
最终我要的到的东西是：一个共享隐式函数 F_Θ，每个样本一个潜向量 z_i。我利用这两个东西，可以推理得到完整的多类分割：10 个前景类 （只用 F_Θ 和 z_i，在网格上 evaluate 一次 ）。

------------------------------------------------------------------------------------------------------
项目流程如下：
阶段一：
对每个类 c（LAA / PV / CA / Myo / LA / LV / RA / RV / Ao / PA）：用原数据集标签抽出二值掩膜 M_{i,c}（0背景类及当前类）。在该类的 bbox+margin 上（已做，数据集地址为/home/user/persistent/NeAR_fix_Public-Cardiac-CT-Dataset/dataset/near_format_data）训练 shape-only（要改代码，只要输入shape，不要appearance） NeAR（要小改模型，前面的板块不动，MLP改成
fc1: in_ch（323 channels） -> 256 + ReLU
fc2: 256 -> 256 + ReLU
skip: concat([h:256, input:in_ch]) -> 256+in_ch
fc3: (256+in_ch) -> 128 + ReLU
fc4: 128 -> 64 + ReLU
output: 64 -> 1）：隐式函数：F_c(z_{i,c}, p) → o ∈ (0,1)损失：BCE + ||z_{i,c}||²
在训练集上，用 F_c 得到每个样本修复后的 mask：阈值 → M^{ref}_{i,c}
注意我们要的是修复后的掩膜，而不是模型本身，所以将所有样本作为训练集和验证集，不考虑任何防过拟合。我们就是要让模型“过拟合”，让验证集（还是全体样本）的loss最小。
做 per-class CC 清理（小连通分量删掉等）得到一个「单类干净版本」。
在采样时使用偏采样（biased_sampling：sampling_bias_ratio=0.5，sampling_dilation_radius=2）
阶段一目标：获得比原始 noisy 标签更干净的 per-class ground truth，尤其是小结构。

阶段二：
融合成干净的多类标签：对每个样本 i：手上有 10 个 M^{ref}_{i,c}。用固定优先级做一次性的融合，得到多类标签 L^{ref}_i：
优先级：Coronary (索引 9)＞LAA (索引 8)＞PV (索引 10)＞LA (索引 2)＞LV (索引 3)＞Myocardium (索引 1)＞Aorta (索引 6)＞RA (索引 4)＞RV (索引 5)＞PA (索引 7)
然后做一轮解剖一致性修正：LAA 需连 LA；不连删掉。PV 需连 LA；不连删。CA 需在 Myo/Ao 附近；悬空小段删。Myo 填不合理小洞。心腔/Ao/PA 保留最大 CC，删假腔。
各个类CC为：
心肌 (Myocardium) ： 1.00 
左房 (LA) ： 1.00 
左室 (LV) ： 1.00 
右房 (RA) ： 1.00 
右室 (RV) ： 1.00 
主动脉 (Aorta) ：2
肺动脉 (PA) ： 1
左心耳 (LAA)：1
冠状动脉 (Coronary) ： 2
肺静脉 (PV) ： 2


阶段二目标：得到一套 高质量的多类标签 L^{ref}_i，这是后面训练统一隐式函数的监督信号。

阶段三：
1.模型形式：一个隐式函数 F_Θ(z_i, p) → 多个独立二分类头（sigmoid），再通过规则选最终类别。
对每个样本 i，有一个 latent 向量 z_i（最终我们只保留它）。
隐式函数：FΘ(zi,p)=[s1(p),s2(p),...,sC(p)]
C = 10 前景类。
每个 s_c(p) 通过 sigmoid 变成 P_c(p) ∈ (0,1)，表示“属于类 c 的置信度”。
2.监督信号：来自 L^{ref}_i
对训练点 p：L^{ref}_i(p) = 真正的类别 k（0..C）。
对每个类 c：
我们定义二值 label：
y_c(p) = 1 if (k == c)
y_c(p) = 0 if (k != c)
也就是说，对每个 voxel，我们对所有前景类同时施加约束：
正类：对应的头要输出接近 1；
其他类：对应的头输出接近 0。
3.损失：独立 BCE + 类别重加权 + 正样本重采样
核心公式（对前景类）：
L=c=1∑Cαc⋅BCE(σ(sc(p)),yc(p))+λ∥zi∥2
关键操作来救小类：
正样本重采样（最重要）：
每个 iteration：
从每个类 c 的正例中均匀抽取一定数量的点（比如每类 500–2000）；
再加一部分背景/大类点作负例。
等价于：人为保证每个 batch 内，小类也有足够多正样本参与更新。
类别权重 α_c：αc∝1/fc，f_c 为该类体素占比。
这样即使小类占比 0.19%，也能有可观梯度。
不使用 softmax：
这样小类不用和背景争同一份“总概率”，只要它在自己那路 head 上学会“这里是我/不是我”就行。
4.推理：从 F_Θ + z_i 得到最终多类掩膜
对样本 i：
拿到训练好的 z_i 和 F_Θ。
在目标体素网格上，对每个 voxel 中心 p：算各类的 P_c(p) = sigmoid(s_c(p))。
决策类别：优先级 + 阈值（更稳，对小类友好）
对每个 voxel：
从高优先级到低优先级检查：
若某类 c 的 P_c(p) ≥ t_c（类自适应阈值），就赋值为 c，停止；
若都不满足，就设为背景或低优先级预测。
这样可以显式照顾小类（给更低阈值 / 更高优先级），而不会被大类压制。
注意：这里的“融合”完全在一个模型输出内部完成，不是跑 10 个独立模型后拼。

最终得到：
一个 F_Θ
一个 z_i（每样本）
一个固定的 per-class index 约定
一个明确的决策规则（优先级）

总结：
单类 NeAR（shape-only） → 修干净每个类的标签（训练用 teacher）。

用这些修好的标签构造高质量多类 L^{ref}。

训练一个统一隐式模型：
输入 (z_i, p)
输出每个类的独立 occupancy（多头 sigmoid，而不是 softmax）
用重采样 + 类别加权 + 蒸馏确保小类学得住。
（蒸馏在这里的作用
BCE + 伪标签 L_ref 教给 FΘ 哪个 voxel 属于哪个类；
蒸馏 教给 FΘ “这个类的隐式表面应该长什么样”、“边界的模糊区在哪”，
尤其对小类（样本少、正例稀）提供更密的信号。
简单说：
蒸馏 = 用单类 NeAR 的隐式输出 (teacher) 去软约束统一隐式模型的对应通道 (student)，
这样即使小类样本少，FΘ 也能继承它们的几何形状和边界分布。）

推理时只用这个统一模型，一次性生成全心脏 10 类掩膜。

-----------------------------------------------------------------------------------------------
工程细节：
项目环境：
module load python-data/3.10-24.04
source /projappl/project_2016517/JunjieCheng/junjieenv/bin/activate 

#原始CT图像
/scratch/project_2016517/junjie/dataset/original/images
#原始mask
/scratch/project_2016517/junjie/dataset/original/segmentations
#bounding box(margin:10)后的图像：
/scratch/project_2016517/junjie/dataset/bboxed/images
#bounding box(margin:10)后的mask：
/scratch/project_2016517/junjie/dataset/bboxed/segmentations
resize到256的原图：
/scratch/project_2016517/junjie/dataset/near_format_data/appearance
resize到256的mask（训练用数据集）：
/scratch/project_2016517/junjie/dataset/near_format_data/shape


启动训练用nohub和-u
nohub python3 -u train_coronary.py

9coronary：

构造边界带（band）
对当前类的二值 mask M：
膨胀：M_dilate = dilate(M, radius=2)
腐蚀：M_erode = erode(M, radius=2)
边界带：B=(Mdilate∖Merode)
这个 B 就是一圈包住真实边界（inside+outside）的“困难区域”。
每个 batch / 每个 ROI 采 N 个点：
N × 0.5 从边界带 B 里面采（正负都有，自动包含 inside/outside）；
N × 0.5 在整个 ROI 里均匀采样（保证覆盖整体、避免只学边界）。
这样有几个好处：
边界点被显著放大权重 → 隐式面的位置和光滑度学得好；
仍保留 ROI 全局采样 → 模型知道哪里绝对是 0/1，不会只记一圈；
对小结构尤为关键：如果不偏置，它们在 ROI 里的正样本本来就不多。
sampling_bias_ratio = 0.5
sampling_dilation_radius = 2（单位=voxel）

动态调整边界偏采样比例
在训练过程中逐步降低边界偏采样比例，让模型从"容易"（边界密集）过渡到"困难"（真实分布）：
Epoch 1-100: 50% boundary bias（当前）
Epoch 101-200: 30% boundary bias
Epoch 201-300: 10% boundary bias
这样模型最终能适应真实分布。

Enhanced Combined Loss - 70% Dice + 30% Focal (gamma=4.0)

目前进度：
阶段一，coronary，测试多GPU代码，申请临时节点，调用
python train_coronary_pl.py --devices 1 --config config_coronary_debug
很好，单GPU测试成功
接下来测试2个GPU
nohup python3 -u train_coronary_pl.py --devices 2 --strategy ddp --config config_coronary_debug > test_2gpu.log 2>&1 &
双卡测试成功
现在测试gpu显存，正式任务可以用4张V100，每张32G显存,现在找不OOM的最大bs。
测试已完成，128分辨率，最大bs只能是1
第一次正式训练，发现config有些要改：
lr改成2e-3，增大学习率
cfg["resume_checkpoint"] 改成绝对路径
cfg["resume_checkpoint"] = "/projappl/project_2016517/chengjun/NeAR_fix_Public-Cardiac-CT-Dataset/repairing/near_repairing/stage1_coronary/checkpoints/Coronary_class9_shape_only_251109_061814/best.pth"
cfg["eval_interval"] = 5
epoch再来300，观察。

stage1_coronary 
目前最好：/projappl/project_2016517/chengjun/NeAR_fix_Public-Cardiac-CT-Dataset/repairing/near_repairing/stage1_coronary/checkpoints/Coronary_class9_shape_only_251119_185259/best.ckpt
val dice： 0.38
待完成：推理到256，然后进行可视化对比，如果效果不好，要再训练
